{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db851207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 00:31:41.718888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 00:31:42.298305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dataset import LayoutDataset\n",
    "from models import LayoutMLP\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aedd7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 00:31:44.568412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:44.593519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:44.593771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:44.595345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:44.595568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:44.595751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:45.076461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:45.076722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:45.076914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-17 00:31:45.077070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train layout:nlp:default 7500 False\n",
      "train layout:nlp:random 7500 False\n",
      "train layout:xla:random 7500 True\n",
      "train layout:xla:default 7500 True\n",
      "test all_filenames 10000 False\n",
      "valid all_filenames 1000 False\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataset_take = 7500\n",
    "learning_rate = 1.5e-3\n",
    "batch_per_file_size = 8\n",
    "node_embedding_size = 15\n",
    "layers = [128, 64, 32, 36, 24]\n",
    "\n",
    "dataset = LayoutDataset(\n",
    "    batch_size, dataset_take,\n",
    "    build_tfrecords=False,\n",
    "    batch_per_file_size=batch_per_file_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57227338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = LayoutMLP(\n",
    "    batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_per_file_size=batch_per_file_size,\n",
    "    node_embedding_size=node_embedding_size,\n",
    "    validation_frequency=10_000,\n",
    "    validations_without_improvement=5,\n",
    "    layer_sizes=layers,\n",
    "    loss='pairwise_hinge',\n",
    "    n_siblings=dataset.n_siblings,\n",
    "    l1_multiplier=2e-8,\n",
    "    output_name='layout_11_17_00_31'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a4de898",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 00:34:27.591259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-11-17 00:34:27.618264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa455738dd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-17 00:34:27.618289: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2023-11-17 00:34:27.636154: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-17 00:34:27.857208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-17 00:34:28.005048: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 training loss 3.1785538 lr 0.00007\n",
      "iteration 1000 training loss 3.0411444 lr 0.00015\n",
      "iteration 1500 training loss 2.5490065 lr 0.00022\n",
      "iteration 2000 training loss 2.672461 lr 0.00030\n",
      "iteration 2500 training loss 2.205079 lr 0.00037\n",
      "iteration 3000 training loss 1.9584506 lr 0.00045\n",
      "iteration 3500 training loss 2.5627975 lr 0.00052\n",
      "iteration 4000 training loss 1.9557202 lr 0.00060\n",
      "iteration 4500 training loss 2.2714658 lr 0.00067\n",
      "iteration 5000 training loss 2.4174385 lr 0.00075\n",
      "iteration 5500 training loss 1.6839917 lr 0.00082\n",
      "iteration 6000 training loss 1.8695959 lr 0.00090\n",
      "iteration 6500 training loss 1.3981333 lr 0.00097\n",
      "iteration 7000 training loss 2.2989886 lr 0.00105\n",
      "iteration 7500 training loss 1.9915638 lr 0.00112\n",
      "iteration 8000 training loss 1.5128672 lr 0.00120\n",
      "iteration 8500 training loss 1.5050665 lr 0.00127\n",
      "iteration 9000 training loss 1.0806851 lr 0.00135\n",
      "iteration 9500 training loss 1.0383729 lr 0.00142\n",
      "iteration 10000 training loss 1.5898135 lr 0.00150\n",
      "layout:nlp:random 0.8632948200472645\n",
      "layout:nlp:default 0.4325676969446888\n",
      "layout:xla:random 0.48571578089101586\n",
      "layout:xla:default 0.1971685312425246\n",
      "epoch 0, it 10000 validation loss -0.495\n",
      "iteration 10500 training loss 1.4945326 lr 0.00150\n",
      "iteration 11000 training loss 1.283984 lr 0.00150\n",
      "iteration 11500 training loss 1.8818903 lr 0.00150\n",
      "iteration 12000 training loss 2.1300101 lr 0.00150\n",
      "iteration 12500 training loss 1.4332888 lr 0.00150\n",
      "iteration 13000 training loss 1.7884781 lr 0.00150\n",
      "iteration 13500 training loss 1.8231058 lr 0.00150\n",
      "iteration 14000 training loss 1.1033925 lr 0.00150\n",
      "iteration 14500 training loss 1.4604092 lr 0.00150\n",
      "iteration 15000 training loss 1.6878368 lr 0.00150\n",
      "iteration 15500 training loss 1.4606706 lr 0.00150\n",
      "iteration 16000 training loss 1.3448035 lr 0.00150\n",
      "iteration 16500 training loss 1.5979848 lr 0.00150\n",
      "iteration 17000 training loss 1.0716689 lr 0.00150\n",
      "iteration 17500 training loss 1.7075576 lr 0.00150\n",
      "iteration 18000 training loss 1.6392481 lr 0.00150\n",
      "iteration 18500 training loss 1.3305374 lr 0.00150\n",
      "iteration 19000 training loss 1.3554293 lr 0.00150\n",
      "iteration 19500 training loss 1.1846737 lr 0.00149\n",
      "iteration 20000 training loss 1.1429491 lr 0.00149\n",
      "layout:nlp:random 0.8965395074745703\n",
      "layout:nlp:default 0.4614035269669505\n",
      "layout:xla:random 0.5442292192954578\n",
      "layout:xla:default 0.31162519792916477\n",
      "epoch 0, it 20000 validation loss -0.553\n",
      "iteration 20500 training loss 1.27635 lr 0.00149\n",
      "iteration 21000 training loss 0.966573 lr 0.00149\n",
      "iteration 21500 training loss 1.496396 lr 0.00149\n",
      "iteration 22000 training loss 1.0632502 lr 0.00149\n",
      "iteration 22500 training loss 1.3285227 lr 0.00149\n",
      "iteration 23000 training loss 1.5707012 lr 0.00149\n",
      "iteration 23500 training loss 1.4526855 lr 0.00149\n",
      "iteration 24000 training loss 1.0965338 lr 0.00149\n",
      "iteration 24500 training loss 1.3757905 lr 0.00149\n",
      "iteration 25000 training loss 1.0656272 lr 0.00149\n",
      "iteration 25500 training loss 1.6963844 lr 0.00149\n",
      "iteration 26000 training loss 1.9097455 lr 0.00149\n",
      "iteration 26500 training loss 1.2425038 lr 0.00148\n",
      "iteration 27000 training loss 0.8651458 lr 0.00148\n",
      "iteration 27500 training loss 1.4893087 lr 0.00148\n",
      "iteration 28000 training loss 1.4965935 lr 0.00148\n",
      "iteration 28500 training loss 1.3539927 lr 0.00148\n",
      "iteration 29000 training loss 1.2126646 lr 0.00148\n",
      "iteration 29500 training loss 1.7012521 lr 0.00148\n",
      "iteration 30000 training loss 1.3371159 lr 0.00148\n",
      "layout:nlp:random 0.8935567389424784\n",
      "layout:nlp:default 0.48024075479957257\n",
      "layout:xla:random 0.5270790371173945\n",
      "layout:xla:default 0.2900393809578892\n",
      "epoch 0, it 30000 validation loss -0.548\n",
      "iteration 30500 training loss 1.1556122 lr 0.00148\n",
      "iteration 31000 training loss 1.3037946 lr 0.00148\n",
      "iteration 31500 training loss 1.56382 lr 0.00147\n",
      "iteration 32000 training loss 1.1287682 lr 0.00147\n",
      "iteration 32500 training loss 1.585686 lr 0.00147\n",
      "iteration 33000 training loss 1.3113806 lr 0.00147\n",
      "iteration 33500 training loss 1.4944623 lr 0.00147\n",
      "iteration 34000 training loss 1.4868051 lr 0.00147\n",
      "iteration 34500 training loss 1.1800988 lr 0.00147\n",
      "iteration 35000 training loss 1.2260756 lr 0.00147\n",
      "iteration 35500 training loss 1.3015805 lr 0.00146\n",
      "iteration 36000 training loss 0.824232 lr 0.00146\n",
      "iteration 36500 training loss 1.5677829 lr 0.00146\n",
      "iteration 37000 training loss 1.6407948 lr 0.00146\n",
      "iteration 37500 training loss 1.6394081 lr 0.00146\n",
      "iteration 38000 training loss 1.6429181 lr 0.00146\n",
      "iteration 38500 training loss 0.91226226 lr 0.00145\n",
      "iteration 39000 training loss 1.1958356 lr 0.00145\n",
      "iteration 39500 training loss 1.1691499 lr 0.00145\n",
      "iteration 40000 training loss 1.1572236 lr 0.00145\n",
      "layout:nlp:random 0.8887764757537756\n",
      "layout:nlp:default 0.4791366789033867\n",
      "layout:xla:random 0.5437129566272614\n",
      "layout:xla:default 0.3060793090157569\n",
      "epoch 0, it 40000 validation loss -0.554\n",
      "iteration 40500 training loss 1.0170718 lr 0.00145\n",
      "iteration 41000 training loss 1.4182353 lr 0.00145\n",
      "iteration 41500 training loss 1.63517 lr 0.00144\n",
      "iteration 42000 training loss 1.3281932 lr 0.00144\n",
      "iteration 42500 training loss 0.9454085 lr 0.00144\n",
      "iteration 43000 training loss 1.2045859 lr 0.00144\n",
      "iteration 43500 training loss 0.9062559 lr 0.00144\n",
      "iteration 44000 training loss 1.5697913 lr 0.00144\n",
      "iteration 44500 training loss 1.2479898 lr 0.00143\n",
      "iteration 45000 training loss 1.6335256 lr 0.00143\n",
      "iteration 45500 training loss 1.2557558 lr 0.00143\n",
      "iteration 46000 training loss 1.1900283 lr 0.00143\n",
      "iteration 46500 training loss 1.5931581 lr 0.00143\n",
      "iteration 47000 training loss 0.7134431 lr 0.00142\n",
      "iteration 47500 training loss 1.4510231 lr 0.00142\n",
      "iteration 48000 training loss 1.6445934 lr 0.00142\n",
      "iteration 48500 training loss 0.959676 lr 0.00142\n",
      "iteration 49000 training loss 1.539212 lr 0.00142\n",
      "iteration 49500 training loss 1.1710392 lr 0.00141\n",
      "iteration 50000 training loss 1.4975224 lr 0.00141\n",
      "layout:nlp:random 0.9028458896416109\n",
      "layout:nlp:default 0.4852187931215889\n",
      "layout:xla:random 0.5388566579862161\n",
      "layout:xla:default 0.28525174131864695\n",
      "epoch 0, it 50000 validation loss -0.553\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/kaggle_model_runtime/models.py:90\u001b[0m, in \u001b[0;36mMLP.train\u001b[0;34m(self, dataset, validation_callback)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m training_dataset:\n\u001b[0;32m---> 90\u001b[0m         training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;66;03m# TODO: use tensorboard -> tune learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.train(dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.best_val_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc61a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(mlp.best_val_subsets.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3418de",
   "metadata": {},
   "source": [
    "## Evaluate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = mlp.predict_over_dataset(dataset.valid_data, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(val_df['target'], bins=50)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(val_df['prediction'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded26b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_df.groupby('ID').apply(lambda x: x.min()).sort_values('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72249379",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin = np.linspace(15, 25, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "random_sample = val_df.sample(1_000)\n",
    "graph_id = np.random.choice(val_df['ID'].unique())\n",
    "#graph_id = b'layout:xla:default:inception_v3_batch_128_train'\n",
    "#graph_id = b'layout:xla:default:unet_3d.4x4.bf16'\n",
    "random_sample = val_df[val_df['ID'] == graph_id].copy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    random_sample.target,\n",
    "    np.clip(random_sample.prediction, a_min=-500.0, a_max=1000.0),\n",
    "    alpha=0.1,\n",
    "    #c=random_sample['ID'].apply(lambda x: x.decode('UTF-8').split(':')[1] == 'xla').values.astype(float)\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(graph_id)\n",
    "#plt.colorbar()\n",
    "\n",
    "random_sample.sort_values('target', inplace=True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    np.arange(len(random_sample)),\n",
    "    np.clip(random_sample.prediction, a_min=-500.0, a_max=1000.0),\n",
    "    alpha=0.1,\n",
    "    #c=random_sample['ID'].apply(lambda x: x.decode('UTF-8').split(':')[1] == 'xla').values.astype(float)\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(graph_id)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mlp.dense_layer_node_1.kernel.numpy().flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = mlp.embedding_layer_node_ops.weights[0].numpy()\n",
    "emb = emb / np.expand_dims(np.linalg.norm(emb, axis=1), axis=-1)\n",
    "dots = np.matmul(emb, emb.T)\n",
    "plt.imshow(dots)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ecd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.unravel_index(np.argsort(dots.flatten())[-127], dots.shape)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[val_df['ID'] == b'layout:xla:default:unet_3d.4x4.bf16'].sort_values('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_batch in dataset.valid_data:\n",
    "    if b'layout:xla:default:mlperf_bert_batch_24_2x2' in val_batch['layout_id'].numpy():\n",
    "        print(val_batch['layout_id'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(val_batch['layout_id'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c99ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'layout:xla:default:mlperf_bert_batch_24_2x2' in val_batch['layout_id'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0639b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = val_df.sample(5_000)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.abs(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('abs error')\n",
    "x_lin = np.linspace(0, 0.7, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.square(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('squared error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_configs(df):\n",
    "    top = df.sort_values('prediction')\n",
    "    top = top['config_index'].values.tolist()\n",
    "    top = [str(i) for i in top]\n",
    "    return ';'.join(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c359b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_prediction = val_df.groupby('ID').apply(sort_configs)\n",
    "val_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb521f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3])).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layout_score_group(df):\n",
    "    score, _ = kendalltau(df['prediction'], df['target'])\n",
    "    return score\n",
    "\n",
    "val_df['subset'] = val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3]))\n",
    "all_means = []\n",
    "for subset in val_df['subset'].unique():\n",
    "    mean = np.mean(val_df[val_df['subset'] == subset].groupby('ID').apply(compute_layout_score_group))\n",
    "    all_means.append(mean)\n",
    "    print(subset, mean)\n",
    "print(np.mean(all_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([\n",
    "    0.9235,\n",
    "    0.6591,\n",
    "    0.516,\n",
    "    0.358,\n",
    "    0.968\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fffb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_scores = val_df.groupby('ID').apply(compute_layout_score_group)\n",
    "val_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ed6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_nodes_list = []\n",
    "for batch in dataset.valid_data:\n",
    "    df = pd.DataFrame(\n",
    "        np.stack([\n",
    "            batch['layout_id'].numpy(), \n",
    "            batch['valid_nodes'].numpy()], axis=-1),\n",
    "        columns=['ID', 'valid_nodes']\n",
    "    ).drop_duplicates('ID')\n",
    "    valid_nodes_list.append(df)\n",
    "valid_nodes = pd.concat(valid_nodes_list).drop_duplicates('ID')\n",
    "valid_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nodes.set_index('ID', inplace=True)\n",
    "valid_nodes['scores'] = val_scores\n",
    "valid_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810f776",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_nodes.reset_index(inplace=True)\n",
    "valid_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09650c5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_nodes['subset'] = valid_nodes['ID'].apply(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3]))\n",
    "valid_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    valid_nodes['scores'],\n",
    "    valid_nodes['valid_nodes']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d41b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'layout:xla:random'\n",
    "valid_nodes_subset = valid_nodes[valid_nodes['subset'] == subset]\n",
    "print(valid_nodes_subset.sort_values('valid_nodes').iloc[-1])\n",
    "plt.scatter(\n",
    "    valid_nodes_subset['scores'],\n",
    "    valid_nodes_subset['valid_nodes']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_score(candidate_order, layout_dict):\n",
    "    runtimes = layout_dict['config_runtime']\n",
    "    best_ranking = np.argsort(runtimes)\n",
    "    assert len(candidate_order) == len(runtimes)\n",
    "    score, _ = kendalltau(candidate_order, best_ranking)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4289a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_order = np.argsort(layout_dict['config_runtime'])\n",
    "plt.scatter(true_order, candidate_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d439d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layout_set = 'valid'\n",
    "true_orders = []\n",
    "layout_ids = []\n",
    "for dirpath, dirnames, filenames in os.walk('predict-ai-model-runtime/npz_all/npz/layout'):\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "    \n",
    "    if dirpath.split('/')[-1] != layout_set:\n",
    "        continue\n",
    "        \n",
    "    layout_id_prefix = ':'.join(dirpath.split('/')[-4:-1])\n",
    "    for filename in os.listdir(dirpath):\n",
    "        print(filename)\n",
    "        layout_id = layout_id_prefix+':'+filename[:-4]\n",
    "        layout_dict = dict(np.load(os.path.join(dirpath, filename)))\n",
    "        runtimes = layout_dict['config_runtime']\n",
    "        best_ranking = np.argsort(runtimes)\n",
    "        best_ranking = ';'.join([str(i) for i in best_ranking])\n",
    "        true_orders.append(best_ranking)\n",
    "        layout_ids.append(layout_id)\n",
    "        \n",
    "true_order_df = pd.DataFrame(\n",
    "    data=np.stack([layout_ids, true_orders], axis=-1),\n",
    "    columns=['ID', 'true_order']\n",
    ")\n",
    "true_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6177a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layout_id = true_order_df.sample()['ID'].values[0]\n",
    "layout_id = 'layout:xla:default:resnet50.4x4.fp16'\n",
    "true_order = [int(i) for i in true_order_df[true_order_df['ID'] == layout_id]['true_order'].values[0].split(';')]\n",
    "candidate_order = [int(i) for i in val_prediction[layout_id].split(';')]\n",
    "\n",
    "plt.scatter(true_order, candidate_order)\n",
    "plt.xlabel('true order')\n",
    "plt.ylabel('candidate order')\n",
    "plt.title(f'{layout_id}, len {len(true_order)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d506a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dict = dict(np.load('predict-ai-model-runtime/npz_all/npz/layout/nlp/default/valid/small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train.npz'))\n",
    "layout_dict['node_config_feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[val_df['ID'] == b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18339",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_layout['score'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434f868",
   "metadata": {},
   "source": [
    "## Inference over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = mlp.predict_over_dataset(dataset.test_data, return_labels=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39927c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby('ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ba9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = test_df.groupby('ID').apply(sort_configs)\n",
    "test_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_prediction, columns=['TopConfigs']).to_csv('predictions/layout_final_test_prediction_11_16_12_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f73f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
