{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db851207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 15:26:55.221971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-08 15:26:56.126647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dataset import LayoutDataset\n",
    "from models import LayoutMLP\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dda8a3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 15:26:57.225860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.258401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.258648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.260964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.261162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.261345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.783872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.784098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.784287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-08 15:26:57.784436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataset = LayoutDataset(\n",
    "    batch_size, train_sample_fraction=1.0, \n",
    "    subset=None, build_tfrecords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d32ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = LayoutMLP(batch_size, learning_rate=5e-4, mask_max_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a121fb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 15:27:01.232743: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1447936000 exceeds 10% of free system memory.\n",
      "2023-10-08 15:27:02.544163: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1447936000 exceeds 10% of free system memory.\n",
      "2023-10-08 15:27:07.739446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-08 15:27:07.753272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4caeeff6d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-08 15:27:07.753289: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2023-10-08 15:27:07.771786: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-08 15:27:07.995731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-08 15:27:08.137941: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 training loss 1.3257735 lr 0.00050\n",
      "iteration 200 training loss 1.2829813 lr 0.00050\n",
      "iteration 300 training loss 1.2209314 lr 0.00050\n",
      "iteration 400 training loss 1.2100298 lr 0.00050\n",
      "iteration 500 training loss 1.0676243 lr 0.00050\n",
      "iteration 600 training loss 1.0673662 lr 0.00050\n",
      "iteration 700 training loss 0.9702305 lr 0.00050\n",
      "iteration 800 training loss 1.0280004 lr 0.00050\n",
      "iteration 900 training loss 0.9351675 lr 0.00050\n",
      "iteration 1000 training loss 0.9892819 lr 0.00050\n",
      "iteration 1100 training loss 1.0601578 lr 0.00050\n",
      "iteration 1200 training loss 0.8941721 lr 0.00050\n",
      "iteration 1300 training loss 0.9378957 lr 0.00050\n",
      "iteration 1400 training loss 0.9158257 lr 0.00050\n",
      "iteration 1500 training loss 1.0002735 lr 0.00050\n",
      "iteration 1600 training loss 0.94331545 lr 0.00050\n",
      "iteration 1700 training loss 0.85230154 lr 0.00050\n",
      "iteration 1800 training loss 1.014942 lr 0.00050\n",
      "iteration 1900 training loss 0.9053944 lr 0.00050\n",
      "iteration 2000 training loss 0.87291616 lr 0.00050\n",
      "iteration 2100 training loss 0.8838277 lr 0.00050\n",
      "iteration 2200 training loss 1.0557159 lr 0.00050\n",
      "iteration 2300 training loss 0.72947663 lr 0.00050\n",
      "iteration 2400 training loss 0.92303574 lr 0.00050\n",
      "iteration 2500 training loss 0.9128898 lr 0.00050\n",
      "iteration 2600 training loss 0.8558347 lr 0.00050\n",
      "iteration 2700 training loss 0.9690129 lr 0.00050\n",
      "iteration 2800 training loss 0.8247108 lr 0.00050\n",
      "iteration 2900 training loss 0.9208438 lr 0.00050\n",
      "iteration 3000 training loss 1.037534 lr 0.00050\n",
      "iteration 3100 training loss 0.75965476 lr 0.00050\n",
      "iteration 3200 training loss 1.0701927 lr 0.00050\n",
      "iteration 3300 training loss 1.1998963 lr 0.00050\n",
      "iteration 3400 training loss 1.0202391 lr 0.00050\n",
      "iteration 3500 training loss 0.8872246 lr 0.00050\n",
      "iteration 3600 training loss 1.094878 lr 0.00050\n",
      "iteration 3700 training loss 1.125225 lr 0.00050\n",
      "iteration 3800 training loss 0.88098556 lr 0.00050\n",
      "iteration 3900 training loss 1.1255982 lr 0.00050\n",
      "iteration 4000 training loss 0.9809637 lr 0.00050\n",
      "iteration 4100 training loss 1.047193 lr 0.00050\n",
      "iteration 4200 training loss 1.1356132 lr 0.00050\n",
      "iteration 4300 training loss 1.0347718 lr 0.00050\n",
      "iteration 4400 training loss 1.0433706 lr 0.00050\n",
      "iteration 4500 training loss 0.9463486 lr 0.00050\n",
      "iteration 4600 training loss 0.8218219 lr 0.00050\n",
      "iteration 4700 training loss 0.64984125 lr 0.00050\n",
      "iteration 4800 training loss 1.0272429 lr 0.00050\n",
      "iteration 4900 training loss 0.97414225 lr 0.00050\n",
      "iteration 5000 training loss 0.9029055 lr 0.00050\n",
      "iteration 5100 training loss 0.9086103 lr 0.00050\n",
      "iteration 5200 training loss 0.8424948 lr 0.00050\n",
      "iteration 5300 training loss 0.9769763 lr 0.00050\n",
      "iteration 5400 training loss 0.9255959 lr 0.00050\n",
      "iteration 5500 training loss 0.9397733 lr 0.00050\n",
      "iteration 5600 training loss 1.0455432 lr 0.00050\n",
      "iteration 5700 training loss 0.7624666 lr 0.00050\n",
      "iteration 5800 training loss 1.0901673 lr 0.00050\n",
      "iteration 5900 training loss 1.0733668 lr 0.00050\n",
      "iteration 6000 training loss 0.8408689 lr 0.00050\n",
      "iteration 6100 training loss 0.9082031 lr 0.00050\n",
      "iteration 6200 training loss 0.9194789 lr 0.00050\n",
      "iteration 6300 training loss 1.1168972 lr 0.00050\n",
      "iteration 6400 training loss 0.99911773 lr 0.00050\n",
      "iteration 6500 training loss 0.9865262 lr 0.00050\n",
      "iteration 6600 training loss 1.0355449 lr 0.00050\n",
      "iteration 6700 training loss 0.9953543 lr 0.00050\n",
      "iteration 6800 training loss 0.96122336 lr 0.00050\n",
      "iteration 6900 training loss 1.066173 lr 0.00050\n",
      "iteration 7000 training loss 0.94249636 lr 0.00050\n",
      "iteration 7100 training loss 0.88135946 lr 0.00050\n",
      "iteration 7200 training loss 0.87439394 lr 0.00050\n",
      "iteration 7300 training loss 1.0072323 lr 0.00050\n",
      "iteration 7400 training loss 0.8954506 lr 0.00050\n",
      "iteration 7500 training loss 0.8729461 lr 0.00050\n",
      "iteration 7600 training loss 0.968385 lr 0.00050\n",
      "iteration 7700 training loss 1.0343183 lr 0.00050\n",
      "iteration 7800 training loss 0.7722082 lr 0.00050\n",
      "iteration 7900 training loss 0.78592265 lr 0.00050\n",
      "iteration 8000 training loss 0.94347334 lr 0.00050\n",
      "iteration 8100 training loss 0.91394836 lr 0.00050\n",
      "iteration 8200 training loss 0.95303005 lr 0.00050\n",
      "iteration 8300 training loss 0.98713106 lr 0.00050\n",
      "iteration 8400 training loss 1.0519217 lr 0.00050\n",
      "iteration 8500 training loss 0.9604193 lr 0.00050\n",
      "iteration 8600 training loss 0.89880174 lr 0.00050\n",
      "iteration 8700 training loss 1.109848 lr 0.00050\n",
      "iteration 8800 training loss 0.8091925 lr 0.00050\n",
      "iteration 8900 training loss 0.97011375 lr 0.00050\n",
      "iteration 9000 training loss 0.7712822 lr 0.00050\n",
      "iteration 9100 training loss 0.9280184 lr 0.00050\n",
      "iteration 9200 training loss 0.95096743 lr 0.00050\n",
      "iteration 9300 training loss 0.85054755 lr 0.00050\n",
      "iteration 9400 training loss 0.82744193 lr 0.00050\n",
      "iteration 9500 training loss 0.9416626 lr 0.00050\n",
      "iteration 9600 training loss 0.8028346 lr 0.00050\n",
      "iteration 9700 training loss 0.8754928 lr 0.00050\n",
      "iteration 9800 training loss 0.96307373 lr 0.00050\n",
      "iteration 9900 training loss 0.83460957 lr 0.00050\n",
      "iteration 10000 training loss 1.0205643 lr 0.00050\n",
      "iteration 10100 training loss 0.84125334 lr 0.00050\n",
      "iteration 10200 training loss 1.1774558 lr 0.00050\n",
      "iteration 10300 training loss 1.0044023 lr 0.00050\n",
      "iteration 10400 training loss 0.9359285 lr 0.00050\n",
      "iteration 10500 training loss 0.6862396 lr 0.00050\n",
      "iteration 10600 training loss 0.9428729 lr 0.00050\n",
      "iteration 10700 training loss 0.81797236 lr 0.00050\n",
      "iteration 10800 training loss 0.943362 lr 0.00050\n",
      "iteration 10900 training loss 0.8302598 lr 0.00050\n",
      "iteration 11000 training loss 0.93147945 lr 0.00050\n",
      "iteration 11100 training loss 0.86657333 lr 0.00050\n",
      "iteration 11200 training loss 1.0741773 lr 0.00050\n",
      "iteration 11300 training loss 0.8806464 lr 0.00050\n",
      "iteration 11400 training loss 0.7168583 lr 0.00050\n",
      "iteration 11500 training loss 0.8719208 lr 0.00050\n",
      "iteration 11600 training loss 0.8494018 lr 0.00050\n",
      "iteration 11700 training loss 0.6971894 lr 0.00050\n",
      "iteration 11800 training loss 0.9300816 lr 0.00050\n",
      "iteration 11900 training loss 0.8351139 lr 0.00050\n",
      "iteration 12000 training loss 0.8676948 lr 0.00050\n",
      "iteration 12100 training loss 0.8956223 lr 0.00050\n",
      "iteration 12200 training loss 1.1446393 lr 0.00050\n",
      "iteration 12300 training loss 0.8588922 lr 0.00050\n",
      "iteration 12400 training loss 0.8191195 lr 0.00050\n",
      "iteration 12500 training loss 0.85205084 lr 0.00050\n",
      "iteration 12600 training loss 0.75590485 lr 0.00050\n",
      "iteration 12700 training loss 1.0070045 lr 0.00050\n",
      "iteration 12800 training loss 0.8037948 lr 0.00050\n",
      "iteration 12900 training loss 0.7260372 lr 0.00050\n",
      "iteration 13000 training loss 0.9151715 lr 0.00050\n",
      "iteration 13100 training loss 0.78785694 lr 0.00050\n",
      "iteration 13200 training loss 0.83818126 lr 0.00050\n",
      "iteration 13300 training loss 0.90721995 lr 0.00050\n",
      "iteration 13400 training loss 1.058315 lr 0.00050\n",
      "iteration 13500 training loss 0.93066245 lr 0.00050\n",
      "iteration 13600 training loss 0.77705604 lr 0.00050\n",
      "iteration 13700 training loss 0.9263632 lr 0.00050\n",
      "iteration 13800 training loss 0.7675432 lr 0.00050\n",
      "iteration 13900 training loss 0.8533779 lr 0.00050\n",
      "iteration 14000 training loss 0.67166096 lr 0.00050\n",
      "iteration 14100 training loss 1.0512768 lr 0.00050\n",
      "iteration 14200 training loss 0.6397855 lr 0.00050\n",
      "iteration 14300 training loss 0.6894438 lr 0.00050\n",
      "iteration 14400 training loss 0.7182713 lr 0.00050\n",
      "iteration 14500 training loss 0.8839648 lr 0.00050\n",
      "iteration 14600 training loss 0.9867659 lr 0.00050\n",
      "iteration 14700 training loss 0.817622 lr 0.00050\n",
      "iteration 14800 training loss 0.91216606 lr 0.00050\n",
      "iteration 14900 training loss 0.87768275 lr 0.00050\n",
      "iteration 15000 training loss 0.6522927 lr 0.00050\n",
      "iteration 15100 training loss 1.0049406 lr 0.00050\n",
      "iteration 15200 training loss 0.8056839 lr 0.00050\n",
      "iteration 15300 training loss 0.7713496 lr 0.00050\n",
      "iteration 15400 training loss 0.9757868 lr 0.00050\n",
      "iteration 15500 training loss 0.852071 lr 0.00050\n",
      "iteration 15600 training loss 0.75077456 lr 0.00050\n",
      "iteration 15700 training loss 0.8328127 lr 0.00050\n",
      "iteration 15800 training loss 0.8580273 lr 0.00050\n",
      "iteration 15900 training loss 0.89987695 lr 0.00050\n",
      "iteration 16000 training loss 0.78138787 lr 0.00050\n",
      "iteration 16100 training loss 0.9265575 lr 0.00050\n",
      "iteration 16200 training loss 0.98462087 lr 0.00050\n",
      "iteration 16300 training loss 0.7451279 lr 0.00050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16400 training loss 0.8473049 lr 0.00050\n",
      "iteration 16500 training loss 0.8165746 lr 0.00050\n",
      "iteration 16600 training loss 0.9651622 lr 0.00050\n",
      "iteration 16700 training loss 0.69399697 lr 0.00050\n",
      "iteration 16800 training loss 0.9360864 lr 0.00050\n",
      "iteration 16900 training loss 0.80530375 lr 0.00050\n",
      "iteration 17000 training loss 0.9076395 lr 0.00050\n",
      "iteration 17100 training loss 0.7837477 lr 0.00050\n",
      "iteration 17200 training loss 0.7423207 lr 0.00050\n",
      "iteration 17300 training loss 0.98560554 lr 0.00050\n",
      "iteration 17400 training loss 0.6944104 lr 0.00050\n",
      "iteration 17500 training loss 0.77224606 lr 0.00050\n",
      "iteration 17600 training loss 1.0458962 lr 0.00050\n",
      "iteration 17700 training loss 0.95815754 lr 0.00050\n",
      "iteration 17800 training loss 0.8462781 lr 0.00050\n",
      "iteration 17900 training loss 0.8034338 lr 0.00050\n",
      "iteration 18000 training loss 0.8124923 lr 0.00050\n",
      "iteration 18100 training loss 0.9538169 lr 0.00050\n",
      "iteration 18200 training loss 0.6939997 lr 0.00050\n",
      "iteration 18300 training loss 0.8739722 lr 0.00050\n",
      "iteration 18400 training loss 0.8935204 lr 0.00050\n",
      "iteration 18500 training loss 1.0337589 lr 0.00050\n",
      "iteration 18600 training loss 0.8515091 lr 0.00050\n",
      "iteration 18700 training loss 0.93485963 lr 0.00050\n",
      "iteration 18800 training loss 0.74985087 lr 0.00050\n",
      "iteration 18900 training loss 0.8260421 lr 0.00050\n",
      "iteration 19000 training loss 0.87388784 lr 0.00050\n",
      "iteration 19100 training loss 0.60195667 lr 0.00050\n",
      "iteration 19200 training loss 0.8110904 lr 0.00050\n",
      "iteration 19300 training loss 0.8840305 lr 0.00050\n",
      "iteration 19400 training loss 0.772271 lr 0.00050\n",
      "iteration 19500 training loss 0.8056738 lr 0.00050\n",
      "iteration 19600 training loss 0.9092409 lr 0.00050\n",
      "iteration 19700 training loss 0.6371848 lr 0.00050\n",
      "iteration 19800 training loss 0.77317166 lr 0.00050\n",
      "iteration 19900 training loss 0.96127564 lr 0.00050\n",
      "iteration 20000 training loss 0.7629778 lr 0.00050\n",
      "layout:nlp:random 0.7431443210139557\n",
      "layout:nlp:default 0.36310885367579726\n",
      "layout:xla:random 0.35810861750737766\n",
      "layout:xla:default 0.1441690997209648\n",
      "epoch 0, it 20000 validation loss -0.402\n",
      "iteration 20100 training loss 1.0243206 lr 0.00045\n",
      "iteration 20200 training loss 0.72070885 lr 0.00045\n",
      "iteration 20300 training loss 0.79249793 lr 0.00045\n",
      "iteration 20400 training loss 0.8707855 lr 0.00045\n",
      "iteration 20500 training loss 0.83132046 lr 0.00045\n",
      "iteration 20600 training loss 0.8426104 lr 0.00045\n",
      "iteration 20700 training loss 0.8399562 lr 0.00045\n",
      "iteration 20800 training loss 0.76752806 lr 0.00045\n",
      "iteration 20900 training loss 0.79064405 lr 0.00045\n",
      "iteration 21000 training loss 0.93599796 lr 0.00045\n",
      "iteration 21100 training loss 0.7745359 lr 0.00045\n",
      "iteration 21200 training loss 0.8915494 lr 0.00045\n",
      "iteration 21300 training loss 0.8094272 lr 0.00045\n",
      "iteration 21400 training loss 0.78267926 lr 0.00045\n",
      "iteration 21500 training loss 0.90195626 lr 0.00045\n",
      "iteration 21600 training loss 0.9355717 lr 0.00045\n",
      "iteration 21700 training loss 0.6876052 lr 0.00045\n",
      "iteration 21800 training loss 0.71550065 lr 0.00045\n",
      "iteration 21900 training loss 0.95338666 lr 0.00045\n",
      "iteration 22000 training loss 0.8270426 lr 0.00045\n",
      "iteration 22100 training loss 0.8211834 lr 0.00045\n",
      "iteration 22200 training loss 0.88082564 lr 0.00045\n",
      "iteration 22300 training loss 0.83614075 lr 0.00045\n",
      "iteration 22400 training loss 0.84882677 lr 0.00045\n",
      "iteration 22500 training loss 0.8577923 lr 0.00045\n",
      "iteration 22600 training loss 0.80285186 lr 0.00045\n",
      "iteration 22700 training loss 0.7894501 lr 0.00045\n",
      "iteration 22800 training loss 0.92406136 lr 0.00045\n",
      "iteration 22900 training loss 0.6670047 lr 0.00045\n",
      "iteration 23000 training loss 0.93884164 lr 0.00045\n",
      "iteration 23100 training loss 0.9195691 lr 0.00045\n",
      "iteration 23200 training loss 0.9475143 lr 0.00045\n",
      "iteration 23300 training loss 0.7957238 lr 0.00045\n",
      "iteration 23400 training loss 0.643651 lr 0.00045\n",
      "iteration 23500 training loss 0.7949788 lr 0.00045\n",
      "iteration 23600 training loss 0.80001956 lr 0.00045\n",
      "iteration 23700 training loss 0.89987946 lr 0.00045\n",
      "iteration 23800 training loss 0.92385185 lr 0.00045\n",
      "iteration 23900 training loss 0.67468435 lr 0.00045\n",
      "iteration 24000 training loss 0.9179638 lr 0.00045\n",
      "iteration 24100 training loss 0.8272979 lr 0.00045\n",
      "iteration 24200 training loss 1.1313373 lr 0.00045\n",
      "iteration 24300 training loss 0.9730069 lr 0.00045\n",
      "iteration 24400 training loss 0.85850614 lr 0.00045\n",
      "iteration 24500 training loss 1.0447563 lr 0.00045\n",
      "iteration 24600 training loss 0.86469483 lr 0.00045\n",
      "iteration 24700 training loss 0.93368495 lr 0.00045\n",
      "iteration 24800 training loss 0.956132 lr 0.00045\n",
      "iteration 24900 training loss 0.7878297 lr 0.00045\n",
      "iteration 25000 training loss 0.85144556 lr 0.00045\n",
      "iteration 25100 training loss 1.1337891 lr 0.00045\n",
      "iteration 25200 training loss 0.7960462 lr 0.00045\n",
      "iteration 25300 training loss 0.8307268 lr 0.00045\n",
      "iteration 25400 training loss 0.78457433 lr 0.00045\n",
      "iteration 25500 training loss 0.8028682 lr 0.00045\n",
      "iteration 25600 training loss 1.0265464 lr 0.00045\n",
      "iteration 25700 training loss 1.0733624 lr 0.00045\n",
      "iteration 25800 training loss 0.8161551 lr 0.00045\n",
      "iteration 25900 training loss 0.896788 lr 0.00045\n",
      "iteration 26000 training loss 1.0571239 lr 0.00045\n",
      "iteration 26100 training loss 1.0523559 lr 0.00045\n",
      "iteration 26200 training loss 0.7953892 lr 0.00045\n",
      "iteration 26300 training loss 0.950797 lr 0.00045\n",
      "iteration 26400 training loss 0.8479395 lr 0.00045\n",
      "iteration 26500 training loss 0.9117443 lr 0.00045\n",
      "iteration 26600 training loss 0.9444246 lr 0.00045\n",
      "iteration 26700 training loss 0.81488115 lr 0.00045\n",
      "iteration 26800 training loss 0.6935259 lr 0.00045\n",
      "iteration 26900 training loss 0.8555628 lr 0.00045\n",
      "iteration 27000 training loss 0.5802469 lr 0.00045\n",
      "iteration 27100 training loss 0.71664333 lr 0.00045\n",
      "iteration 27200 training loss 0.72545093 lr 0.00045\n",
      "iteration 27300 training loss 0.64960533 lr 0.00045\n",
      "iteration 27400 training loss 0.9037685 lr 0.00045\n",
      "iteration 27500 training loss 0.6599299 lr 0.00045\n",
      "iteration 27600 training loss 0.78352565 lr 0.00045\n",
      "iteration 27700 training loss 0.6224246 lr 0.00045\n",
      "iteration 27800 training loss 0.6770658 lr 0.00045\n",
      "iteration 27900 training loss 0.79537785 lr 0.00045\n",
      "iteration 28000 training loss 0.7757229 lr 0.00045\n",
      "iteration 28100 training loss 0.8434384 lr 0.00045\n",
      "iteration 28200 training loss 0.7097347 lr 0.00045\n",
      "iteration 28300 training loss 0.80636567 lr 0.00045\n",
      "iteration 28400 training loss 0.79864156 lr 0.00045\n",
      "iteration 28500 training loss 0.7526046 lr 0.00045\n",
      "iteration 28600 training loss 0.6991791 lr 0.00045\n",
      "iteration 28700 training loss 0.77626985 lr 0.00045\n",
      "iteration 28800 training loss 0.6941794 lr 0.00045\n",
      "iteration 28900 training loss 0.740417 lr 0.00045\n",
      "iteration 29000 training loss 0.83019674 lr 0.00045\n",
      "iteration 29100 training loss 0.87878674 lr 0.00045\n",
      "iteration 29200 training loss 0.6046772 lr 0.00045\n",
      "iteration 29300 training loss 0.83897525 lr 0.00045\n",
      "iteration 29400 training loss 0.67243165 lr 0.00045\n",
      "iteration 29500 training loss 0.91894007 lr 0.00045\n",
      "iteration 29600 training loss 0.77010894 lr 0.00045\n",
      "iteration 29700 training loss 0.61107725 lr 0.00045\n",
      "iteration 29800 training loss 0.95397365 lr 0.00045\n",
      "iteration 29900 training loss 0.8793802 lr 0.00045\n",
      "iteration 30000 training loss 0.99437296 lr 0.00045\n",
      "iteration 30100 training loss 0.8473059 lr 0.00045\n",
      "iteration 30200 training loss 0.884778 lr 0.00045\n",
      "iteration 30300 training loss 0.8795978 lr 0.00045\n",
      "iteration 30400 training loss 0.933944 lr 0.00045\n",
      "iteration 30500 training loss 0.82807434 lr 0.00045\n",
      "iteration 30600 training loss 1.0451618 lr 0.00045\n",
      "iteration 30700 training loss 0.79899657 lr 0.00045\n",
      "iteration 30800 training loss 0.93113554 lr 0.00045\n",
      "iteration 30900 training loss 0.7225205 lr 0.00045\n",
      "iteration 31000 training loss 0.7311497 lr 0.00045\n",
      "iteration 31100 training loss 0.6893321 lr 0.00045\n",
      "iteration 31200 training loss 0.9737207 lr 0.00045\n",
      "iteration 31300 training loss 0.90620863 lr 0.00045\n",
      "iteration 31400 training loss 0.83580315 lr 0.00045\n",
      "iteration 31500 training loss 0.90016824 lr 0.00045\n",
      "iteration 31600 training loss 0.74481547 lr 0.00045\n",
      "iteration 31700 training loss 0.74891955 lr 0.00045\n",
      "iteration 31800 training loss 0.5813962 lr 0.00045\n",
      "iteration 31900 training loss 0.7801849 lr 0.00045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 32000 training loss 0.77594244 lr 0.00045\n",
      "iteration 32100 training loss 0.9624666 lr 0.00045\n",
      "iteration 32200 training loss 0.75192 lr 0.00045\n",
      "iteration 32300 training loss 0.8191583 lr 0.00045\n",
      "iteration 32400 training loss 1.0161132 lr 0.00045\n",
      "iteration 32500 training loss 0.7502479 lr 0.00045\n",
      "iteration 32600 training loss 0.9423812 lr 0.00045\n",
      "iteration 32700 training loss 0.7600463 lr 0.00045\n",
      "iteration 32800 training loss 0.77173895 lr 0.00045\n",
      "iteration 32900 training loss 0.6755571 lr 0.00045\n",
      "iteration 33000 training loss 0.5423075 lr 0.00045\n",
      "iteration 33100 training loss 0.75584376 lr 0.00045\n",
      "iteration 33200 training loss 0.73689187 lr 0.00045\n",
      "iteration 33300 training loss 0.87706393 lr 0.00045\n",
      "iteration 33400 training loss 0.7475372 lr 0.00045\n",
      "iteration 33500 training loss 0.7210299 lr 0.00045\n",
      "iteration 33600 training loss 0.9345867 lr 0.00045\n",
      "iteration 33700 training loss 0.884164 lr 0.00045\n",
      "iteration 33800 training loss 0.796454 lr 0.00045\n",
      "iteration 33900 training loss 0.77774113 lr 0.00045\n",
      "iteration 34000 training loss 0.7640413 lr 0.00045\n",
      "iteration 34100 training loss 0.977922 lr 0.00045\n",
      "iteration 34200 training loss 0.74345577 lr 0.00045\n",
      "iteration 34300 training loss 0.82552433 lr 0.00045\n",
      "iteration 34400 training loss 0.7576271 lr 0.00045\n",
      "iteration 34500 training loss 0.9479064 lr 0.00045\n",
      "iteration 34600 training loss 0.8915447 lr 0.00045\n",
      "iteration 34700 training loss 0.87562066 lr 0.00045\n",
      "iteration 34800 training loss 1.0795674 lr 0.00045\n",
      "iteration 34900 training loss 0.815765 lr 0.00045\n",
      "iteration 35000 training loss 0.8639924 lr 0.00045\n",
      "iteration 35100 training loss 0.79832345 lr 0.00045\n",
      "iteration 35200 training loss 0.7062216 lr 0.00045\n",
      "iteration 35300 training loss 0.8753186 lr 0.00045\n",
      "iteration 35400 training loss 0.8232875 lr 0.00045\n",
      "iteration 35500 training loss 0.92363274 lr 0.00045\n",
      "iteration 35600 training loss 0.8362457 lr 0.00045\n",
      "iteration 35700 training loss 0.6366686 lr 0.00045\n",
      "iteration 35800 training loss 0.5758589 lr 0.00045\n",
      "iteration 35900 training loss 0.7340635 lr 0.00045\n",
      "iteration 36000 training loss 0.7276819 lr 0.00045\n",
      "iteration 36100 training loss 0.62888503 lr 0.00045\n",
      "iteration 36200 training loss 0.8747392 lr 0.00045\n",
      "iteration 36300 training loss 0.8638923 lr 0.00045\n",
      "iteration 36400 training loss 0.95966643 lr 0.00045\n",
      "iteration 36500 training loss 0.8861877 lr 0.00045\n",
      "iteration 36600 training loss 0.80445415 lr 0.00045\n",
      "iteration 36700 training loss 0.72980386 lr 0.00045\n",
      "iteration 36800 training loss 0.9433168 lr 0.00045\n",
      "iteration 36900 training loss 0.81451756 lr 0.00045\n",
      "iteration 37000 training loss 0.87371266 lr 0.00045\n",
      "iteration 37100 training loss 0.8885575 lr 0.00045\n",
      "iteration 37200 training loss 0.7856902 lr 0.00045\n",
      "iteration 37300 training loss 0.7492587 lr 0.00045\n",
      "iteration 37400 training loss 0.91289425 lr 0.00045\n",
      "iteration 37500 training loss 0.8725089 lr 0.00045\n",
      "iteration 37600 training loss 0.83642817 lr 0.00045\n",
      "iteration 37700 training loss 0.6520099 lr 0.00045\n",
      "iteration 37800 training loss 1.0711424 lr 0.00045\n",
      "iteration 37900 training loss 0.812902 lr 0.00045\n",
      "iteration 38000 training loss 0.91456175 lr 0.00045\n",
      "iteration 38100 training loss 0.83838105 lr 0.00045\n",
      "iteration 38200 training loss 0.97715694 lr 0.00045\n",
      "iteration 38300 training loss 0.94553274 lr 0.00045\n",
      "iteration 38400 training loss 0.95677876 lr 0.00045\n",
      "iteration 38500 training loss 0.82322425 lr 0.00045\n",
      "iteration 38600 training loss 0.877569 lr 0.00045\n",
      "iteration 38700 training loss 0.86190206 lr 0.00045\n",
      "iteration 38800 training loss 0.9641479 lr 0.00045\n",
      "iteration 38900 training loss 0.78682184 lr 0.00045\n",
      "iteration 39000 training loss 0.7623259 lr 0.00045\n",
      "iteration 39100 training loss 0.87949663 lr 0.00045\n",
      "iteration 39200 training loss 0.9000265 lr 0.00045\n",
      "iteration 39300 training loss 0.7961939 lr 0.00045\n",
      "iteration 39400 training loss 0.8992073 lr 0.00045\n",
      "iteration 39500 training loss 0.7225649 lr 0.00045\n",
      "iteration 39600 training loss 0.749815 lr 0.00045\n",
      "iteration 39700 training loss 0.694983 lr 0.00045\n",
      "iteration 39800 training loss 0.6828982 lr 0.00045\n",
      "iteration 39900 training loss 0.8563416 lr 0.00045\n",
      "iteration 40000 training loss 0.90081364 lr 0.00045\n",
      "layout:nlp:random 0.7601681949764689\n",
      "layout:nlp:default 0.3752478486607794\n",
      "layout:xla:random 0.40158782750386113\n",
      "layout:xla:default 0.14953268636626835\n",
      "epoch 0, it 40000 validation loss -0.422\n",
      "iteration 40100 training loss 0.9068498 lr 0.00040\n",
      "iteration 40200 training loss 0.8554905 lr 0.00040\n",
      "iteration 40300 training loss 0.8819848 lr 0.00040\n",
      "iteration 40400 training loss 0.8526424 lr 0.00040\n",
      "iteration 40500 training loss 0.7092348 lr 0.00040\n",
      "iteration 40600 training loss 0.773001 lr 0.00040\n",
      "iteration 40700 training loss 0.7529185 lr 0.00040\n",
      "iteration 40800 training loss 0.8829619 lr 0.00040\n",
      "iteration 40900 training loss 0.83621573 lr 0.00040\n",
      "iteration 41000 training loss 0.79479825 lr 0.00040\n",
      "iteration 41100 training loss 0.78380764 lr 0.00040\n",
      "iteration 41200 training loss 0.69614893 lr 0.00040\n",
      "iteration 41300 training loss 0.7617273 lr 0.00040\n",
      "iteration 41400 training loss 0.8254048 lr 0.00040\n",
      "iteration 41500 training loss 0.7517746 lr 0.00040\n",
      "iteration 41600 training loss 0.7045809 lr 0.00040\n",
      "iteration 41700 training loss 0.7751131 lr 0.00040\n",
      "iteration 41800 training loss 0.7297776 lr 0.00040\n",
      "iteration 41900 training loss 0.69383454 lr 0.00040\n",
      "iteration 42000 training loss 0.78369796 lr 0.00040\n",
      "iteration 42100 training loss 0.57230705 lr 0.00040\n",
      "iteration 42200 training loss 0.5968073 lr 0.00040\n",
      "iteration 42300 training loss 0.93951833 lr 0.00040\n",
      "iteration 42400 training loss 0.73527765 lr 0.00040\n",
      "iteration 42500 training loss 0.7798345 lr 0.00040\n",
      "iteration 42600 training loss 0.81405735 lr 0.00040\n",
      "iteration 42700 training loss 0.6580568 lr 0.00040\n",
      "iteration 42800 training loss 0.735503 lr 0.00040\n",
      "iteration 42900 training loss 0.81139827 lr 0.00040\n",
      "iteration 43000 training loss 0.85067505 lr 0.00040\n",
      "iteration 43100 training loss 0.7685985 lr 0.00040\n",
      "iteration 43200 training loss 0.6249607 lr 0.00040\n",
      "iteration 43300 training loss 0.66892004 lr 0.00040\n",
      "iteration 43400 training loss 0.7340385 lr 0.00040\n",
      "iteration 43500 training loss 1.0053773 lr 0.00040\n",
      "iteration 43600 training loss 0.605139 lr 0.00040\n",
      "iteration 43700 training loss 0.7342661 lr 0.00040\n",
      "iteration 43800 training loss 0.91648823 lr 0.00040\n",
      "iteration 43900 training loss 0.80122745 lr 0.00040\n",
      "iteration 44000 training loss 0.88279474 lr 0.00040\n",
      "iteration 44100 training loss 0.90389603 lr 0.00040\n",
      "iteration 44200 training loss 0.905701 lr 0.00040\n",
      "iteration 44300 training loss 0.6727095 lr 0.00040\n",
      "iteration 44400 training loss 0.8840478 lr 0.00040\n",
      "iteration 44500 training loss 0.6563612 lr 0.00040\n",
      "iteration 44600 training loss 0.6957327 lr 0.00040\n",
      "iteration 44700 training loss 0.8623606 lr 0.00040\n",
      "iteration 44800 training loss 0.6993623 lr 0.00040\n",
      "iteration 44900 training loss 0.66460145 lr 0.00040\n",
      "iteration 45000 training loss 0.82819086 lr 0.00040\n",
      "iteration 45100 training loss 0.9037586 lr 0.00040\n",
      "iteration 45200 training loss 0.8356848 lr 0.00040\n",
      "iteration 45300 training loss 0.83165 lr 0.00040\n",
      "iteration 45400 training loss 0.61854213 lr 0.00040\n",
      "iteration 45500 training loss 0.63682514 lr 0.00040\n",
      "iteration 45600 training loss 0.78396875 lr 0.00040\n",
      "iteration 45700 training loss 0.636093 lr 0.00040\n",
      "iteration 45800 training loss 0.7022234 lr 0.00040\n",
      "iteration 45900 training loss 0.8980198 lr 0.00040\n",
      "iteration 46000 training loss 0.8674483 lr 0.00040\n",
      "iteration 46100 training loss 0.6354653 lr 0.00040\n",
      "iteration 46200 training loss 0.62292486 lr 0.00040\n",
      "iteration 46300 training loss 0.80169916 lr 0.00040\n",
      "iteration 46400 training loss 0.7739037 lr 0.00040\n",
      "iteration 46500 training loss 0.75590944 lr 0.00040\n",
      "iteration 46600 training loss 0.7453071 lr 0.00040\n",
      "iteration 46700 training loss 0.8659753 lr 0.00040\n",
      "iteration 46800 training loss 0.714457 lr 0.00040\n",
      "iteration 46900 training loss 0.5973177 lr 0.00040\n",
      "iteration 47000 training loss 0.84345055 lr 0.00040\n",
      "iteration 47100 training loss 0.7922936 lr 0.00040\n",
      "iteration 47200 training loss 0.93334144 lr 0.00040\n",
      "iteration 47300 training loss 0.8290082 lr 0.00040\n",
      "iteration 47400 training loss 0.9707268 lr 0.00040\n",
      "iteration 47500 training loss 0.96703726 lr 0.00040\n",
      "iteration 47600 training loss 0.7211481 lr 0.00040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47700 training loss 0.8718508 lr 0.00040\n",
      "iteration 47800 training loss 0.8060298 lr 0.00040\n",
      "iteration 47900 training loss 0.78880405 lr 0.00040\n",
      "iteration 48000 training loss 0.83153456 lr 0.00040\n",
      "iteration 48100 training loss 0.82782185 lr 0.00040\n",
      "iteration 48200 training loss 0.7812141 lr 0.00040\n",
      "iteration 48300 training loss 0.6107345 lr 0.00040\n",
      "iteration 48400 training loss 0.9559404 lr 0.00040\n",
      "iteration 48500 training loss 0.7786035 lr 0.00040\n",
      "iteration 48600 training loss 0.9065105 lr 0.00040\n",
      "iteration 48700 training loss 0.6891464 lr 0.00040\n",
      "iteration 48800 training loss 0.7581754 lr 0.00040\n",
      "iteration 48900 training loss 0.7789147 lr 0.00040\n",
      "iteration 49000 training loss 0.6366353 lr 0.00040\n",
      "iteration 49100 training loss 0.5701274 lr 0.00040\n",
      "iteration 49200 training loss 0.9198642 lr 0.00040\n",
      "iteration 49300 training loss 0.652399 lr 0.00040\n",
      "iteration 49400 training loss 0.88059664 lr 0.00040\n",
      "iteration 49500 training loss 0.79097736 lr 0.00040\n",
      "iteration 49600 training loss 0.8471817 lr 0.00040\n",
      "iteration 49700 training loss 0.8379249 lr 0.00040\n",
      "iteration 49800 training loss 0.7674819 lr 0.00040\n",
      "iteration 49900 training loss 0.62068546 lr 0.00040\n",
      "iteration 50000 training loss 0.7536804 lr 0.00040\n",
      "iteration 50100 training loss 0.88067853 lr 0.00040\n",
      "iteration 50200 training loss 0.6913663 lr 0.00040\n",
      "iteration 50300 training loss 0.7511795 lr 0.00040\n",
      "iteration 50400 training loss 0.6644473 lr 0.00040\n",
      "iteration 50500 training loss 0.85018337 lr 0.00040\n",
      "iteration 50600 training loss 0.7704194 lr 0.00040\n",
      "iteration 50700 training loss 0.74535316 lr 0.00040\n",
      "iteration 50800 training loss 0.66926956 lr 0.00040\n",
      "iteration 50900 training loss 0.85315907 lr 0.00040\n",
      "iteration 51000 training loss 0.8656375 lr 0.00040\n",
      "iteration 51100 training loss 0.86667806 lr 0.00040\n",
      "iteration 51200 training loss 0.8705927 lr 0.00040\n",
      "iteration 51300 training loss 0.7632585 lr 0.00040\n",
      "iteration 51400 training loss 0.8077581 lr 0.00040\n",
      "iteration 51500 training loss 0.8957553 lr 0.00040\n",
      "iteration 51600 training loss 0.8296277 lr 0.00040\n",
      "iteration 51700 training loss 0.6790715 lr 0.00040\n",
      "iteration 51800 training loss 0.9123397 lr 0.00040\n",
      "iteration 51900 training loss 0.820771 lr 0.00040\n",
      "iteration 52000 training loss 0.9607847 lr 0.00040\n",
      "iteration 52100 training loss 0.82449114 lr 0.00040\n",
      "iteration 52200 training loss 0.688322 lr 0.00040\n",
      "iteration 52300 training loss 0.92151165 lr 0.00040\n",
      "iteration 52400 training loss 0.8426244 lr 0.00040\n",
      "iteration 52500 training loss 0.68798804 lr 0.00040\n",
      "iteration 52600 training loss 0.6294101 lr 0.00040\n",
      "iteration 52700 training loss 0.79361284 lr 0.00040\n",
      "iteration 52800 training loss 0.8402651 lr 0.00040\n",
      "iteration 52900 training loss 0.7140257 lr 0.00040\n",
      "iteration 53000 training loss 0.8454273 lr 0.00040\n",
      "iteration 53100 training loss 0.75506806 lr 0.00040\n",
      "iteration 53200 training loss 0.85771596 lr 0.00040\n",
      "iteration 53300 training loss 1.0387739 lr 0.00040\n",
      "iteration 53400 training loss 0.858725 lr 0.00040\n",
      "iteration 53500 training loss 0.8193841 lr 0.00040\n",
      "iteration 53600 training loss 0.83294684 lr 0.00040\n",
      "iteration 53700 training loss 0.5953711 lr 0.00040\n",
      "iteration 53800 training loss 0.8987531 lr 0.00040\n",
      "iteration 53900 training loss 0.74061644 lr 0.00040\n",
      "iteration 54000 training loss 0.7259021 lr 0.00040\n",
      "iteration 54100 training loss 0.8330056 lr 0.00040\n",
      "iteration 54200 training loss 0.63874 lr 0.00040\n",
      "iteration 54300 training loss 0.67949194 lr 0.00040\n",
      "iteration 54400 training loss 0.7399709 lr 0.00040\n",
      "iteration 54500 training loss 0.6651801 lr 0.00040\n",
      "iteration 54600 training loss 0.7211398 lr 0.00040\n",
      "iteration 54700 training loss 0.8299012 lr 0.00040\n",
      "iteration 54800 training loss 0.8999512 lr 0.00040\n",
      "iteration 54900 training loss 0.9331459 lr 0.00040\n",
      "iteration 55000 training loss 0.7162076 lr 0.00040\n",
      "iteration 55100 training loss 0.8976116 lr 0.00040\n",
      "iteration 55200 training loss 0.9706113 lr 0.00040\n",
      "iteration 55300 training loss 0.7035436 lr 0.00040\n",
      "iteration 55400 training loss 0.9097473 lr 0.00040\n",
      "iteration 55500 training loss 0.92135125 lr 0.00040\n",
      "iteration 55600 training loss 0.6297719 lr 0.00040\n",
      "iteration 55700 training loss 0.7990974 lr 0.00040\n",
      "iteration 55800 training loss 0.86773235 lr 0.00040\n",
      "iteration 55900 training loss 0.54117996 lr 0.00040\n",
      "iteration 56000 training loss 0.71006864 lr 0.00040\n",
      "iteration 56100 training loss 0.60843587 lr 0.00040\n",
      "iteration 56200 training loss 0.8344353 lr 0.00040\n",
      "iteration 56300 training loss 0.81381196 lr 0.00040\n",
      "iteration 56400 training loss 0.81555706 lr 0.00040\n",
      "iteration 56500 training loss 0.64828897 lr 0.00040\n",
      "iteration 56600 training loss 0.82356757 lr 0.00040\n",
      "iteration 56700 training loss 0.5746061 lr 0.00040\n",
      "iteration 56800 training loss 0.5972245 lr 0.00040\n",
      "iteration 56900 training loss 0.77777624 lr 0.00040\n",
      "iteration 57000 training loss 0.7094241 lr 0.00040\n",
      "iteration 57100 training loss 0.7008584 lr 0.00040\n",
      "iteration 57200 training loss 0.8002808 lr 0.00040\n",
      "iteration 57300 training loss 0.5090219 lr 0.00040\n",
      "iteration 57400 training loss 0.7429022 lr 0.00040\n",
      "iteration 57500 training loss 0.6604219 lr 0.00040\n",
      "iteration 57600 training loss 0.64637905 lr 0.00040\n",
      "iteration 57700 training loss 0.6833945 lr 0.00040\n",
      "iteration 57800 training loss 0.5863499 lr 0.00040\n",
      "iteration 57900 training loss 0.8854755 lr 0.00040\n",
      "iteration 58000 training loss 0.7836879 lr 0.00040\n",
      "iteration 58100 training loss 0.6222967 lr 0.00040\n",
      "iteration 58200 training loss 0.78654295 lr 0.00040\n",
      "iteration 58300 training loss 0.9139346 lr 0.00040\n",
      "iteration 58400 training loss 0.9247077 lr 0.00040\n",
      "iteration 58500 training loss 0.9164412 lr 0.00040\n",
      "iteration 58600 training loss 0.6958395 lr 0.00040\n",
      "iteration 58700 training loss 0.7091063 lr 0.00040\n",
      "iteration 58800 training loss 0.64330024 lr 0.00040\n",
      "iteration 58900 training loss 0.760399 lr 0.00040\n",
      "iteration 59000 training loss 0.71357477 lr 0.00040\n",
      "iteration 59100 training loss 0.6887921 lr 0.00040\n",
      "iteration 59200 training loss 0.9372811 lr 0.00040\n",
      "iteration 59300 training loss 0.76765984 lr 0.00040\n",
      "iteration 59400 training loss 0.8356549 lr 0.00040\n",
      "iteration 59500 training loss 0.83160317 lr 0.00040\n",
      "iteration 59600 training loss 0.5919524 lr 0.00040\n",
      "iteration 59700 training loss 0.9799058 lr 0.00040\n",
      "iteration 59800 training loss 0.7807085 lr 0.00040\n",
      "iteration 59900 training loss 0.8115952 lr 0.00040\n",
      "iteration 60000 training loss 0.8530375 lr 0.00040\n",
      "layout:nlp:random 0.7935496155175797\n",
      "layout:nlp:default 0.3762811223635476\n",
      "layout:xla:random 0.47354651584753915\n",
      "layout:xla:default 0.20465063115510015\n",
      "epoch 0, it 60000 validation loss -0.462\n",
      "iteration 60100 training loss 0.8073797 lr 0.00036\n",
      "iteration 60200 training loss 0.7586337 lr 0.00036\n",
      "iteration 60300 training loss 0.7665178 lr 0.00036\n",
      "iteration 60400 training loss 1.1240814 lr 0.00036\n",
      "iteration 60500 training loss 0.93740064 lr 0.00036\n",
      "iteration 60600 training loss 0.78852487 lr 0.00036\n",
      "iteration 60700 training loss 0.80007493 lr 0.00036\n",
      "iteration 60800 training loss 0.7463575 lr 0.00036\n",
      "iteration 60900 training loss 0.79109037 lr 0.00036\n",
      "iteration 61000 training loss 0.81651413 lr 0.00036\n",
      "iteration 61100 training loss 0.8977785 lr 0.00036\n",
      "iteration 61200 training loss 0.7780792 lr 0.00036\n",
      "iteration 61300 training loss 0.9294466 lr 0.00036\n",
      "iteration 61400 training loss 0.81071806 lr 0.00036\n",
      "iteration 61500 training loss 0.7124643 lr 0.00036\n",
      "iteration 61600 training loss 0.77352786 lr 0.00036\n",
      "iteration 61700 training loss 0.76775104 lr 0.00036\n",
      "iteration 61800 training loss 0.92404836 lr 0.00036\n",
      "iteration 61900 training loss 0.64432997 lr 0.00036\n",
      "iteration 62000 training loss 0.76308453 lr 0.00036\n",
      "iteration 62100 training loss 0.74995905 lr 0.00036\n",
      "iteration 62200 training loss 0.83663386 lr 0.00036\n",
      "iteration 62300 training loss 0.80015624 lr 0.00036\n",
      "iteration 62400 training loss 0.7875303 lr 0.00036\n",
      "iteration 62500 training loss 0.78312635 lr 0.00036\n",
      "iteration 62600 training loss 0.6797596 lr 0.00036\n",
      "iteration 62700 training loss 0.60388756 lr 0.00036\n",
      "iteration 62800 training loss 0.7916954 lr 0.00036\n",
      "iteration 62900 training loss 0.64960843 lr 0.00036\n",
      "iteration 63000 training loss 0.6806325 lr 0.00036\n",
      "iteration 63100 training loss 0.93344355 lr 0.00036\n",
      "iteration 63200 training loss 0.7176361 lr 0.00036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 63300 training loss 0.783146 lr 0.00036\n",
      "iteration 63400 training loss 0.679434 lr 0.00036\n",
      "iteration 63500 training loss 0.7079006 lr 0.00036\n",
      "iteration 63600 training loss 0.7949524 lr 0.00036\n",
      "iteration 63700 training loss 0.7764738 lr 0.00036\n",
      "iteration 63800 training loss 0.66308886 lr 0.00036\n",
      "iteration 63900 training loss 0.69304734 lr 0.00036\n",
      "iteration 64000 training loss 0.95704216 lr 0.00036\n",
      "iteration 64100 training loss 0.8624059 lr 0.00036\n",
      "iteration 64200 training loss 0.8515241 lr 0.00036\n",
      "iteration 64300 training loss 0.6816975 lr 0.00036\n",
      "iteration 64400 training loss 0.6534553 lr 0.00036\n",
      "iteration 64500 training loss 0.70383054 lr 0.00036\n",
      "iteration 64600 training loss 0.91853684 lr 0.00036\n",
      "iteration 64700 training loss 0.73445857 lr 0.00036\n",
      "iteration 64800 training loss 0.6030957 lr 0.00036\n",
      "iteration 64900 training loss 0.72988385 lr 0.00036\n",
      "iteration 65000 training loss 0.9066955 lr 0.00036\n",
      "iteration 65100 training loss 0.76916474 lr 0.00036\n",
      "iteration 65200 training loss 0.89749485 lr 0.00036\n",
      "iteration 65300 training loss 0.6744025 lr 0.00036\n",
      "iteration 65400 training loss 0.7885872 lr 0.00036\n",
      "iteration 65500 training loss 0.68410826 lr 0.00036\n",
      "iteration 65600 training loss 0.6958029 lr 0.00036\n",
      "iteration 65700 training loss 0.7353955 lr 0.00036\n",
      "iteration 65800 training loss 0.7615343 lr 0.00036\n",
      "iteration 65900 training loss 0.7220439 lr 0.00036\n",
      "iteration 66000 training loss 0.7554256 lr 0.00036\n",
      "iteration 66100 training loss 0.75635964 lr 0.00036\n",
      "iteration 66200 training loss 0.85991544 lr 0.00036\n",
      "iteration 66300 training loss 0.64931434 lr 0.00036\n",
      "iteration 66400 training loss 0.7382679 lr 0.00036\n",
      "iteration 66500 training loss 0.67969584 lr 0.00036\n",
      "iteration 66600 training loss 0.7331733 lr 0.00036\n",
      "iteration 66700 training loss 0.97697455 lr 0.00036\n",
      "iteration 66800 training loss 0.7200031 lr 0.00036\n",
      "iteration 66900 training loss 0.85445255 lr 0.00036\n",
      "iteration 67000 training loss 0.96912074 lr 0.00036\n",
      "iteration 67100 training loss 0.7545968 lr 0.00036\n",
      "iteration 67200 training loss 0.74347186 lr 0.00036\n",
      "iteration 67300 training loss 0.8078301 lr 0.00036\n",
      "iteration 67400 training loss 0.8665214 lr 0.00036\n",
      "iteration 67500 training loss 0.83178055 lr 0.00036\n",
      "iteration 67600 training loss 0.84102786 lr 0.00036\n",
      "iteration 67700 training loss 0.59388226 lr 0.00036\n",
      "iteration 67800 training loss 0.77819455 lr 0.00036\n",
      "iteration 67900 training loss 0.8604525 lr 0.00036\n",
      "iteration 68000 training loss 0.94546044 lr 0.00036\n",
      "iteration 68100 training loss 0.79518193 lr 0.00036\n",
      "iteration 68200 training loss 0.94745725 lr 0.00036\n",
      "iteration 68300 training loss 0.967716 lr 0.00036\n",
      "iteration 68400 training loss 0.9558533 lr 0.00036\n",
      "iteration 68500 training loss 0.75254536 lr 0.00036\n",
      "iteration 68600 training loss 0.8868479 lr 0.00036\n",
      "iteration 68700 training loss 0.8274462 lr 0.00036\n",
      "iteration 68800 training loss 0.8278236 lr 0.00036\n",
      "iteration 68900 training loss 0.6815942 lr 0.00036\n",
      "iteration 69000 training loss 0.69366246 lr 0.00036\n",
      "iteration 69100 training loss 0.6807726 lr 0.00036\n",
      "iteration 69200 training loss 0.922015 lr 0.00036\n",
      "iteration 69300 training loss 0.866025 lr 0.00036\n",
      "iteration 69400 training loss 0.7372647 lr 0.00036\n",
      "iteration 69500 training loss 0.8721633 lr 0.00036\n",
      "iteration 69600 training loss 0.7212525 lr 0.00036\n",
      "iteration 69700 training loss 0.8381046 lr 0.00036\n",
      "iteration 69800 training loss 0.8987432 lr 0.00036\n",
      "iteration 69900 training loss 0.5549487 lr 0.00036\n",
      "iteration 70000 training loss 0.81791687 lr 0.00036\n",
      "iteration 70100 training loss 0.873036 lr 0.00036\n",
      "iteration 70200 training loss 0.6194239 lr 0.00036\n",
      "iteration 70300 training loss 0.77064365 lr 0.00036\n",
      "iteration 70400 training loss 0.75537264 lr 0.00036\n",
      "iteration 70500 training loss 0.8491411 lr 0.00036\n",
      "iteration 70600 training loss 0.64893675 lr 0.00036\n",
      "iteration 70700 training loss 0.62188274 lr 0.00036\n",
      "iteration 70800 training loss 0.79117686 lr 0.00036\n",
      "iteration 70900 training loss 0.7058084 lr 0.00036\n",
      "iteration 71000 training loss 0.8806194 lr 0.00036\n",
      "iteration 71100 training loss 0.76354533 lr 0.00036\n",
      "iteration 71200 training loss 0.9219539 lr 0.00036\n",
      "iteration 71300 training loss 0.7512312 lr 0.00036\n",
      "iteration 71400 training loss 0.7898044 lr 0.00036\n",
      "iteration 71500 training loss 0.56663316 lr 0.00036\n",
      "iteration 71600 training loss 0.62246996 lr 0.00036\n",
      "iteration 71700 training loss 0.78124475 lr 0.00036\n",
      "iteration 71800 training loss 0.58913 lr 0.00036\n",
      "iteration 71900 training loss 0.84515184 lr 0.00036\n",
      "iteration 72000 training loss 0.86371094 lr 0.00036\n",
      "iteration 72100 training loss 0.5798316 lr 0.00036\n",
      "iteration 72200 training loss 0.6343177 lr 0.00036\n",
      "iteration 72300 training loss 0.71903366 lr 0.00036\n",
      "iteration 72400 training loss 0.6737992 lr 0.00036\n",
      "iteration 72500 training loss 0.7381046 lr 0.00036\n",
      "iteration 72600 training loss 0.63393176 lr 0.00036\n",
      "iteration 72700 training loss 0.68331945 lr 0.00036\n",
      "iteration 72800 training loss 0.8912382 lr 0.00036\n",
      "iteration 72900 training loss 0.5865598 lr 0.00036\n",
      "iteration 73000 training loss 0.791389 lr 0.00036\n",
      "iteration 73100 training loss 0.6322756 lr 0.00036\n",
      "iteration 73200 training loss 0.849452 lr 0.00036\n",
      "iteration 73300 training loss 0.69462633 lr 0.00036\n",
      "iteration 73400 training loss 0.8882703 lr 0.00036\n",
      "iteration 73500 training loss 0.8852909 lr 0.00036\n",
      "iteration 73600 training loss 0.61977035 lr 0.00036\n",
      "iteration 73700 training loss 0.5546598 lr 0.00036\n",
      "iteration 73800 training loss 0.8768905 lr 0.00036\n",
      "iteration 73900 training loss 0.7249717 lr 0.00036\n",
      "iteration 74000 training loss 0.8148678 lr 0.00036\n",
      "iteration 74100 training loss 0.85108525 lr 0.00036\n",
      "iteration 74200 training loss 0.68331087 lr 0.00036\n",
      "iteration 74300 training loss 0.8199688 lr 0.00036\n",
      "iteration 74400 training loss 0.741824 lr 0.00036\n",
      "iteration 74500 training loss 0.6336491 lr 0.00036\n",
      "iteration 74600 training loss 0.56588995 lr 0.00036\n",
      "iteration 74700 training loss 0.72986466 lr 0.00036\n",
      "iteration 74800 training loss 0.6884821 lr 0.00036\n",
      "iteration 74900 training loss 0.72901064 lr 0.00036\n",
      "iteration 75000 training loss 0.65706515 lr 0.00036\n",
      "iteration 75100 training loss 0.745886 lr 0.00036\n",
      "iteration 75200 training loss 0.7217486 lr 0.00036\n",
      "iteration 75300 training loss 0.77689344 lr 0.00036\n",
      "iteration 75400 training loss 0.71499443 lr 0.00036\n",
      "iteration 75500 training loss 0.5778512 lr 0.00036\n",
      "iteration 75600 training loss 0.7967037 lr 0.00036\n",
      "iteration 75700 training loss 0.88647264 lr 0.00036\n",
      "iteration 75800 training loss 0.7371062 lr 0.00036\n",
      "iteration 75900 training loss 0.815734 lr 0.00036\n",
      "iteration 76000 training loss 0.8696952 lr 0.00036\n",
      "iteration 76100 training loss 0.8353222 lr 0.00036\n",
      "iteration 76200 training loss 0.76672775 lr 0.00036\n",
      "iteration 76300 training loss 0.8584011 lr 0.00036\n",
      "iteration 76400 training loss 0.8649892 lr 0.00036\n",
      "iteration 76500 training loss 0.7905681 lr 0.00036\n",
      "iteration 76600 training loss 0.72561693 lr 0.00036\n",
      "iteration 76700 training loss 0.8095844 lr 0.00036\n",
      "iteration 76800 training loss 0.7628805 lr 0.00036\n",
      "iteration 76900 training loss 0.80566657 lr 0.00036\n",
      "iteration 77000 training loss 0.8310667 lr 0.00036\n",
      "iteration 77100 training loss 0.8344262 lr 0.00036\n",
      "iteration 77200 training loss 0.7781058 lr 0.00036\n",
      "iteration 77300 training loss 0.78372717 lr 0.00036\n",
      "iteration 77400 training loss 0.6973534 lr 0.00036\n",
      "iteration 77500 training loss 0.78433645 lr 0.00036\n",
      "iteration 77600 training loss 0.8189202 lr 0.00036\n",
      "iteration 77700 training loss 0.68083024 lr 0.00036\n",
      "iteration 77800 training loss 0.63081264 lr 0.00036\n",
      "iteration 77900 training loss 0.7240718 lr 0.00036\n",
      "iteration 78000 training loss 1.0889924 lr 0.00036\n",
      "iteration 78100 training loss 0.6001835 lr 0.00036\n",
      "iteration 78200 training loss 0.8115791 lr 0.00036\n",
      "iteration 78300 training loss 0.67386526 lr 0.00036\n",
      "iteration 78400 training loss 0.85683984 lr 0.00036\n",
      "iteration 78500 training loss 0.75011975 lr 0.00036\n",
      "iteration 78600 training loss 0.7711319 lr 0.00036\n",
      "iteration 78700 training loss 0.57887423 lr 0.00036\n",
      "iteration 78800 training loss 0.8009518 lr 0.00036\n",
      "iteration 78900 training loss 0.74040496 lr 0.00036\n",
      "iteration 79000 training loss 0.6745816 lr 0.00036\n",
      "iteration 79100 training loss 0.797471 lr 0.00036\n",
      "iteration 79200 training loss 0.7212405 lr 0.00036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 79300 training loss 0.8525893 lr 0.00036\n",
      "iteration 79400 training loss 0.73486894 lr 0.00036\n",
      "iteration 79500 training loss 0.6928071 lr 0.00036\n",
      "iteration 79600 training loss 0.64413804 lr 0.00036\n",
      "iteration 79700 training loss 0.6218603 lr 0.00036\n",
      "iteration 79800 training loss 0.7192797 lr 0.00036\n",
      "iteration 79900 training loss 0.5968606 lr 0.00036\n",
      "iteration 80000 training loss 0.67919624 lr 0.00036\n",
      "layout:nlp:random 0.7884264252047628\n",
      "layout:nlp:default 0.3855141869785855\n",
      "layout:xla:random 0.4329559452336573\n",
      "layout:xla:default 0.16663401447477483\n",
      "epoch 0, it 80000 validation loss -0.443\n",
      "iteration 80100 training loss 0.7369029 lr 0.00033\n",
      "iteration 80200 training loss 0.76074547 lr 0.00033\n",
      "iteration 80300 training loss 0.717153 lr 0.00033\n",
      "iteration 80400 training loss 0.73582625 lr 0.00033\n",
      "iteration 80500 training loss 0.72074324 lr 0.00033\n",
      "iteration 80600 training loss 0.80158156 lr 0.00033\n",
      "iteration 80700 training loss 0.5779217 lr 0.00033\n",
      "iteration 80800 training loss 0.89057696 lr 0.00033\n",
      "iteration 80900 training loss 0.7027125 lr 0.00033\n",
      "iteration 81000 training loss 0.8398382 lr 0.00033\n",
      "iteration 81100 training loss 0.92427397 lr 0.00033\n",
      "iteration 81200 training loss 0.8393518 lr 0.00033\n",
      "iteration 81300 training loss 0.72951865 lr 0.00033\n",
      "iteration 81400 training loss 0.8186268 lr 0.00033\n",
      "iteration 81500 training loss 0.8201942 lr 0.00033\n",
      "iteration 81600 training loss 0.68424374 lr 0.00033\n",
      "iteration 81700 training loss 0.5831684 lr 0.00033\n",
      "iteration 81800 training loss 0.8832581 lr 0.00033\n",
      "iteration 81900 training loss 0.9518769 lr 0.00033\n",
      "iteration 82000 training loss 0.84067047 lr 0.00033\n",
      "iteration 82100 training loss 0.7363217 lr 0.00033\n",
      "iteration 82200 training loss 0.8470745 lr 0.00033\n",
      "iteration 82300 training loss 0.7651491 lr 0.00033\n",
      "iteration 82400 training loss 0.75406873 lr 0.00033\n",
      "iteration 82500 training loss 0.87448514 lr 0.00033\n",
      "iteration 82600 training loss 0.7812027 lr 0.00033\n",
      "iteration 82700 training loss 0.59188527 lr 0.00033\n",
      "iteration 82800 training loss 0.8187863 lr 0.00033\n",
      "iteration 82900 training loss 0.60462946 lr 0.00033\n",
      "iteration 83000 training loss 0.6775911 lr 0.00033\n",
      "iteration 83100 training loss 0.79908687 lr 0.00033\n",
      "iteration 83200 training loss 0.72568524 lr 0.00033\n",
      "iteration 83300 training loss 0.7433172 lr 0.00033\n",
      "iteration 83400 training loss 0.7990885 lr 0.00033\n",
      "iteration 83500 training loss 0.9196491 lr 0.00033\n",
      "iteration 83600 training loss 0.84898525 lr 0.00033\n",
      "iteration 83700 training loss 0.79862744 lr 0.00033\n",
      "iteration 83800 training loss 0.56566304 lr 0.00033\n",
      "iteration 83900 training loss 0.9969592 lr 0.00033\n",
      "iteration 84000 training loss 0.87967175 lr 0.00033\n",
      "iteration 84100 training loss 0.941915 lr 0.00033\n",
      "iteration 84200 training loss 0.6240772 lr 0.00033\n",
      "iteration 84300 training loss 0.61166096 lr 0.00033\n",
      "iteration 84400 training loss 0.711661 lr 0.00033\n",
      "iteration 84500 training loss 0.7796883 lr 0.00033\n",
      "iteration 84600 training loss 0.65744555 lr 0.00033\n",
      "iteration 84700 training loss 0.79760015 lr 0.00033\n",
      "iteration 84800 training loss 0.6858551 lr 0.00033\n",
      "iteration 84900 training loss 0.6323326 lr 0.00033\n",
      "iteration 85000 training loss 0.68914956 lr 0.00033\n",
      "iteration 85100 training loss 0.8822696 lr 0.00033\n",
      "iteration 85200 training loss 0.7263345 lr 0.00033\n",
      "iteration 85300 training loss 0.7351452 lr 0.00033\n",
      "iteration 85400 training loss 0.6779789 lr 0.00033\n",
      "iteration 85500 training loss 0.51860535 lr 0.00033\n",
      "iteration 85600 training loss 0.6505988 lr 0.00033\n",
      "iteration 85700 training loss 0.71728426 lr 0.00033\n",
      "iteration 85800 training loss 0.8012875 lr 0.00033\n",
      "iteration 85900 training loss 0.84120303 lr 0.00033\n",
      "iteration 86000 training loss 0.5102672 lr 0.00033\n",
      "iteration 86100 training loss 0.7739145 lr 0.00033\n",
      "iteration 86200 training loss 0.8065997 lr 0.00033\n",
      "iteration 86300 training loss 0.75491494 lr 0.00033\n",
      "iteration 86400 training loss 0.61361575 lr 0.00033\n",
      "iteration 86500 training loss 0.5386805 lr 0.00033\n",
      "iteration 86600 training loss 0.53208625 lr 0.00033\n",
      "iteration 86700 training loss 0.4046293 lr 0.00033\n",
      "iteration 86800 training loss 0.8065752 lr 0.00033\n",
      "iteration 86900 training loss 0.6994518 lr 0.00033\n",
      "iteration 87000 training loss 0.8467019 lr 0.00033\n",
      "iteration 87100 training loss 0.73866063 lr 0.00033\n",
      "iteration 87200 training loss 0.9584232 lr 0.00033\n",
      "iteration 87300 training loss 0.882454 lr 0.00033\n",
      "iteration 87400 training loss 0.5817411 lr 0.00033\n",
      "iteration 87500 training loss 0.76840544 lr 0.00033\n",
      "iteration 87600 training loss 0.69572484 lr 0.00033\n",
      "iteration 87700 training loss 0.61935925 lr 0.00033\n",
      "iteration 87800 training loss 0.6175205 lr 0.00033\n",
      "iteration 87900 training loss 0.76720446 lr 0.00033\n",
      "iteration 88000 training loss 0.7277291 lr 0.00033\n",
      "iteration 88100 training loss 0.7925162 lr 0.00033\n",
      "iteration 88200 training loss 0.79385096 lr 0.00033\n",
      "iteration 88300 training loss 0.72514415 lr 0.00033\n",
      "iteration 88400 training loss 0.7241902 lr 0.00033\n",
      "iteration 88500 training loss 0.7530946 lr 0.00033\n",
      "iteration 88600 training loss 0.7473567 lr 0.00033\n",
      "iteration 88700 training loss 0.6201917 lr 0.00033\n",
      "iteration 88800 training loss 0.73713946 lr 0.00033\n",
      "iteration 88900 training loss 0.8228667 lr 0.00033\n",
      "iteration 89000 training loss 0.7036849 lr 0.00033\n",
      "iteration 89100 training loss 0.79475814 lr 0.00033\n",
      "iteration 89200 training loss 0.7067467 lr 0.00033\n",
      "iteration 89300 training loss 0.7512056 lr 0.00033\n",
      "iteration 89400 training loss 0.67944485 lr 0.00033\n",
      "iteration 89500 training loss 0.8546274 lr 0.00033\n",
      "iteration 89600 training loss 0.6459149 lr 0.00033\n",
      "iteration 89700 training loss 0.58907276 lr 0.00033\n",
      "iteration 89800 training loss 0.6968917 lr 0.00033\n",
      "iteration 89900 training loss 0.71763885 lr 0.00033\n",
      "iteration 90000 training loss 0.97096246 lr 0.00033\n",
      "iteration 90100 training loss 0.65058863 lr 0.00033\n",
      "iteration 90200 training loss 0.72813594 lr 0.00033\n",
      "iteration 90300 training loss 0.5954631 lr 0.00033\n",
      "iteration 90400 training loss 0.90578276 lr 0.00033\n",
      "iteration 90500 training loss 0.89392525 lr 0.00033\n",
      "iteration 90600 training loss 0.88004625 lr 0.00033\n",
      "iteration 90700 training loss 0.7758166 lr 0.00033\n",
      "iteration 90800 training loss 0.9009866 lr 0.00033\n",
      "iteration 90900 training loss 0.5307528 lr 0.00033\n",
      "iteration 91000 training loss 0.80712533 lr 0.00033\n",
      "iteration 91100 training loss 0.7746027 lr 0.00033\n",
      "iteration 91200 training loss 0.7841769 lr 0.00033\n",
      "iteration 91300 training loss 0.6865859 lr 0.00033\n",
      "iteration 91400 training loss 0.70511806 lr 0.00033\n",
      "iteration 91500 training loss 0.6984182 lr 0.00033\n",
      "iteration 91600 training loss 0.8321394 lr 0.00033\n",
      "iteration 91700 training loss 0.67525 lr 0.00033\n",
      "iteration 91800 training loss 0.5503613 lr 0.00033\n",
      "iteration 91900 training loss 0.91155577 lr 0.00033\n",
      "iteration 92000 training loss 0.66853577 lr 0.00033\n",
      "iteration 92100 training loss 0.68679357 lr 0.00033\n",
      "iteration 92200 training loss 0.5260609 lr 0.00033\n",
      "iteration 92300 training loss 0.70075405 lr 0.00033\n",
      "iteration 92400 training loss 0.9121278 lr 0.00033\n",
      "iteration 92500 training loss 0.79148054 lr 0.00033\n",
      "iteration 92600 training loss 0.84748846 lr 0.00033\n",
      "iteration 92700 training loss 0.69334143 lr 0.00033\n",
      "iteration 92800 training loss 0.8258285 lr 0.00033\n",
      "iteration 92900 training loss 0.8202136 lr 0.00033\n",
      "iteration 93000 training loss 0.83557403 lr 0.00033\n",
      "iteration 93100 training loss 0.8690756 lr 0.00033\n",
      "iteration 93200 training loss 0.755136 lr 0.00033\n",
      "iteration 93300 training loss 0.6703121 lr 0.00033\n",
      "iteration 93400 training loss 0.72627527 lr 0.00033\n",
      "iteration 93500 training loss 0.6278601 lr 0.00033\n",
      "iteration 93600 training loss 0.639387 lr 0.00033\n",
      "iteration 93700 training loss 0.78880394 lr 0.00033\n",
      "iteration 93800 training loss 0.9325887 lr 0.00033\n",
      "iteration 93900 training loss 0.77539706 lr 0.00033\n",
      "iteration 94000 training loss 0.83563536 lr 0.00033\n",
      "iteration 94100 training loss 0.54316545 lr 0.00033\n",
      "iteration 94200 training loss 0.7973006 lr 0.00033\n",
      "iteration 94300 training loss 0.88680553 lr 0.00033\n",
      "iteration 94400 training loss 0.6708163 lr 0.00033\n",
      "iteration 94500 training loss 0.9110812 lr 0.00033\n",
      "iteration 94600 training loss 0.6263484 lr 0.00033\n",
      "iteration 94700 training loss 0.8214892 lr 0.00033\n",
      "iteration 94800 training loss 0.7632206 lr 0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 94900 training loss 0.90175587 lr 0.00033\n",
      "iteration 95000 training loss 0.7360443 lr 0.00033\n",
      "iteration 95100 training loss 0.5862333 lr 0.00033\n",
      "iteration 95200 training loss 0.739223 lr 0.00033\n",
      "iteration 95300 training loss 0.56813776 lr 0.00033\n",
      "iteration 95400 training loss 0.80422676 lr 0.00033\n",
      "iteration 95500 training loss 0.66996 lr 0.00033\n",
      "iteration 95600 training loss 0.6576758 lr 0.00033\n",
      "iteration 95700 training loss 1.2733235 lr 0.00033\n",
      "iteration 95800 training loss 0.6254215 lr 0.00033\n",
      "iteration 95900 training loss 0.66243047 lr 0.00033\n",
      "iteration 96000 training loss 0.6916873 lr 0.00033\n",
      "iteration 96100 training loss 0.8177258 lr 0.00033\n",
      "iteration 96200 training loss 0.8908639 lr 0.00033\n",
      "iteration 96300 training loss 0.73088765 lr 0.00033\n",
      "iteration 96400 training loss 0.57737523 lr 0.00033\n",
      "iteration 96500 training loss 1.0585313 lr 0.00033\n",
      "iteration 96600 training loss 0.8931962 lr 0.00033\n",
      "iteration 96700 training loss 0.7209979 lr 0.00033\n",
      "iteration 96800 training loss 0.62920964 lr 0.00033\n",
      "iteration 96900 training loss 0.85640806 lr 0.00033\n",
      "iteration 97000 training loss 0.97963965 lr 0.00033\n",
      "iteration 97100 training loss 0.85530156 lr 0.00033\n",
      "iteration 97200 training loss 0.84810054 lr 0.00033\n",
      "iteration 97300 training loss 0.9093159 lr 0.00033\n",
      "iteration 97400 training loss 0.85626036 lr 0.00033\n",
      "iteration 97500 training loss 0.8477469 lr 0.00033\n",
      "iteration 97600 training loss 0.7276248 lr 0.00033\n",
      "iteration 97700 training loss 0.7530159 lr 0.00033\n",
      "iteration 97800 training loss 0.65395135 lr 0.00033\n",
      "iteration 97900 training loss 0.6864934 lr 0.00033\n",
      "iteration 98000 training loss 0.6220275 lr 0.00033\n",
      "iteration 98100 training loss 0.6165513 lr 0.00033\n",
      "iteration 98200 training loss 0.7070297 lr 0.00033\n",
      "iteration 98300 training loss 0.86193126 lr 0.00033\n",
      "iteration 98400 training loss 0.9009967 lr 0.00033\n",
      "iteration 98500 training loss 0.67021745 lr 0.00033\n",
      "iteration 98600 training loss 0.7266514 lr 0.00033\n",
      "iteration 98700 training loss 0.8706402 lr 0.00033\n",
      "iteration 98800 training loss 0.6151383 lr 0.00033\n",
      "iteration 98900 training loss 0.838111 lr 0.00033\n",
      "iteration 99000 training loss 0.61569315 lr 0.00033\n",
      "iteration 99100 training loss 0.8227589 lr 0.00033\n",
      "iteration 99200 training loss 0.6518468 lr 0.00033\n",
      "iteration 99300 training loss 0.6574954 lr 0.00033\n",
      "iteration 99400 training loss 0.8294419 lr 0.00033\n",
      "iteration 99500 training loss 0.5495617 lr 0.00033\n",
      "iteration 99600 training loss 0.71955544 lr 0.00033\n",
      "iteration 99700 training loss 0.7606344 lr 0.00033\n",
      "iteration 99800 training loss 0.7058279 lr 0.00033\n",
      "iteration 99900 training loss 0.5926313 lr 0.00033\n",
      "iteration 100000 training loss 0.7666237 lr 0.00033\n",
      "layout:nlp:random 0.7936152315898135\n",
      "layout:nlp:default 0.4007994259629446\n",
      "layout:xla:random 0.4195973135644247\n",
      "layout:xla:default 0.13973108863526254\n",
      "epoch 0, it 100000 validation loss -0.438\n",
      "iteration 100100 training loss 0.47102726 lr 0.00030\n",
      "iteration 100200 training loss 0.6903034 lr 0.00030\n",
      "iteration 100300 training loss 0.65911436 lr 0.00030\n",
      "iteration 100400 training loss 0.67363673 lr 0.00030\n",
      "iteration 100500 training loss 0.7108393 lr 0.00030\n",
      "iteration 100600 training loss 0.7715701 lr 0.00030\n",
      "iteration 100700 training loss 0.71658164 lr 0.00030\n",
      "iteration 100800 training loss 0.69824034 lr 0.00030\n",
      "iteration 100900 training loss 0.4842439 lr 0.00030\n",
      "iteration 101000 training loss 0.69472086 lr 0.00030\n",
      "iteration 101100 training loss 0.6971864 lr 0.00030\n",
      "iteration 101200 training loss 0.70654905 lr 0.00030\n",
      "iteration 101300 training loss 0.6159396 lr 0.00030\n",
      "iteration 101400 training loss 0.7484325 lr 0.00030\n",
      "iteration 101500 training loss 0.7448184 lr 0.00030\n",
      "iteration 101600 training loss 0.72956026 lr 0.00030\n",
      "iteration 101700 training loss 0.85094154 lr 0.00030\n",
      "iteration 101800 training loss 0.8261345 lr 0.00030\n",
      "iteration 101900 training loss 0.7035619 lr 0.00030\n",
      "iteration 102000 training loss 0.74000597 lr 0.00030\n",
      "iteration 102100 training loss 0.7761058 lr 0.00030\n",
      "iteration 102200 training loss 0.6092819 lr 0.00030\n",
      "iteration 102300 training loss 0.80991155 lr 0.00030\n",
      "iteration 102400 training loss 0.62123317 lr 0.00030\n",
      "iteration 102500 training loss 0.89492095 lr 0.00030\n",
      "iteration 102600 training loss 0.74027175 lr 0.00030\n",
      "iteration 102700 training loss 0.92037 lr 0.00030\n",
      "iteration 102800 training loss 0.78871894 lr 0.00030\n",
      "iteration 102900 training loss 0.763775 lr 0.00030\n",
      "iteration 103000 training loss 0.73479164 lr 0.00030\n",
      "iteration 103100 training loss 0.83082783 lr 0.00030\n",
      "iteration 103200 training loss 0.7029733 lr 0.00030\n",
      "iteration 103300 training loss 0.7296026 lr 0.00030\n",
      "iteration 103400 training loss 0.6036885 lr 0.00030\n",
      "iteration 103500 training loss 0.66088444 lr 0.00030\n",
      "iteration 103600 training loss 0.7206217 lr 0.00030\n",
      "iteration 103700 training loss 0.8005387 lr 0.00030\n",
      "iteration 103800 training loss 0.6824384 lr 0.00030\n",
      "iteration 103900 training loss 0.7493523 lr 0.00030\n",
      "iteration 104000 training loss 0.69820696 lr 0.00030\n",
      "iteration 104100 training loss 0.70457894 lr 0.00030\n",
      "iteration 104200 training loss 0.80186045 lr 0.00030\n",
      "iteration 104300 training loss 0.8025747 lr 0.00030\n",
      "iteration 104400 training loss 0.74194366 lr 0.00030\n",
      "iteration 104500 training loss 0.8556827 lr 0.00030\n",
      "iteration 104600 training loss 0.6980004 lr 0.00030\n",
      "iteration 104700 training loss 0.78410727 lr 0.00030\n",
      "iteration 104800 training loss 0.70849234 lr 0.00030\n",
      "iteration 104900 training loss 0.68412596 lr 0.00030\n",
      "iteration 105000 training loss 0.8845247 lr 0.00030\n",
      "iteration 105100 training loss 0.67043424 lr 0.00030\n",
      "iteration 105200 training loss 0.7640453 lr 0.00030\n",
      "iteration 105300 training loss 0.6791998 lr 0.00030\n",
      "iteration 105400 training loss 0.8330131 lr 0.00030\n",
      "iteration 105500 training loss 0.8471714 lr 0.00030\n",
      "iteration 105600 training loss 0.6880385 lr 0.00030\n",
      "iteration 105700 training loss 0.8130993 lr 0.00030\n",
      "iteration 105800 training loss 0.57262486 lr 0.00030\n",
      "iteration 105900 training loss 0.8823282 lr 0.00030\n",
      "iteration 106000 training loss 0.67106605 lr 0.00030\n",
      "iteration 106100 training loss 0.64632213 lr 0.00030\n",
      "iteration 106200 training loss 0.84080255 lr 0.00030\n",
      "iteration 106300 training loss 0.77028954 lr 0.00030\n",
      "iteration 106400 training loss 0.66026014 lr 0.00030\n",
      "iteration 106500 training loss 0.82379997 lr 0.00030\n",
      "iteration 106600 training loss 0.7498205 lr 0.00030\n",
      "iteration 106700 training loss 0.81535226 lr 0.00030\n",
      "iteration 106800 training loss 0.6858263 lr 0.00030\n",
      "iteration 106900 training loss 0.7428985 lr 0.00030\n",
      "iteration 107000 training loss 0.74036276 lr 0.00030\n",
      "iteration 107100 training loss 0.6420229 lr 0.00030\n",
      "iteration 107200 training loss 0.7161367 lr 0.00030\n",
      "iteration 107300 training loss 0.7446901 lr 0.00030\n",
      "iteration 107400 training loss 0.63093215 lr 0.00030\n",
      "iteration 107500 training loss 0.671673 lr 0.00030\n",
      "iteration 107600 training loss 0.7412257 lr 0.00030\n",
      "iteration 107700 training loss 0.7183247 lr 0.00030\n",
      "iteration 107800 training loss 0.71121436 lr 0.00030\n",
      "iteration 107900 training loss 0.5053803 lr 0.00030\n",
      "iteration 108000 training loss 0.5834699 lr 0.00030\n",
      "iteration 108100 training loss 0.75282776 lr 0.00030\n",
      "iteration 108200 training loss 0.72623605 lr 0.00030\n",
      "iteration 108300 training loss 0.8058488 lr 0.00030\n",
      "iteration 108400 training loss 0.66019076 lr 0.00030\n",
      "iteration 108500 training loss 0.7977741 lr 0.00030\n",
      "iteration 108600 training loss 0.7685043 lr 0.00030\n",
      "iteration 108700 training loss 0.6135468 lr 0.00030\n",
      "iteration 108800 training loss 0.88023955 lr 0.00030\n",
      "iteration 108900 training loss 0.7525957 lr 0.00030\n",
      "iteration 109000 training loss 0.6104819 lr 0.00030\n",
      "iteration 109100 training loss 0.7316065 lr 0.00030\n",
      "iteration 109200 training loss 0.7220418 lr 0.00030\n",
      "iteration 109300 training loss 0.65605634 lr 0.00030\n",
      "iteration 109400 training loss 0.65783745 lr 0.00030\n",
      "iteration 109500 training loss 0.6964928 lr 0.00030\n",
      "iteration 109600 training loss 0.8538256 lr 0.00030\n",
      "iteration 109700 training loss 0.76898694 lr 0.00030\n",
      "iteration 109800 training loss 0.80037284 lr 0.00030\n",
      "iteration 109900 training loss 0.79810655 lr 0.00030\n",
      "iteration 110000 training loss 0.7819519 lr 0.00030\n",
      "iteration 110100 training loss 0.81707466 lr 0.00030\n",
      "iteration 110200 training loss 0.8399893 lr 0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 110300 training loss 0.6529971 lr 0.00030\n",
      "iteration 110400 training loss 0.7976926 lr 0.00030\n",
      "iteration 110500 training loss 0.72445923 lr 0.00030\n",
      "iteration 110600 training loss 0.64263064 lr 0.00030\n",
      "iteration 110700 training loss 0.7376076 lr 0.00030\n",
      "iteration 110800 training loss 0.5641546 lr 0.00030\n",
      "iteration 110900 training loss 0.55696344 lr 0.00030\n",
      "iteration 111000 training loss 0.60368836 lr 0.00030\n",
      "iteration 111100 training loss 0.6288745 lr 0.00030\n",
      "iteration 111200 training loss 0.6324744 lr 0.00030\n",
      "iteration 111300 training loss 0.8384634 lr 0.00030\n",
      "iteration 111400 training loss 0.5679785 lr 0.00030\n",
      "iteration 111500 training loss 0.803193 lr 0.00030\n",
      "iteration 111600 training loss 0.7608369 lr 0.00030\n",
      "iteration 111700 training loss 0.6695054 lr 0.00030\n",
      "iteration 111800 training loss 0.68893 lr 0.00030\n",
      "iteration 111900 training loss 0.764591 lr 0.00030\n",
      "iteration 112000 training loss 0.73611057 lr 0.00030\n",
      "iteration 112100 training loss 0.72917646 lr 0.00030\n",
      "iteration 112200 training loss 0.64179343 lr 0.00030\n",
      "iteration 112300 training loss 0.6762861 lr 0.00030\n",
      "iteration 112400 training loss 0.8607563 lr 0.00030\n",
      "iteration 112500 training loss 0.58326477 lr 0.00030\n",
      "iteration 112600 training loss 0.80389535 lr 0.00030\n",
      "iteration 112700 training loss 0.72622144 lr 0.00030\n",
      "iteration 112800 training loss 0.5578739 lr 0.00030\n",
      "iteration 112900 training loss 0.7126384 lr 0.00030\n",
      "iteration 113000 training loss 0.90367824 lr 0.00030\n",
      "iteration 113100 training loss 0.78931344 lr 0.00030\n",
      "iteration 113200 training loss 0.7597269 lr 0.00030\n",
      "iteration 113300 training loss 0.67896193 lr 0.00030\n",
      "iteration 113400 training loss 0.7926605 lr 0.00030\n",
      "iteration 113500 training loss 0.627576 lr 0.00030\n",
      "iteration 113600 training loss 0.67342263 lr 0.00030\n",
      "iteration 113700 training loss 0.798766 lr 0.00030\n",
      "iteration 113800 training loss 0.6853232 lr 0.00030\n",
      "iteration 113900 training loss 0.68081737 lr 0.00030\n",
      "iteration 114000 training loss 0.63280076 lr 0.00030\n",
      "iteration 114100 training loss 0.7613062 lr 0.00030\n",
      "iteration 114200 training loss 0.5271263 lr 0.00030\n",
      "iteration 114300 training loss 0.87313354 lr 0.00030\n",
      "iteration 114400 training loss 0.66318303 lr 0.00030\n",
      "iteration 114500 training loss 0.8358723 lr 0.00030\n",
      "iteration 114600 training loss 0.6767272 lr 0.00030\n",
      "iteration 114700 training loss 0.6100263 lr 0.00030\n",
      "iteration 114800 training loss 0.7959388 lr 0.00030\n",
      "iteration 114900 training loss 0.7558312 lr 0.00030\n",
      "iteration 115000 training loss 0.63946813 lr 0.00030\n",
      "iteration 115100 training loss 0.64547575 lr 0.00030\n",
      "iteration 115200 training loss 0.68587714 lr 0.00030\n",
      "iteration 115300 training loss 0.8370698 lr 0.00030\n",
      "iteration 115400 training loss 0.6409838 lr 0.00030\n",
      "iteration 115500 training loss 0.67571753 lr 0.00030\n",
      "iteration 115600 training loss 0.71304035 lr 0.00030\n",
      "iteration 115700 training loss 0.8106196 lr 0.00030\n",
      "iteration 115800 training loss 0.75773275 lr 0.00030\n",
      "iteration 115900 training loss 0.53597724 lr 0.00030\n",
      "iteration 116000 training loss 0.7831229 lr 0.00030\n",
      "iteration 116100 training loss 0.79247576 lr 0.00030\n",
      "iteration 116200 training loss 0.6719688 lr 0.00030\n",
      "iteration 116300 training loss 0.7194732 lr 0.00030\n",
      "iteration 116400 training loss 0.85723644 lr 0.00030\n",
      "iteration 116500 training loss 0.9013308 lr 0.00030\n",
      "iteration 116600 training loss 0.61979103 lr 0.00030\n",
      "iteration 116700 training loss 0.7672597 lr 0.00030\n",
      "iteration 116800 training loss 0.7107534 lr 0.00030\n",
      "iteration 116900 training loss 0.8050647 lr 0.00030\n",
      "iteration 117000 training loss 0.7780624 lr 0.00030\n",
      "iteration 117100 training loss 0.63200647 lr 0.00030\n",
      "iteration 117200 training loss 0.6929699 lr 0.00030\n",
      "iteration 117300 training loss 0.72323585 lr 0.00030\n",
      "iteration 117400 training loss 0.7705518 lr 0.00030\n",
      "iteration 117500 training loss 0.8117575 lr 0.00030\n",
      "iteration 117600 training loss 0.58568543 lr 0.00030\n",
      "iteration 117700 training loss 0.60730034 lr 0.00030\n",
      "iteration 117800 training loss 0.57025623 lr 0.00030\n",
      "iteration 117900 training loss 0.7184604 lr 0.00030\n",
      "iteration 118000 training loss 0.8219477 lr 0.00030\n",
      "iteration 118100 training loss 0.70147645 lr 0.00030\n",
      "iteration 118200 training loss 0.7190964 lr 0.00030\n",
      "iteration 118300 training loss 0.64001 lr 0.00030\n",
      "iteration 118400 training loss 0.73699635 lr 0.00030\n",
      "iteration 118500 training loss 0.73909265 lr 0.00030\n",
      "iteration 118600 training loss 0.8457319 lr 0.00030\n",
      "iteration 118700 training loss 0.86990905 lr 0.00030\n",
      "iteration 118800 training loss 0.91870934 lr 0.00030\n",
      "iteration 118900 training loss 0.9594453 lr 0.00030\n",
      "iteration 119000 training loss 0.48152918 lr 0.00030\n",
      "iteration 119100 training loss 0.674526 lr 0.00030\n",
      "iteration 119200 training loss 0.9513306 lr 0.00030\n",
      "iteration 119300 training loss 0.80911773 lr 0.00030\n",
      "iteration 119400 training loss 0.7193998 lr 0.00030\n",
      "iteration 119500 training loss 0.81965303 lr 0.00030\n",
      "iteration 119600 training loss 0.6105545 lr 0.00030\n",
      "iteration 119700 training loss 0.76344913 lr 0.00030\n",
      "iteration 119800 training loss 0.87317044 lr 0.00030\n",
      "iteration 119900 training loss 0.76647675 lr 0.00030\n",
      "iteration 120000 training loss 0.6998902 lr 0.00030\n",
      "layout:nlp:random 0.7918208072746427\n",
      "layout:nlp:default 0.3927346401309005\n",
      "layout:xla:random 0.4417997191388499\n",
      "layout:xla:default 0.19220643841546728\n",
      "epoch 0, it 120000 validation loss -0.455\n",
      "iteration 120100 training loss 0.7038599 lr 0.00027\n",
      "iteration 120200 training loss 0.7225895 lr 0.00027\n",
      "iteration 120300 training loss 0.754748 lr 0.00027\n",
      "iteration 120400 training loss 0.69249123 lr 0.00027\n",
      "iteration 120500 training loss 0.76254183 lr 0.00027\n",
      "iteration 120600 training loss 0.82393384 lr 0.00027\n",
      "iteration 120700 training loss 0.67787373 lr 0.00027\n",
      "iteration 120800 training loss 0.6966152 lr 0.00027\n",
      "iteration 120900 training loss 0.6166216 lr 0.00027\n",
      "iteration 121000 training loss 0.788624 lr 0.00027\n",
      "iteration 121100 training loss 0.69661003 lr 0.00027\n",
      "iteration 121200 training loss 0.742719 lr 0.00027\n",
      "iteration 121300 training loss 0.67973995 lr 0.00027\n",
      "iteration 121400 training loss 0.7900959 lr 0.00027\n",
      "iteration 121500 training loss 0.7558939 lr 0.00027\n",
      "iteration 121600 training loss 0.66775143 lr 0.00027\n",
      "iteration 121700 training loss 0.75089914 lr 0.00027\n",
      "iteration 121800 training loss 0.6660855 lr 0.00027\n",
      "iteration 121900 training loss 0.9532601 lr 0.00027\n",
      "iteration 122000 training loss 0.74294096 lr 0.00027\n",
      "iteration 122100 training loss 0.58923584 lr 0.00027\n",
      "iteration 122200 training loss 0.77943414 lr 0.00027\n",
      "iteration 122300 training loss 0.7034964 lr 0.00027\n",
      "iteration 122400 training loss 0.76030797 lr 0.00027\n",
      "iteration 122500 training loss 0.67637867 lr 0.00027\n",
      "iteration 122600 training loss 0.893882 lr 0.00027\n",
      "iteration 122700 training loss 0.735371 lr 0.00027\n",
      "iteration 122800 training loss 0.58700687 lr 0.00027\n",
      "iteration 122900 training loss 0.8572203 lr 0.00027\n",
      "iteration 123000 training loss 0.4771853 lr 0.00027\n",
      "iteration 123100 training loss 0.6949572 lr 0.00027\n",
      "iteration 123200 training loss 0.5995933 lr 0.00027\n",
      "iteration 123300 training loss 0.78733826 lr 0.00027\n",
      "iteration 123400 training loss 0.6750652 lr 0.00027\n",
      "iteration 123500 training loss 0.5333629 lr 0.00027\n",
      "iteration 123600 training loss 0.76945585 lr 0.00027\n",
      "iteration 123700 training loss 0.63487047 lr 0.00027\n",
      "iteration 123800 training loss 0.7596782 lr 0.00027\n",
      "iteration 123900 training loss 0.610406 lr 0.00027\n",
      "iteration 124000 training loss 0.7653455 lr 0.00027\n",
      "iteration 124100 training loss 0.6254119 lr 0.00027\n",
      "iteration 124200 training loss 0.54825824 lr 0.00027\n",
      "iteration 124300 training loss 0.8771477 lr 0.00027\n",
      "iteration 124400 training loss 0.7347207 lr 0.00027\n",
      "iteration 124500 training loss 0.7581118 lr 0.00027\n",
      "iteration 124600 training loss 0.7098141 lr 0.00027\n",
      "iteration 124700 training loss 0.8076684 lr 0.00027\n",
      "iteration 124800 training loss 0.57953304 lr 0.00027\n",
      "iteration 124900 training loss 0.80801666 lr 0.00027\n",
      "iteration 125000 training loss 0.9429914 lr 0.00027\n",
      "iteration 125100 training loss 0.75266045 lr 0.00027\n",
      "iteration 125200 training loss 0.81973225 lr 0.00027\n",
      "iteration 125300 training loss 0.8450346 lr 0.00027\n",
      "iteration 125400 training loss 0.95712626 lr 0.00027\n",
      "iteration 125500 training loss 0.695729 lr 0.00027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 125600 training loss 0.74666965 lr 0.00027\n",
      "iteration 125700 training loss 0.64421153 lr 0.00027\n",
      "iteration 125800 training loss 1.0233527 lr 0.00027\n",
      "iteration 125900 training loss 0.8877452 lr 0.00027\n",
      "iteration 126000 training loss 0.69830245 lr 0.00027\n",
      "iteration 126100 training loss 0.78934777 lr 0.00027\n",
      "iteration 126200 training loss 0.86654705 lr 0.00027\n",
      "iteration 126300 training loss 0.86471313 lr 0.00027\n",
      "iteration 126400 training loss 0.84529567 lr 0.00027\n",
      "iteration 126500 training loss 0.8860013 lr 0.00027\n",
      "iteration 126600 training loss 0.7389257 lr 0.00027\n",
      "iteration 126700 training loss 0.7810896 lr 0.00027\n",
      "iteration 126800 training loss 0.7242737 lr 0.00027\n",
      "iteration 126900 training loss 0.68742824 lr 0.00027\n",
      "iteration 127000 training loss 0.66637504 lr 0.00027\n",
      "iteration 127100 training loss 0.8150722 lr 0.00027\n",
      "iteration 127200 training loss 0.6029477 lr 0.00027\n",
      "iteration 127300 training loss 0.54177886 lr 0.00027\n",
      "iteration 127400 training loss 0.5999653 lr 0.00027\n",
      "iteration 127500 training loss 0.77381814 lr 0.00027\n",
      "iteration 127600 training loss 0.72158945 lr 0.00027\n",
      "iteration 127700 training loss 0.7280403 lr 0.00027\n",
      "iteration 127800 training loss 0.60129607 lr 0.00027\n",
      "iteration 127900 training loss 0.76514256 lr 0.00027\n",
      "iteration 128000 training loss 0.55636156 lr 0.00027\n",
      "iteration 128100 training loss 0.5938076 lr 0.00027\n",
      "iteration 128200 training loss 0.66739935 lr 0.00027\n",
      "iteration 128300 training loss 0.70061535 lr 0.00027\n",
      "iteration 128400 training loss 0.70044357 lr 0.00027\n",
      "iteration 128500 training loss 0.64474624 lr 0.00027\n",
      "iteration 128600 training loss 0.64209855 lr 0.00027\n",
      "iteration 128700 training loss 0.8293022 lr 0.00027\n",
      "iteration 128800 training loss 0.6733205 lr 0.00027\n",
      "iteration 128900 training loss 0.54505897 lr 0.00027\n",
      "iteration 129000 training loss 0.58383244 lr 0.00027\n",
      "iteration 129100 training loss 0.5704716 lr 0.00027\n",
      "iteration 129200 training loss 0.58376867 lr 0.00027\n",
      "iteration 129300 training loss 0.5123754 lr 0.00027\n",
      "iteration 129400 training loss 0.58768904 lr 0.00027\n",
      "iteration 129500 training loss 0.5771468 lr 0.00027\n",
      "iteration 129600 training loss 0.48856235 lr 0.00027\n",
      "iteration 129700 training loss 0.6331939 lr 0.00027\n",
      "iteration 129800 training loss 0.50893724 lr 0.00027\n",
      "iteration 129900 training loss 0.57385993 lr 0.00027\n",
      "iteration 130000 training loss 0.58029777 lr 0.00027\n",
      "iteration 130100 training loss 0.72795755 lr 0.00027\n",
      "iteration 130200 training loss 0.65143585 lr 0.00027\n",
      "iteration 130300 training loss 0.72731227 lr 0.00027\n",
      "iteration 130400 training loss 0.7311503 lr 0.00027\n",
      "iteration 130500 training loss 0.82715964 lr 0.00027\n",
      "iteration 130600 training loss 0.5482774 lr 0.00027\n",
      "iteration 130700 training loss 0.80271804 lr 0.00027\n",
      "iteration 130800 training loss 0.6340844 lr 0.00027\n",
      "iteration 130900 training loss 0.8318235 lr 0.00027\n",
      "iteration 131000 training loss 0.55753386 lr 0.00027\n",
      "iteration 131100 training loss 0.65668947 lr 0.00027\n",
      "iteration 131200 training loss 0.7480341 lr 0.00027\n",
      "iteration 131300 training loss 0.7161056 lr 0.00027\n",
      "iteration 131400 training loss 0.6791034 lr 0.00027\n",
      "iteration 131500 training loss 0.6160093 lr 0.00027\n",
      "iteration 131600 training loss 0.6553213 lr 0.00027\n",
      "iteration 131700 training loss 0.66358334 lr 0.00027\n",
      "iteration 131800 training loss 0.64252037 lr 0.00027\n",
      "iteration 131900 training loss 0.7531698 lr 0.00027\n",
      "iteration 132000 training loss 0.6043547 lr 0.00027\n",
      "iteration 132100 training loss 0.7439359 lr 0.00027\n",
      "iteration 132200 training loss 0.5122203 lr 0.00027\n",
      "iteration 132300 training loss 0.8008695 lr 0.00027\n",
      "iteration 132400 training loss 0.6653973 lr 0.00027\n",
      "iteration 132500 training loss 0.54049295 lr 0.00027\n",
      "iteration 132600 training loss 0.71557945 lr 0.00027\n",
      "iteration 132700 training loss 0.6557675 lr 0.00027\n",
      "iteration 132800 training loss 0.56036997 lr 0.00027\n",
      "iteration 132900 training loss 0.74084395 lr 0.00027\n",
      "iteration 133000 training loss 0.7162069 lr 0.00027\n",
      "iteration 133100 training loss 0.7503784 lr 0.00027\n",
      "iteration 133200 training loss 0.8245451 lr 0.00027\n",
      "iteration 133300 training loss 0.7754184 lr 0.00027\n",
      "iteration 133400 training loss 0.6962091 lr 0.00027\n",
      "iteration 133500 training loss 0.73837185 lr 0.00027\n",
      "iteration 133600 training loss 0.677709 lr 0.00027\n",
      "iteration 133700 training loss 0.6557358 lr 0.00027\n",
      "iteration 133800 training loss 0.87538016 lr 0.00027\n",
      "iteration 133900 training loss 0.6958064 lr 0.00027\n",
      "iteration 134000 training loss 0.8804646 lr 0.00027\n",
      "iteration 134100 training loss 0.69805455 lr 0.00027\n",
      "iteration 134200 training loss 0.67810094 lr 0.00027\n",
      "iteration 134300 training loss 0.88616306 lr 0.00027\n",
      "iteration 134400 training loss 0.76296914 lr 0.00027\n",
      "iteration 134500 training loss 0.84318835 lr 0.00027\n",
      "iteration 134600 training loss 0.6770844 lr 0.00027\n",
      "iteration 134700 training loss 0.74021685 lr 0.00027\n",
      "iteration 134800 training loss 0.7430121 lr 0.00027\n",
      "iteration 134900 training loss 0.8018186 lr 0.00027\n",
      "iteration 135000 training loss 0.84532183 lr 0.00027\n",
      "iteration 135100 training loss 0.6567541 lr 0.00027\n",
      "iteration 135200 training loss 0.7556903 lr 0.00027\n",
      "iteration 135300 training loss 0.71774477 lr 0.00027\n",
      "iteration 135400 training loss 0.5367263 lr 0.00027\n",
      "iteration 135500 training loss 0.787556 lr 0.00027\n",
      "iteration 135600 training loss 0.83037776 lr 0.00027\n",
      "iteration 135700 training loss 0.76772827 lr 0.00027\n",
      "iteration 135800 training loss 0.7353534 lr 0.00027\n",
      "iteration 135900 training loss 0.63199145 lr 0.00027\n",
      "iteration 136000 training loss 0.64221114 lr 0.00027\n",
      "iteration 136100 training loss 0.6774867 lr 0.00027\n",
      "iteration 136200 training loss 0.7830066 lr 0.00027\n",
      "iteration 136300 training loss 0.655451 lr 0.00027\n",
      "iteration 136400 training loss 0.785762 lr 0.00027\n",
      "iteration 136500 training loss 0.64248073 lr 0.00027\n",
      "iteration 136600 training loss 0.48888692 lr 0.00027\n",
      "iteration 136700 training loss 0.6811339 lr 0.00027\n",
      "iteration 136800 training loss 0.6383446 lr 0.00027\n",
      "iteration 136900 training loss 0.61129826 lr 0.00027\n",
      "iteration 137000 training loss 0.6859487 lr 0.00027\n",
      "iteration 137100 training loss 0.74312675 lr 0.00027\n",
      "iteration 137200 training loss 0.5768777 lr 0.00027\n",
      "iteration 137300 training loss 0.6152565 lr 0.00027\n",
      "iteration 137400 training loss 0.7017846 lr 0.00027\n",
      "iteration 137500 training loss 0.5978156 lr 0.00027\n",
      "iteration 137600 training loss 0.7391301 lr 0.00027\n",
      "iteration 137700 training loss 0.78519344 lr 0.00027\n",
      "iteration 137800 training loss 0.5988865 lr 0.00027\n",
      "iteration 137900 training loss 1.0340002 lr 0.00027\n",
      "iteration 138000 training loss 0.59014803 lr 0.00027\n",
      "iteration 138100 training loss 0.5331059 lr 0.00027\n",
      "iteration 138200 training loss 0.7214439 lr 0.00027\n",
      "iteration 138300 training loss 0.7547861 lr 0.00027\n",
      "iteration 138400 training loss 0.6030692 lr 0.00027\n",
      "iteration 138500 training loss 0.638162 lr 0.00027\n",
      "iteration 138600 training loss 0.8021178 lr 0.00027\n",
      "iteration 138700 training loss 0.47978604 lr 0.00027\n",
      "iteration 138800 training loss 0.67684823 lr 0.00027\n",
      "iteration 138900 training loss 0.6039168 lr 0.00027\n",
      "iteration 139000 training loss 0.55382395 lr 0.00027\n",
      "iteration 139100 training loss 0.6123008 lr 0.00027\n",
      "iteration 139200 training loss 0.5957935 lr 0.00027\n",
      "iteration 139300 training loss 0.57718575 lr 0.00027\n",
      "iteration 139400 training loss 0.71819794 lr 0.00027\n",
      "iteration 139500 training loss 0.7890473 lr 0.00027\n",
      "iteration 139600 training loss 0.7061018 lr 0.00027\n",
      "iteration 139700 training loss 0.6189342 lr 0.00027\n",
      "iteration 139800 training loss 0.68309593 lr 0.00027\n",
      "iteration 139900 training loss 0.60501397 lr 0.00027\n",
      "iteration 140000 training loss 0.9071186 lr 0.00027\n",
      "layout:nlp:random 0.8046525554993389\n",
      "layout:nlp:default 0.4018535781315641\n",
      "layout:xla:random 0.41567224182705137\n",
      "layout:xla:default 0.18814660172380623\n",
      "epoch 0, it 140000 validation loss -0.453\n",
      "iteration 140100 training loss 0.73032564 lr 0.00024\n",
      "iteration 140200 training loss 0.72234386 lr 0.00024\n",
      "iteration 140300 training loss 0.77892095 lr 0.00024\n",
      "iteration 140400 training loss 0.5317495 lr 0.00024\n",
      "iteration 140500 training loss 0.77598745 lr 0.00024\n",
      "iteration 140600 training loss 0.85366005 lr 0.00024\n",
      "iteration 140700 training loss 0.65393794 lr 0.00024\n",
      "iteration 140800 training loss 0.61318207 lr 0.00024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 140900 training loss 0.51175004 lr 0.00024\n",
      "iteration 141000 training loss 0.70797324 lr 0.00024\n",
      "iteration 141100 training loss 0.84620625 lr 0.00024\n",
      "iteration 141200 training loss 0.4216613 lr 0.00024\n",
      "iteration 141300 training loss 0.7523691 lr 0.00024\n",
      "iteration 141400 training loss 0.60348874 lr 0.00024\n",
      "iteration 141500 training loss 0.611216 lr 0.00024\n",
      "iteration 141600 training loss 0.72240734 lr 0.00024\n",
      "iteration 141700 training loss 0.65903366 lr 0.00024\n",
      "iteration 141800 training loss 0.8893886 lr 0.00024\n",
      "iteration 141900 training loss 0.7082434 lr 0.00024\n",
      "iteration 142000 training loss 0.61687124 lr 0.00024\n",
      "iteration 142100 training loss 0.8328676 lr 0.00024\n",
      "iteration 142200 training loss 0.84053445 lr 0.00024\n",
      "iteration 142300 training loss 0.6858124 lr 0.00024\n",
      "iteration 142400 training loss 0.77474594 lr 0.00024\n",
      "iteration 142500 training loss 0.88518846 lr 0.00024\n",
      "iteration 142600 training loss 0.6391702 lr 0.00024\n",
      "iteration 142700 training loss 0.5773489 lr 0.00024\n",
      "iteration 142800 training loss 0.6664137 lr 0.00024\n",
      "iteration 142900 training loss 0.68754554 lr 0.00024\n",
      "iteration 143000 training loss 0.60123736 lr 0.00024\n",
      "iteration 143100 training loss 0.689901 lr 0.00024\n",
      "iteration 143200 training loss 0.6540871 lr 0.00024\n",
      "iteration 143300 training loss 0.6120446 lr 0.00024\n",
      "iteration 143400 training loss 0.5579284 lr 0.00024\n",
      "iteration 143500 training loss 0.74007547 lr 0.00024\n",
      "iteration 143600 training loss 0.64604425 lr 0.00024\n",
      "iteration 143700 training loss 0.596692 lr 0.00024\n",
      "iteration 143800 training loss 0.71053064 lr 0.00024\n",
      "iteration 143900 training loss 0.6771304 lr 0.00024\n",
      "iteration 144000 training loss 0.6680066 lr 0.00024\n",
      "iteration 144100 training loss 0.70309126 lr 0.00024\n",
      "iteration 144200 training loss 0.7363377 lr 0.00024\n",
      "iteration 144300 training loss 0.57740337 lr 0.00024\n",
      "iteration 144400 training loss 0.68912214 lr 0.00024\n",
      "iteration 144500 training loss 0.65460944 lr 0.00024\n",
      "iteration 144600 training loss 0.64902157 lr 0.00024\n",
      "iteration 144700 training loss 0.860326 lr 0.00024\n",
      "iteration 144800 training loss 0.74662733 lr 0.00024\n",
      "iteration 144900 training loss 0.8011136 lr 0.00024\n",
      "iteration 145000 training loss 0.6196173 lr 0.00024\n",
      "iteration 145100 training loss 0.84574234 lr 0.00024\n",
      "iteration 145200 training loss 0.7435027 lr 0.00024\n",
      "iteration 145300 training loss 0.5840452 lr 0.00024\n",
      "iteration 145400 training loss 0.7865876 lr 0.00024\n",
      "iteration 145500 training loss 0.68449056 lr 0.00024\n",
      "iteration 145600 training loss 0.81031716 lr 0.00024\n",
      "iteration 145700 training loss 0.8814936 lr 0.00024\n",
      "iteration 145800 training loss 0.8099642 lr 0.00024\n",
      "iteration 145900 training loss 0.73552513 lr 0.00024\n",
      "iteration 146000 training loss 0.68852043 lr 0.00024\n",
      "iteration 146100 training loss 1.0232427 lr 0.00024\n",
      "iteration 146200 training loss 0.8129171 lr 0.00024\n",
      "iteration 146300 training loss 0.7526669 lr 0.00024\n",
      "iteration 146400 training loss 0.6684556 lr 0.00024\n",
      "iteration 146500 training loss 0.78777516 lr 0.00024\n",
      "iteration 146600 training loss 0.6960007 lr 0.00024\n",
      "iteration 146700 training loss 0.74540395 lr 0.00024\n",
      "iteration 146800 training loss 0.65256447 lr 0.00024\n",
      "iteration 146900 training loss 0.6915393 lr 0.00024\n",
      "iteration 147000 training loss 0.60463035 lr 0.00024\n",
      "iteration 147100 training loss 0.8019134 lr 0.00024\n",
      "iteration 147200 training loss 0.6969552 lr 0.00024\n",
      "iteration 147300 training loss 0.63483775 lr 0.00024\n",
      "iteration 147400 training loss 0.7124741 lr 0.00024\n",
      "iteration 147500 training loss 0.8520524 lr 0.00024\n",
      "iteration 147600 training loss 0.8027957 lr 0.00024\n",
      "iteration 147700 training loss 0.8972556 lr 0.00024\n",
      "iteration 147800 training loss 0.93832666 lr 0.00024\n",
      "iteration 147900 training loss 0.8738584 lr 0.00024\n",
      "iteration 148000 training loss 0.85522205 lr 0.00024\n",
      "iteration 148100 training loss 0.7729326 lr 0.00024\n",
      "iteration 148200 training loss 0.67726386 lr 0.00024\n",
      "iteration 148300 training loss 0.80320275 lr 0.00024\n",
      "iteration 148400 training loss 0.80473506 lr 0.00024\n",
      "iteration 148500 training loss 0.6509865 lr 0.00024\n",
      "iteration 148600 training loss 0.7753894 lr 0.00024\n",
      "iteration 148700 training loss 0.88263524 lr 0.00024\n",
      "iteration 148800 training loss 0.63545567 lr 0.00024\n",
      "iteration 148900 training loss 0.7975541 lr 0.00024\n",
      "iteration 149000 training loss 0.5911102 lr 0.00024\n",
      "iteration 149100 training loss 0.6503176 lr 0.00024\n",
      "iteration 149200 training loss 1.0138588 lr 0.00024\n",
      "iteration 149300 training loss 0.62961173 lr 0.00024\n",
      "iteration 149400 training loss 0.61577374 lr 0.00024\n",
      "iteration 149500 training loss 0.8660043 lr 0.00024\n",
      "iteration 149600 training loss 0.7217936 lr 0.00024\n",
      "iteration 149700 training loss 0.85124516 lr 0.00024\n",
      "iteration 149800 training loss 0.8215509 lr 0.00024\n",
      "iteration 149900 training loss 0.51087576 lr 0.00024\n",
      "iteration 150000 training loss 0.7572816 lr 0.00024\n",
      "iteration 150100 training loss 0.85924256 lr 0.00024\n",
      "iteration 150200 training loss 0.82499665 lr 0.00024\n",
      "iteration 150300 training loss 0.7095104 lr 0.00024\n",
      "iteration 150400 training loss 0.6349188 lr 0.00024\n",
      "iteration 150500 training loss 0.7863246 lr 0.00024\n",
      "iteration 150600 training loss 0.7429728 lr 0.00024\n",
      "iteration 150700 training loss 0.74982655 lr 0.00024\n",
      "iteration 150800 training loss 0.7430981 lr 0.00024\n",
      "iteration 150900 training loss 0.8615317 lr 0.00024\n",
      "iteration 151000 training loss 0.87969476 lr 0.00024\n",
      "iteration 151100 training loss 0.5720485 lr 0.00024\n",
      "iteration 151200 training loss 0.5856748 lr 0.00024\n",
      "iteration 151300 training loss 0.88674927 lr 0.00024\n",
      "iteration 151400 training loss 0.7880988 lr 0.00024\n",
      "iteration 151500 training loss 0.6905894 lr 0.00024\n",
      "iteration 151600 training loss 0.5707453 lr 0.00024\n",
      "iteration 151700 training loss 0.6453685 lr 0.00024\n",
      "iteration 151800 training loss 0.8143918 lr 0.00024\n",
      "iteration 151900 training loss 0.66505176 lr 0.00024\n",
      "iteration 152000 training loss 0.80495495 lr 0.00024\n",
      "iteration 152100 training loss 0.5077715 lr 0.00024\n",
      "iteration 152200 training loss 0.6198215 lr 0.00024\n",
      "iteration 152300 training loss 0.65995014 lr 0.00024\n",
      "iteration 152400 training loss 0.62024903 lr 0.00024\n",
      "iteration 152500 training loss 0.90523845 lr 0.00024\n",
      "iteration 152600 training loss 0.5845245 lr 0.00024\n",
      "iteration 152700 training loss 0.6003342 lr 0.00024\n",
      "iteration 152800 training loss 0.76234114 lr 0.00024\n",
      "iteration 152900 training loss 0.5996512 lr 0.00024\n",
      "iteration 153000 training loss 0.66267705 lr 0.00024\n",
      "iteration 153100 training loss 0.7504324 lr 0.00024\n",
      "iteration 153200 training loss 0.7047592 lr 0.00024\n",
      "iteration 153300 training loss 0.60475165 lr 0.00024\n",
      "iteration 153400 training loss 0.8226614 lr 0.00024\n",
      "iteration 153500 training loss 0.5695638 lr 0.00024\n",
      "iteration 153600 training loss 0.7473989 lr 0.00024\n",
      "iteration 153700 training loss 0.90379155 lr 0.00024\n",
      "iteration 153800 training loss 0.7308659 lr 0.00024\n",
      "iteration 153900 training loss 0.511831 lr 0.00024\n",
      "iteration 154000 training loss 0.70033306 lr 0.00024\n",
      "iteration 154100 training loss 0.6875986 lr 0.00024\n",
      "iteration 154200 training loss 0.803989 lr 0.00024\n",
      "iteration 154300 training loss 0.79172385 lr 0.00024\n",
      "iteration 154400 training loss 0.68714684 lr 0.00024\n",
      "iteration 154500 training loss 0.72906274 lr 0.00024\n",
      "iteration 154600 training loss 0.67142284 lr 0.00024\n",
      "iteration 154700 training loss 0.6611626 lr 0.00024\n",
      "iteration 154800 training loss 0.74101853 lr 0.00024\n",
      "iteration 154900 training loss 0.77284217 lr 0.00024\n",
      "iteration 155000 training loss 0.63567525 lr 0.00024\n",
      "iteration 155100 training loss 0.95941436 lr 0.00024\n",
      "iteration 155200 training loss 0.6551711 lr 0.00024\n",
      "iteration 155300 training loss 0.60185486 lr 0.00024\n",
      "iteration 155400 training loss 0.61256444 lr 0.00024\n",
      "iteration 155500 training loss 0.7546313 lr 0.00024\n",
      "iteration 155600 training loss 0.5662186 lr 0.00024\n",
      "iteration 155700 training loss 0.69957024 lr 0.00024\n",
      "iteration 155800 training loss 0.6022601 lr 0.00024\n",
      "iteration 155900 training loss 0.8887986 lr 0.00024\n",
      "iteration 156000 training loss 0.6816611 lr 0.00024\n",
      "iteration 156100 training loss 0.681312 lr 0.00024\n",
      "iteration 156200 training loss 0.64414656 lr 0.00024\n",
      "iteration 156300 training loss 0.61944455 lr 0.00024\n",
      "iteration 156400 training loss 0.59378815 lr 0.00024\n",
      "iteration 156500 training loss 0.63350147 lr 0.00024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 156600 training loss 0.8711926 lr 0.00024\n",
      "iteration 156700 training loss 0.74499464 lr 0.00024\n",
      "iteration 156800 training loss 0.7837531 lr 0.00024\n",
      "iteration 156900 training loss 0.53822994 lr 0.00024\n",
      "iteration 157000 training loss 0.67483807 lr 0.00024\n",
      "iteration 157100 training loss 0.6741885 lr 0.00024\n",
      "iteration 157200 training loss 0.637921 lr 0.00024\n",
      "iteration 157300 training loss 0.7675144 lr 0.00024\n",
      "iteration 157400 training loss 0.68616694 lr 0.00024\n",
      "iteration 157500 training loss 0.6614867 lr 0.00024\n",
      "iteration 157600 training loss 0.5318617 lr 0.00024\n",
      "iteration 157700 training loss 0.64925814 lr 0.00024\n",
      "iteration 157800 training loss 0.7004369 lr 0.00024\n",
      "iteration 157900 training loss 0.5286387 lr 0.00024\n",
      "iteration 158000 training loss 0.8077589 lr 0.00024\n",
      "iteration 158100 training loss 0.68747807 lr 0.00024\n",
      "iteration 158200 training loss 0.7389665 lr 0.00024\n",
      "iteration 158300 training loss 0.695837 lr 0.00024\n",
      "iteration 158400 training loss 0.7295564 lr 0.00024\n",
      "iteration 158500 training loss 0.6633839 lr 0.00024\n",
      "iteration 158600 training loss 0.68175507 lr 0.00024\n",
      "iteration 158700 training loss 0.7353765 lr 0.00024\n",
      "iteration 158800 training loss 0.74242616 lr 0.00024\n",
      "iteration 158900 training loss 0.6783285 lr 0.00024\n",
      "iteration 159000 training loss 0.88796264 lr 0.00024\n",
      "iteration 159100 training loss 0.84765697 lr 0.00024\n",
      "iteration 159200 training loss 0.91261977 lr 0.00024\n",
      "iteration 159300 training loss 0.7381028 lr 0.00024\n",
      "iteration 159400 training loss 0.7447016 lr 0.00024\n",
      "iteration 159500 training loss 0.7933928 lr 0.00024\n",
      "iteration 159600 training loss 0.63702595 lr 0.00024\n",
      "iteration 159700 training loss 0.7791671 lr 0.00024\n",
      "iteration 159800 training loss 0.9402529 lr 0.00024\n",
      "iteration 159900 training loss 0.78350544 lr 0.00024\n",
      "iteration 160000 training loss 0.87678504 lr 0.00024\n",
      "layout:nlp:random 0.8006186831899514\n",
      "layout:nlp:default 0.4040483459897725\n",
      "layout:xla:random 0.4660770450527164\n",
      "layout:xla:default 0.20639817844026334\n",
      "epoch 0, it 160000 validation loss -0.469\n",
      "iteration 160100 training loss 0.75388813 lr 0.00022\n",
      "iteration 160200 training loss 0.72742033 lr 0.00022\n",
      "iteration 160300 training loss 0.65934795 lr 0.00022\n",
      "iteration 160400 training loss 0.72516906 lr 0.00022\n",
      "iteration 160500 training loss 0.6027396 lr 0.00022\n",
      "iteration 160600 training loss 0.845194 lr 0.00022\n",
      "iteration 160700 training loss 0.6286801 lr 0.00022\n",
      "iteration 160800 training loss 0.84915024 lr 0.00022\n",
      "iteration 160900 training loss 0.77861124 lr 0.00022\n",
      "iteration 161000 training loss 0.7104122 lr 0.00022\n",
      "iteration 161100 training loss 0.6091621 lr 0.00022\n",
      "iteration 161200 training loss 0.83250207 lr 0.00022\n",
      "iteration 161300 training loss 0.7305779 lr 0.00022\n",
      "iteration 161400 training loss 0.7484115 lr 0.00022\n",
      "iteration 161500 training loss 0.842347 lr 0.00022\n",
      "iteration 161600 training loss 0.7922709 lr 0.00022\n",
      "iteration 161700 training loss 0.68970144 lr 0.00022\n",
      "iteration 161800 training loss 0.73476475 lr 0.00022\n",
      "iteration 161900 training loss 0.74447584 lr 0.00022\n",
      "iteration 162000 training loss 0.69390804 lr 0.00022\n",
      "iteration 162100 training loss 0.73649096 lr 0.00022\n",
      "iteration 162200 training loss 0.90387124 lr 0.00022\n",
      "iteration 162300 training loss 0.6910536 lr 0.00022\n",
      "iteration 162400 training loss 0.6765664 lr 0.00022\n",
      "iteration 162500 training loss 0.655516 lr 0.00022\n",
      "iteration 162600 training loss 0.74620664 lr 0.00022\n",
      "iteration 162700 training loss 0.75007 lr 0.00022\n",
      "iteration 162800 training loss 0.597464 lr 0.00022\n",
      "iteration 162900 training loss 0.6572365 lr 0.00022\n",
      "iteration 163000 training loss 0.7631387 lr 0.00022\n",
      "iteration 163100 training loss 0.73300755 lr 0.00022\n",
      "iteration 163200 training loss 0.73003024 lr 0.00022\n",
      "iteration 163300 training loss 0.88520765 lr 0.00022\n",
      "iteration 163400 training loss 0.70355403 lr 0.00022\n",
      "iteration 163500 training loss 0.79141235 lr 0.00022\n",
      "iteration 163600 training loss 0.8104151 lr 0.00022\n",
      "iteration 163700 training loss 0.7880799 lr 0.00022\n",
      "iteration 163800 training loss 0.6367104 lr 0.00022\n",
      "iteration 163900 training loss 0.88416076 lr 0.00022\n",
      "iteration 164000 training loss 0.81836253 lr 0.00022\n",
      "iteration 164100 training loss 0.7030152 lr 0.00022\n",
      "iteration 164200 training loss 0.94054186 lr 0.00022\n",
      "iteration 164300 training loss 0.8561437 lr 0.00022\n",
      "iteration 164400 training loss 0.846939 lr 0.00022\n",
      "iteration 164500 training loss 0.7960275 lr 0.00022\n",
      "iteration 164600 training loss 0.8102145 lr 0.00022\n",
      "iteration 164700 training loss 0.57533234 lr 0.00022\n",
      "iteration 164800 training loss 0.7197995 lr 0.00022\n",
      "iteration 164900 training loss 0.718976 lr 0.00022\n",
      "iteration 165000 training loss 0.8204728 lr 0.00022\n",
      "iteration 165100 training loss 0.7203079 lr 0.00022\n",
      "iteration 165200 training loss 0.6169181 lr 0.00022\n",
      "iteration 165300 training loss 0.6843271 lr 0.00022\n",
      "iteration 165400 training loss 0.8412917 lr 0.00022\n",
      "iteration 165500 training loss 0.8477916 lr 0.00022\n",
      "iteration 165600 training loss 0.8171527 lr 0.00022\n",
      "iteration 165700 training loss 0.7866984 lr 0.00022\n",
      "iteration 165800 training loss 0.68647784 lr 0.00022\n",
      "iteration 165900 training loss 0.76168096 lr 0.00022\n",
      "iteration 166000 training loss 0.69282794 lr 0.00022\n",
      "iteration 166100 training loss 0.7447413 lr 0.00022\n",
      "iteration 166200 training loss 0.6571224 lr 0.00022\n",
      "iteration 166300 training loss 0.75261176 lr 0.00022\n",
      "iteration 166400 training loss 0.69611645 lr 0.00022\n",
      "iteration 166500 training loss 0.63067347 lr 0.00022\n",
      "iteration 166600 training loss 0.6966751 lr 0.00022\n",
      "iteration 166700 training loss 0.5678766 lr 0.00022\n",
      "iteration 166800 training loss 0.74048084 lr 0.00022\n",
      "iteration 166900 training loss 0.93919843 lr 0.00022\n",
      "iteration 167000 training loss 0.6752988 lr 0.00022\n",
      "iteration 167100 training loss 0.79393846 lr 0.00022\n",
      "iteration 167200 training loss 0.80849874 lr 0.00022\n",
      "iteration 167300 training loss 0.7259285 lr 0.00022\n",
      "iteration 167400 training loss 0.7847242 lr 0.00022\n",
      "iteration 167500 training loss 0.6896625 lr 0.00022\n",
      "iteration 167600 training loss 0.66318053 lr 0.00022\n",
      "iteration 167700 training loss 0.5083643 lr 0.00022\n",
      "iteration 167800 training loss 0.61878234 lr 0.00022\n",
      "iteration 167900 training loss 0.60059714 lr 0.00022\n",
      "iteration 168000 training loss 0.74299705 lr 0.00022\n",
      "iteration 168100 training loss 0.74347126 lr 0.00022\n",
      "iteration 168200 training loss 0.7486651 lr 0.00022\n",
      "iteration 168300 training loss 0.73021376 lr 0.00022\n",
      "iteration 168400 training loss 0.6621696 lr 0.00022\n",
      "iteration 168500 training loss 0.6021892 lr 0.00022\n",
      "iteration 168600 training loss 0.8450251 lr 0.00022\n",
      "iteration 168700 training loss 0.712805 lr 0.00022\n",
      "iteration 168800 training loss 0.736774 lr 0.00022\n",
      "iteration 168900 training loss 0.67678714 lr 0.00022\n",
      "iteration 169000 training loss 0.7725712 lr 0.00022\n",
      "iteration 169100 training loss 0.73174345 lr 0.00022\n",
      "iteration 169200 training loss 0.5026168 lr 0.00022\n",
      "iteration 169300 training loss 0.7033452 lr 0.00022\n",
      "iteration 169400 training loss 0.8326508 lr 0.00022\n",
      "iteration 169500 training loss 0.69984365 lr 0.00022\n",
      "iteration 169600 training loss 0.74189585 lr 0.00022\n",
      "iteration 169700 training loss 0.7812503 lr 0.00022\n",
      "iteration 169800 training loss 0.6410475 lr 0.00022\n",
      "iteration 169900 training loss 0.51406914 lr 0.00022\n",
      "iteration 170000 training loss 0.82525545 lr 0.00022\n",
      "iteration 170100 training loss 0.75689316 lr 0.00022\n",
      "iteration 170200 training loss 0.55696064 lr 0.00022\n",
      "iteration 170300 training loss 0.7856701 lr 0.00022\n",
      "iteration 170400 training loss 0.8417811 lr 0.00022\n",
      "iteration 170500 training loss 0.7084819 lr 0.00022\n",
      "iteration 170600 training loss 0.49526393 lr 0.00022\n",
      "iteration 170700 training loss 0.5177474 lr 0.00022\n",
      "iteration 170800 training loss 0.555017 lr 0.00022\n",
      "iteration 170900 training loss 0.74827987 lr 0.00022\n",
      "iteration 171000 training loss 0.8430976 lr 0.00022\n",
      "iteration 171100 training loss 0.6638716 lr 0.00022\n",
      "iteration 171200 training loss 0.7055353 lr 0.00022\n",
      "iteration 171300 training loss 0.5940565 lr 0.00022\n",
      "iteration 171400 training loss 0.61257267 lr 0.00022\n",
      "iteration 171500 training loss 0.67112917 lr 0.00022\n",
      "iteration 171600 training loss 0.5726201 lr 0.00022\n",
      "iteration 171700 training loss 0.5362697 lr 0.00022\n",
      "iteration 171800 training loss 0.7141126 lr 0.00022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 171900 training loss 0.6266705 lr 0.00022\n",
      "iteration 172000 training loss 0.837855 lr 0.00022\n",
      "iteration 172100 training loss 0.7126314 lr 0.00022\n",
      "iteration 172200 training loss 0.54098064 lr 0.00022\n",
      "iteration 172300 training loss 0.70262426 lr 0.00022\n",
      "iteration 172400 training loss 0.8752069 lr 0.00022\n",
      "iteration 172500 training loss 0.5914049 lr 0.00022\n",
      "iteration 172600 training loss 0.4029619 lr 0.00022\n",
      "iteration 172700 training loss 0.50740445 lr 0.00022\n",
      "iteration 172800 training loss 0.77177525 lr 0.00022\n",
      "iteration 172900 training loss 0.7909262 lr 0.00022\n",
      "iteration 173000 training loss 0.6610263 lr 0.00022\n",
      "iteration 173100 training loss 0.9494371 lr 0.00022\n",
      "iteration 173200 training loss 0.63101465 lr 0.00022\n",
      "iteration 173300 training loss 0.56119126 lr 0.00022\n",
      "iteration 173400 training loss 0.60555315 lr 0.00022\n",
      "iteration 173500 training loss 0.6924015 lr 0.00022\n",
      "iteration 173600 training loss 0.6509652 lr 0.00022\n",
      "iteration 173700 training loss 0.6594966 lr 0.00022\n",
      "iteration 173800 training loss 0.6869791 lr 0.00022\n",
      "iteration 173900 training loss 0.7437847 lr 0.00022\n",
      "iteration 174000 training loss 0.60557693 lr 0.00022\n",
      "iteration 174100 training loss 0.84887505 lr 0.00022\n",
      "iteration 174200 training loss 0.53972304 lr 0.00022\n",
      "iteration 174300 training loss 0.59393513 lr 0.00022\n",
      "iteration 174400 training loss 0.71081036 lr 0.00022\n",
      "iteration 174500 training loss 0.6287561 lr 0.00022\n",
      "iteration 174600 training loss 0.80476254 lr 0.00022\n",
      "iteration 174700 training loss 0.51073515 lr 0.00022\n",
      "iteration 174800 training loss 0.72950464 lr 0.00022\n",
      "iteration 174900 training loss 0.7842812 lr 0.00022\n",
      "iteration 175000 training loss 0.7171114 lr 0.00022\n",
      "iteration 175100 training loss 0.57211655 lr 0.00022\n",
      "iteration 175200 training loss 0.73458815 lr 0.00022\n",
      "iteration 175300 training loss 0.57718736 lr 0.00022\n",
      "iteration 175400 training loss 0.6144077 lr 0.00022\n",
      "iteration 175500 training loss 0.6953275 lr 0.00022\n",
      "iteration 175600 training loss 0.80485606 lr 0.00022\n",
      "iteration 175700 training loss 0.75955164 lr 0.00022\n",
      "iteration 175800 training loss 0.92182505 lr 0.00022\n",
      "iteration 175900 training loss 0.66706127 lr 0.00022\n",
      "iteration 176000 training loss 0.8299532 lr 0.00022\n",
      "iteration 176100 training loss 0.64108336 lr 0.00022\n",
      "iteration 176200 training loss 0.67250574 lr 0.00022\n",
      "iteration 176300 training loss 0.5881857 lr 0.00022\n",
      "iteration 176400 training loss 0.72868687 lr 0.00022\n",
      "iteration 176500 training loss 0.85981333 lr 0.00022\n",
      "iteration 176600 training loss 0.89784324 lr 0.00022\n",
      "iteration 176700 training loss 0.71014625 lr 0.00022\n",
      "iteration 176800 training loss 0.6818331 lr 0.00022\n",
      "iteration 176900 training loss 0.9457631 lr 0.00022\n",
      "iteration 177000 training loss 0.59368455 lr 0.00022\n",
      "iteration 177100 training loss 0.78974974 lr 0.00022\n",
      "iteration 177200 training loss 0.6383055 lr 0.00022\n",
      "iteration 177300 training loss 0.60125816 lr 0.00022\n",
      "iteration 177400 training loss 0.8224607 lr 0.00022\n",
      "iteration 177500 training loss 0.67968255 lr 0.00022\n",
      "iteration 177600 training loss 0.60540396 lr 0.00022\n",
      "iteration 177700 training loss 0.62761474 lr 0.00022\n",
      "iteration 177800 training loss 0.7014489 lr 0.00022\n",
      "iteration 177900 training loss 0.83103967 lr 0.00022\n",
      "iteration 178000 training loss 0.6886913 lr 0.00022\n",
      "iteration 178100 training loss 0.80630827 lr 0.00022\n",
      "iteration 178200 training loss 0.9448992 lr 0.00022\n",
      "iteration 178300 training loss 0.71996146 lr 0.00022\n",
      "iteration 178400 training loss 0.73516136 lr 0.00022\n",
      "iteration 178500 training loss 0.7368025 lr 0.00022\n",
      "iteration 178600 training loss 0.6109585 lr 0.00022\n",
      "iteration 178700 training loss 0.70086616 lr 0.00022\n",
      "iteration 178800 training loss 0.6034239 lr 0.00022\n",
      "iteration 178900 training loss 0.5674662 lr 0.00022\n",
      "iteration 179000 training loss 0.73010516 lr 0.00022\n",
      "iteration 179100 training loss 0.7266106 lr 0.00022\n",
      "iteration 179200 training loss 0.7447231 lr 0.00022\n",
      "iteration 179300 training loss 0.74296635 lr 0.00022\n",
      "iteration 179400 training loss 0.7163194 lr 0.00022\n",
      "iteration 179500 training loss 0.6926748 lr 0.00022\n",
      "iteration 179600 training loss 0.6601146 lr 0.00022\n",
      "iteration 179700 training loss 0.75739247 lr 0.00022\n",
      "iteration 179800 training loss 0.8456 lr 0.00022\n",
      "iteration 179900 training loss 0.72376174 lr 0.00022\n",
      "iteration 180000 training loss 0.7582856 lr 0.00022\n",
      "layout:nlp:random 0.8063179940189947\n",
      "layout:nlp:default 0.41391664126678523\n",
      "layout:xla:random 0.46525295635944186\n",
      "layout:xla:default 0.20962118898867935\n",
      "epoch 0, it 180000 validation loss -0.474\n",
      "iteration 180100 training loss 0.55792105 lr 0.00019\n",
      "iteration 180200 training loss 0.6773731 lr 0.00019\n",
      "iteration 180300 training loss 0.55697984 lr 0.00019\n",
      "iteration 180400 training loss 0.76579535 lr 0.00019\n",
      "iteration 180500 training loss 0.6138761 lr 0.00019\n",
      "iteration 180600 training loss 0.65475506 lr 0.00019\n",
      "iteration 180700 training loss 0.58902234 lr 0.00019\n",
      "iteration 180800 training loss 0.8289777 lr 0.00019\n",
      "iteration 180900 training loss 0.6301788 lr 0.00019\n",
      "iteration 181000 training loss 0.76764256 lr 0.00019\n",
      "iteration 181100 training loss 0.73327225 lr 0.00019\n",
      "iteration 181200 training loss 0.74462795 lr 0.00019\n",
      "iteration 181300 training loss 0.662824 lr 0.00019\n",
      "iteration 181400 training loss 0.82802266 lr 0.00019\n",
      "iteration 181500 training loss 0.55793244 lr 0.00019\n",
      "iteration 181600 training loss 0.53083354 lr 0.00019\n",
      "iteration 181700 training loss 0.52240807 lr 0.00019\n",
      "iteration 181800 training loss 0.85989565 lr 0.00019\n",
      "iteration 181900 training loss 0.6688903 lr 0.00019\n",
      "iteration 182000 training loss 0.5804095 lr 0.00019\n",
      "iteration 182100 training loss 0.7432742 lr 0.00019\n",
      "iteration 182200 training loss 0.6680227 lr 0.00019\n",
      "iteration 182300 training loss 0.63330585 lr 0.00019\n",
      "iteration 182400 training loss 0.6829204 lr 0.00019\n",
      "iteration 182500 training loss 0.60146976 lr 0.00019\n",
      "iteration 182600 training loss 0.7063022 lr 0.00019\n",
      "iteration 182700 training loss 0.7825482 lr 0.00019\n",
      "iteration 182800 training loss 0.5660776 lr 0.00019\n",
      "iteration 182900 training loss 0.662209 lr 0.00019\n",
      "iteration 183000 training loss 0.6445383 lr 0.00019\n",
      "iteration 183100 training loss 0.9028638 lr 0.00019\n",
      "iteration 183200 training loss 0.8142458 lr 0.00019\n",
      "iteration 183300 training loss 0.61592203 lr 0.00019\n",
      "iteration 183400 training loss 0.67393154 lr 0.00019\n",
      "iteration 183500 training loss 0.6461035 lr 0.00019\n",
      "iteration 183600 training loss 0.64860284 lr 0.00019\n",
      "iteration 183700 training loss 0.7244526 lr 0.00019\n",
      "iteration 183800 training loss 0.80459803 lr 0.00019\n",
      "iteration 183900 training loss 0.69861144 lr 0.00019\n",
      "iteration 184000 training loss 0.74532115 lr 0.00019\n",
      "iteration 184100 training loss 0.6743544 lr 0.00019\n",
      "iteration 184200 training loss 0.7320099 lr 0.00019\n",
      "iteration 184300 training loss 0.80503416 lr 0.00019\n",
      "iteration 184400 training loss 0.7635034 lr 0.00019\n",
      "iteration 184500 training loss 0.7868969 lr 0.00019\n",
      "iteration 184600 training loss 0.8727666 lr 0.00019\n",
      "iteration 184700 training loss 0.6195553 lr 0.00019\n",
      "iteration 184800 training loss 0.520549 lr 0.00019\n",
      "iteration 184900 training loss 0.6020436 lr 0.00019\n",
      "iteration 185000 training loss 0.56183165 lr 0.00019\n",
      "iteration 185100 training loss 0.69337326 lr 0.00019\n",
      "iteration 185200 training loss 0.6485178 lr 0.00019\n",
      "iteration 185300 training loss 0.70448875 lr 0.00019\n",
      "iteration 185400 training loss 0.47447756 lr 0.00019\n",
      "iteration 185500 training loss 0.40820378 lr 0.00019\n",
      "iteration 185600 training loss 0.5947596 lr 0.00019\n",
      "iteration 185700 training loss 0.6563188 lr 0.00019\n",
      "iteration 185800 training loss 0.5004841 lr 0.00019\n",
      "iteration 185900 training loss 0.56275344 lr 0.00019\n",
      "iteration 186000 training loss 0.5869507 lr 0.00019\n",
      "iteration 186100 training loss 0.7549868 lr 0.00019\n",
      "iteration 186200 training loss 0.6793675 lr 0.00019\n",
      "iteration 186300 training loss 0.5566522 lr 0.00019\n",
      "iteration 186400 training loss 0.72282314 lr 0.00019\n",
      "iteration 186500 training loss 0.7238035 lr 0.00019\n",
      "iteration 186600 training loss 0.5660472 lr 0.00019\n",
      "iteration 186700 training loss 0.75113165 lr 0.00019\n",
      "iteration 186800 training loss 0.70461535 lr 0.00019\n",
      "iteration 186900 training loss 0.5032318 lr 0.00019\n",
      "iteration 187000 training loss 0.72674376 lr 0.00019\n",
      "iteration 187100 training loss 0.45430583 lr 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 187200 training loss 0.75888544 lr 0.00019\n",
      "iteration 187300 training loss 0.6990773 lr 0.00019\n",
      "iteration 187400 training loss 0.6351169 lr 0.00019\n",
      "iteration 187500 training loss 0.5951728 lr 0.00019\n",
      "iteration 187600 training loss 0.69263065 lr 0.00019\n",
      "iteration 187700 training loss 0.6785206 lr 0.00019\n",
      "iteration 187800 training loss 0.81870365 lr 0.00019\n",
      "iteration 187900 training loss 0.6703293 lr 0.00019\n",
      "iteration 188000 training loss 0.69800276 lr 0.00019\n",
      "iteration 188100 training loss 0.55623764 lr 0.00019\n",
      "iteration 188200 training loss 0.59749806 lr 0.00019\n",
      "iteration 188300 training loss 0.6378095 lr 0.00019\n",
      "iteration 188400 training loss 0.79507726 lr 0.00019\n",
      "iteration 188500 training loss 0.8249854 lr 0.00019\n",
      "iteration 188600 training loss 0.65883785 lr 0.00019\n",
      "iteration 188700 training loss 0.77459 lr 0.00019\n",
      "iteration 188800 training loss 0.6391481 lr 0.00019\n",
      "iteration 188900 training loss 0.89123464 lr 0.00019\n",
      "iteration 189000 training loss 0.71137977 lr 0.00019\n",
      "iteration 189100 training loss 0.7021122 lr 0.00019\n",
      "iteration 189200 training loss 0.76603544 lr 0.00019\n",
      "iteration 189300 training loss 0.9222392 lr 0.00019\n",
      "iteration 189400 training loss 0.5982495 lr 0.00019\n",
      "iteration 189500 training loss 0.73322624 lr 0.00019\n",
      "iteration 189600 training loss 0.7530792 lr 0.00019\n",
      "iteration 189700 training loss 0.6427496 lr 0.00019\n",
      "iteration 189800 training loss 0.5761435 lr 0.00019\n",
      "iteration 189900 training loss 0.59873986 lr 0.00019\n",
      "iteration 190000 training loss 0.7524449 lr 0.00019\n",
      "iteration 190100 training loss 0.6115052 lr 0.00019\n",
      "iteration 190200 training loss 0.64302003 lr 0.00019\n",
      "iteration 190300 training loss 0.80831975 lr 0.00019\n",
      "iteration 190400 training loss 0.8249114 lr 0.00019\n",
      "iteration 190500 training loss 0.8045104 lr 0.00019\n",
      "iteration 190600 training loss 0.8802418 lr 0.00019\n",
      "iteration 190700 training loss 0.82631177 lr 0.00019\n",
      "iteration 190800 training loss 0.679454 lr 0.00019\n",
      "iteration 190900 training loss 0.6655494 lr 0.00019\n",
      "iteration 191000 training loss 0.87258637 lr 0.00019\n",
      "iteration 191100 training loss 0.6592255 lr 0.00019\n",
      "iteration 191200 training loss 0.89678615 lr 0.00019\n",
      "iteration 191300 training loss 0.6176224 lr 0.00019\n",
      "iteration 191400 training loss 0.8324114 lr 0.00019\n",
      "iteration 191500 training loss 0.68935263 lr 0.00019\n",
      "iteration 191600 training loss 0.7900975 lr 0.00019\n",
      "iteration 191700 training loss 0.8841196 lr 0.00019\n",
      "iteration 191800 training loss 0.7106931 lr 0.00019\n",
      "iteration 191900 training loss 0.7513984 lr 0.00019\n",
      "iteration 192000 training loss 0.8377643 lr 0.00019\n",
      "iteration 192100 training loss 0.6875414 lr 0.00019\n",
      "iteration 192200 training loss 0.8038375 lr 0.00019\n",
      "iteration 192300 training loss 0.63994235 lr 0.00019\n",
      "iteration 192400 training loss 0.7260757 lr 0.00019\n",
      "iteration 192500 training loss 0.7361682 lr 0.00019\n",
      "iteration 192600 training loss 0.7174125 lr 0.00019\n",
      "iteration 192700 training loss 0.58943635 lr 0.00019\n",
      "iteration 192800 training loss 0.88671106 lr 0.00019\n",
      "iteration 192900 training loss 0.78130215 lr 0.00019\n",
      "iteration 193000 training loss 0.597908 lr 0.00019\n",
      "iteration 193100 training loss 0.8321916 lr 0.00019\n",
      "iteration 193200 training loss 0.7421101 lr 0.00019\n",
      "iteration 193300 training loss 0.5914908 lr 0.00019\n",
      "iteration 193400 training loss 0.65065825 lr 0.00019\n",
      "iteration 193500 training loss 0.711412 lr 0.00019\n",
      "iteration 193600 training loss 0.6168304 lr 0.00019\n",
      "iteration 193700 training loss 0.827284 lr 0.00019\n",
      "iteration 193800 training loss 0.66358614 lr 0.00019\n",
      "iteration 193900 training loss 0.5567437 lr 0.00019\n",
      "iteration 194000 training loss 0.6481596 lr 0.00019\n",
      "iteration 194100 training loss 0.6344607 lr 0.00019\n",
      "iteration 194200 training loss 0.6510075 lr 0.00019\n",
      "iteration 194300 training loss 0.7548067 lr 0.00019\n",
      "iteration 194400 training loss 0.6180451 lr 0.00019\n",
      "iteration 194500 training loss 0.5513213 lr 0.00019\n",
      "iteration 194600 training loss 0.7354035 lr 0.00019\n",
      "iteration 194700 training loss 0.6544157 lr 0.00019\n",
      "iteration 194800 training loss 0.6253828 lr 0.00019\n",
      "iteration 194900 training loss 0.7053655 lr 0.00019\n",
      "iteration 195000 training loss 0.6088458 lr 0.00019\n",
      "iteration 195100 training loss 0.85936403 lr 0.00019\n",
      "iteration 195200 training loss 0.69470537 lr 0.00019\n",
      "iteration 195300 training loss 0.6784459 lr 0.00019\n",
      "iteration 195400 training loss 0.74860054 lr 0.00019\n",
      "iteration 195500 training loss 0.62224346 lr 0.00019\n",
      "iteration 195600 training loss 0.62426263 lr 0.00019\n",
      "iteration 195700 training loss 0.57214606 lr 0.00019\n",
      "iteration 195800 training loss 0.7741984 lr 0.00019\n",
      "iteration 195900 training loss 0.8078062 lr 0.00019\n",
      "iteration 196000 training loss 0.62923306 lr 0.00019\n",
      "iteration 196100 training loss 0.6485298 lr 0.00019\n",
      "iteration 196200 training loss 0.54672444 lr 0.00019\n",
      "iteration 196300 training loss 0.6410123 lr 0.00019\n",
      "iteration 196400 training loss 0.74037844 lr 0.00019\n",
      "iteration 196500 training loss 0.6153428 lr 0.00019\n",
      "iteration 196600 training loss 0.7172073 lr 0.00019\n",
      "iteration 196700 training loss 0.6087805 lr 0.00019\n",
      "iteration 196800 training loss 0.8681948 lr 0.00019\n",
      "iteration 196900 training loss 0.64332116 lr 0.00019\n",
      "iteration 197000 training loss 0.65308994 lr 0.00019\n",
      "iteration 197100 training loss 0.7841023 lr 0.00019\n",
      "iteration 197200 training loss 0.62165934 lr 0.00019\n",
      "iteration 197300 training loss 0.7412823 lr 0.00019\n",
      "iteration 197400 training loss 0.7620808 lr 0.00019\n",
      "iteration 197500 training loss 0.97453636 lr 0.00019\n",
      "iteration 197600 training loss 0.71434677 lr 0.00019\n",
      "iteration 197700 training loss 0.75416416 lr 0.00019\n",
      "iteration 197800 training loss 0.7988874 lr 0.00019\n",
      "iteration 197900 training loss 0.7178072 lr 0.00019\n",
      "iteration 198000 training loss 0.87249166 lr 0.00019\n",
      "iteration 198100 training loss 0.63833237 lr 0.00019\n",
      "iteration 198200 training loss 0.95387614 lr 0.00019\n",
      "iteration 198300 training loss 0.8651658 lr 0.00019\n",
      "iteration 198400 training loss 0.5681559 lr 0.00019\n",
      "iteration 198500 training loss 0.6977955 lr 0.00019\n",
      "iteration 198600 training loss 0.84471875 lr 0.00019\n",
      "iteration 198700 training loss 0.65739197 lr 0.00019\n",
      "iteration 198800 training loss 0.60324866 lr 0.00019\n",
      "iteration 198900 training loss 0.778464 lr 0.00019\n",
      "iteration 199000 training loss 0.7106264 lr 0.00019\n",
      "iteration 199100 training loss 0.7145233 lr 0.00019\n",
      "iteration 199200 training loss 0.7091617 lr 0.00019\n",
      "iteration 199300 training loss 0.87963426 lr 0.00019\n",
      "iteration 199400 training loss 0.85665774 lr 0.00019\n",
      "iteration 199500 training loss 0.66176623 lr 0.00019\n",
      "iteration 199600 training loss 0.7245415 lr 0.00019\n",
      "iteration 199700 training loss 0.6590992 lr 0.00019\n",
      "iteration 199800 training loss 0.60482806 lr 0.00019\n",
      "iteration 199900 training loss 0.76271796 lr 0.00019\n",
      "iteration 200000 training loss 0.736665 lr 0.00019\n",
      "layout:nlp:random 0.7689027643477214\n",
      "layout:nlp:default 0.3949921913840012\n",
      "layout:xla:random 0.4655807664505057\n",
      "layout:xla:default 0.2399351207516267\n",
      "epoch 0, it 200000 validation loss -0.467\n",
      "iteration 200100 training loss 0.74689806 lr 0.00017\n",
      "iteration 200200 training loss 0.5365923 lr 0.00017\n",
      "iteration 200300 training loss 0.6694713 lr 0.00017\n",
      "iteration 200400 training loss 0.6944051 lr 0.00017\n",
      "iteration 200500 training loss 0.67389446 lr 0.00017\n",
      "iteration 200600 training loss 0.7740305 lr 0.00017\n",
      "iteration 200700 training loss 0.55086917 lr 0.00017\n",
      "iteration 200800 training loss 0.67868716 lr 0.00017\n",
      "iteration 200900 training loss 0.6894457 lr 0.00017\n",
      "iteration 201000 training loss 0.6551388 lr 0.00017\n",
      "iteration 201100 training loss 0.5219231 lr 0.00017\n",
      "iteration 201200 training loss 0.78682095 lr 0.00017\n",
      "iteration 201300 training loss 0.7482096 lr 0.00017\n",
      "iteration 201400 training loss 0.576601 lr 0.00017\n",
      "iteration 201500 training loss 0.7334097 lr 0.00017\n",
      "iteration 201600 training loss 0.60392207 lr 0.00017\n",
      "iteration 201700 training loss 0.64922976 lr 0.00017\n",
      "iteration 201800 training loss 0.61161405 lr 0.00017\n",
      "iteration 201900 training loss 0.66016155 lr 0.00017\n",
      "iteration 202000 training loss 0.6666539 lr 0.00017\n",
      "iteration 202100 training loss 0.55175525 lr 0.00017\n",
      "iteration 202200 training loss 0.7374441 lr 0.00017\n",
      "iteration 202300 training loss 0.5990071 lr 0.00017\n",
      "iteration 202400 training loss 0.48094544 lr 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 202500 training loss 0.76886505 lr 0.00017\n",
      "iteration 202600 training loss 0.6522983 lr 0.00017\n",
      "iteration 202700 training loss 0.59484166 lr 0.00017\n",
      "iteration 202800 training loss 0.5529627 lr 0.00017\n",
      "iteration 202900 training loss 0.49665445 lr 0.00017\n",
      "iteration 203000 training loss 0.536439 lr 0.00017\n",
      "iteration 203100 training loss 0.50751036 lr 0.00017\n",
      "iteration 203200 training loss 0.50070053 lr 0.00017\n",
      "iteration 203300 training loss 0.69212687 lr 0.00017\n",
      "iteration 203400 training loss 0.8728911 lr 0.00017\n",
      "iteration 203500 training loss 0.5179787 lr 0.00017\n",
      "iteration 203600 training loss 0.50748736 lr 0.00017\n",
      "iteration 203700 training loss 0.5276546 lr 0.00017\n",
      "iteration 203800 training loss 0.7384858 lr 0.00017\n",
      "iteration 203900 training loss 0.65805894 lr 0.00017\n",
      "iteration 204000 training loss 0.5676558 lr 0.00017\n",
      "iteration 204100 training loss 0.7531276 lr 0.00017\n",
      "iteration 204200 training loss 0.5844258 lr 0.00017\n",
      "iteration 204300 training loss 0.7393686 lr 0.00017\n",
      "iteration 204400 training loss 0.8642467 lr 0.00017\n",
      "iteration 204500 training loss 0.6613683 lr 0.00017\n",
      "iteration 204600 training loss 0.69957066 lr 0.00017\n",
      "iteration 204700 training loss 0.6488839 lr 0.00017\n",
      "iteration 204800 training loss 0.61792564 lr 0.00017\n",
      "iteration 204900 training loss 0.66019803 lr 0.00017\n",
      "iteration 205000 training loss 0.53134805 lr 0.00017\n",
      "iteration 205100 training loss 0.6709101 lr 0.00017\n",
      "iteration 205200 training loss 0.6466114 lr 0.00017\n",
      "iteration 205300 training loss 0.7065862 lr 0.00017\n",
      "iteration 205400 training loss 0.50262576 lr 0.00017\n",
      "iteration 205500 training loss 0.6542475 lr 0.00017\n",
      "iteration 205600 training loss 0.7584211 lr 0.00017\n",
      "iteration 205700 training loss 0.6959169 lr 0.00017\n",
      "iteration 205800 training loss 0.6156506 lr 0.00017\n",
      "iteration 205900 training loss 0.5478894 lr 0.00017\n",
      "iteration 206000 training loss 0.6684157 lr 0.00017\n",
      "iteration 206100 training loss 0.8936939 lr 0.00017\n",
      "iteration 206200 training loss 0.72114956 lr 0.00017\n",
      "iteration 206300 training loss 0.5929571 lr 0.00017\n",
      "iteration 206400 training loss 0.7709125 lr 0.00017\n",
      "iteration 206500 training loss 0.7877809 lr 0.00017\n",
      "iteration 206600 training loss 0.71505475 lr 0.00017\n",
      "iteration 206700 training loss 0.721877 lr 0.00017\n",
      "iteration 206800 training loss 0.6662885 lr 0.00017\n",
      "iteration 206900 training loss 0.5691649 lr 0.00017\n",
      "iteration 207000 training loss 0.5450983 lr 0.00017\n",
      "iteration 207100 training loss 0.5883641 lr 0.00017\n",
      "iteration 207200 training loss 0.6677984 lr 0.00017\n",
      "iteration 207300 training loss 0.7179579 lr 0.00017\n",
      "iteration 207400 training loss 0.78620857 lr 0.00017\n",
      "iteration 207500 training loss 0.68585014 lr 0.00017\n",
      "iteration 207600 training loss 0.5963224 lr 0.00017\n",
      "iteration 207700 training loss 0.6452889 lr 0.00017\n",
      "iteration 207800 training loss 0.64126974 lr 0.00017\n",
      "iteration 207900 training loss 0.55866516 lr 0.00017\n",
      "iteration 208000 training loss 0.80333924 lr 0.00017\n",
      "iteration 208100 training loss 0.8426037 lr 0.00017\n",
      "iteration 208200 training loss 0.9485614 lr 0.00017\n",
      "iteration 208300 training loss 0.76045597 lr 0.00017\n",
      "iteration 208400 training loss 0.6267254 lr 0.00017\n",
      "iteration 208500 training loss 0.72001004 lr 0.00017\n",
      "iteration 208600 training loss 0.796813 lr 0.00017\n",
      "iteration 208700 training loss 0.628395 lr 0.00017\n",
      "iteration 208800 training loss 0.8232271 lr 0.00017\n",
      "iteration 208900 training loss 0.78249925 lr 0.00017\n",
      "iteration 209000 training loss 0.74059355 lr 0.00017\n",
      "iteration 209100 training loss 0.61531645 lr 0.00017\n",
      "iteration 209200 training loss 0.6188186 lr 0.00017\n",
      "iteration 209300 training loss 0.62651455 lr 0.00017\n",
      "iteration 209400 training loss 0.77593607 lr 0.00017\n",
      "iteration 209500 training loss 0.7784318 lr 0.00017\n",
      "iteration 209600 training loss 0.61917764 lr 0.00017\n",
      "iteration 209700 training loss 0.6626458 lr 0.00017\n",
      "iteration 209800 training loss 0.59169906 lr 0.00017\n",
      "iteration 209900 training loss 0.71289736 lr 0.00017\n",
      "iteration 210000 training loss 0.7552273 lr 0.00017\n",
      "iteration 210100 training loss 0.7920209 lr 0.00017\n",
      "iteration 210200 training loss 0.5210226 lr 0.00017\n",
      "iteration 210300 training loss 0.7440808 lr 0.00017\n",
      "iteration 210400 training loss 0.9377293 lr 0.00017\n",
      "iteration 210500 training loss 0.72526276 lr 0.00017\n",
      "iteration 210600 training loss 0.85750115 lr 0.00017\n",
      "iteration 210700 training loss 0.9056363 lr 0.00017\n",
      "iteration 210800 training loss 0.7485092 lr 0.00017\n",
      "iteration 210900 training loss 0.5279637 lr 0.00017\n",
      "iteration 211000 training loss 0.8027125 lr 0.00017\n",
      "iteration 211100 training loss 0.6674446 lr 0.00017\n",
      "iteration 211200 training loss 0.7625921 lr 0.00017\n",
      "iteration 211300 training loss 0.7922044 lr 0.00017\n",
      "iteration 211400 training loss 0.86837727 lr 0.00017\n",
      "iteration 211500 training loss 0.7006194 lr 0.00017\n",
      "iteration 211600 training loss 0.8930415 lr 0.00017\n",
      "iteration 211700 training loss 0.7718679 lr 0.00017\n",
      "iteration 211800 training loss 0.76177424 lr 0.00017\n",
      "iteration 211900 training loss 0.756372 lr 0.00017\n",
      "iteration 212000 training loss 0.7531367 lr 0.00017\n",
      "iteration 212100 training loss 0.7277623 lr 0.00017\n",
      "iteration 212200 training loss 0.74104816 lr 0.00017\n",
      "iteration 212300 training loss 0.6409102 lr 0.00017\n",
      "iteration 212400 training loss 0.88525856 lr 0.00017\n",
      "iteration 212500 training loss 0.69046175 lr 0.00017\n",
      "iteration 212600 training loss 0.68825865 lr 0.00017\n",
      "iteration 212700 training loss 0.83262724 lr 0.00017\n",
      "iteration 212800 training loss 1.0017219 lr 0.00017\n",
      "iteration 212900 training loss 0.8051003 lr 0.00017\n",
      "iteration 213000 training loss 0.53122854 lr 0.00017\n",
      "iteration 213100 training loss 0.8337025 lr 0.00017\n",
      "iteration 213200 training loss 0.7794393 lr 0.00017\n",
      "iteration 213300 training loss 0.6414851 lr 0.00017\n",
      "iteration 213400 training loss 0.60468507 lr 0.00017\n",
      "iteration 213500 training loss 0.6176898 lr 0.00017\n",
      "iteration 213600 training loss 0.7218244 lr 0.00017\n",
      "iteration 213700 training loss 0.5872803 lr 0.00017\n",
      "iteration 213800 training loss 0.71471983 lr 0.00017\n",
      "iteration 213900 training loss 0.77226907 lr 0.00017\n",
      "iteration 214000 training loss 0.5454577 lr 0.00017\n",
      "iteration 214100 training loss 0.5017585 lr 0.00017\n",
      "iteration 214200 training loss 0.78429306 lr 0.00017\n",
      "iteration 214300 training loss 0.7721277 lr 0.00017\n",
      "iteration 214400 training loss 0.60487837 lr 0.00017\n",
      "iteration 214500 training loss 0.6077262 lr 0.00017\n",
      "iteration 214600 training loss 0.72626567 lr 0.00017\n",
      "iteration 214700 training loss 0.6254527 lr 0.00017\n",
      "iteration 214800 training loss 0.5523564 lr 0.00017\n",
      "iteration 214900 training loss 0.53238183 lr 0.00017\n",
      "iteration 215000 training loss 0.5198504 lr 0.00017\n",
      "iteration 215100 training loss 0.66098845 lr 0.00017\n",
      "iteration 215200 training loss 0.80641717 lr 0.00017\n",
      "iteration 215300 training loss 0.63884455 lr 0.00017\n",
      "iteration 215400 training loss 0.8614889 lr 0.00017\n",
      "iteration 215500 training loss 0.5794774 lr 0.00017\n",
      "iteration 215600 training loss 0.64332765 lr 0.00017\n",
      "iteration 215700 training loss 0.59800076 lr 0.00017\n",
      "iteration 215800 training loss 0.68154967 lr 0.00017\n",
      "iteration 215900 training loss 0.6322319 lr 0.00017\n",
      "iteration 216000 training loss 0.6533467 lr 0.00017\n",
      "iteration 216100 training loss 0.53952014 lr 0.00017\n",
      "iteration 216200 training loss 0.50718796 lr 0.00017\n",
      "iteration 216300 training loss 0.74510694 lr 0.00017\n",
      "iteration 216400 training loss 0.59635824 lr 0.00017\n",
      "iteration 216500 training loss 0.68703043 lr 0.00017\n",
      "iteration 216600 training loss 0.5998321 lr 0.00017\n",
      "iteration 216700 training loss 0.7958356 lr 0.00017\n",
      "iteration 216800 training loss 0.8687771 lr 0.00017\n",
      "iteration 216900 training loss 0.66105276 lr 0.00017\n",
      "iteration 217000 training loss 0.6798918 lr 0.00017\n",
      "iteration 217100 training loss 0.71374536 lr 0.00017\n",
      "iteration 217200 training loss 0.7037306 lr 0.00017\n",
      "iteration 217300 training loss 0.6818369 lr 0.00017\n",
      "iteration 217400 training loss 0.6762751 lr 0.00017\n",
      "iteration 217500 training loss 0.5612867 lr 0.00017\n",
      "iteration 217600 training loss 0.6419672 lr 0.00017\n",
      "iteration 217700 training loss 0.6526109 lr 0.00017\n",
      "iteration 217800 training loss 0.6540933 lr 0.00017\n",
      "iteration 217900 training loss 0.74055046 lr 0.00017\n",
      "iteration 218000 training loss 0.55175775 lr 0.00017\n",
      "iteration 218100 training loss 0.56684816 lr 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 218200 training loss 0.6215163 lr 0.00017\n",
      "iteration 218300 training loss 0.86650777 lr 0.00017\n",
      "iteration 218400 training loss 0.75602597 lr 0.00017\n",
      "iteration 218500 training loss 0.8150335 lr 0.00017\n",
      "iteration 218600 training loss 0.8954081 lr 0.00017\n",
      "iteration 218700 training loss 0.68400764 lr 0.00017\n",
      "iteration 218800 training loss 0.7883852 lr 0.00017\n",
      "iteration 218900 training loss 0.7234152 lr 0.00017\n",
      "iteration 219000 training loss 0.490708 lr 0.00017\n",
      "iteration 219100 training loss 0.57116896 lr 0.00017\n",
      "iteration 219200 training loss 0.6497637 lr 0.00017\n",
      "iteration 219300 training loss 0.49255818 lr 0.00017\n",
      "iteration 219400 training loss 0.69467735 lr 0.00017\n",
      "iteration 219500 training loss 0.59309745 lr 0.00017\n",
      "iteration 219600 training loss 0.832987 lr 0.00017\n",
      "iteration 219700 training loss 0.6113469 lr 0.00017\n",
      "iteration 219800 training loss 0.61982363 lr 0.00017\n",
      "iteration 219900 training loss 0.6621973 lr 0.00017\n",
      "iteration 220000 training loss 0.8055366 lr 0.00017\n",
      "layout:nlp:random 0.8025680380092062\n",
      "layout:nlp:default 0.4120544353533904\n",
      "layout:xla:random 0.46938557334958436\n",
      "layout:xla:default 0.23866146112646788\n",
      "epoch 0, it 220000 validation loss -0.481\n",
      "iteration 220100 training loss 0.5487062 lr 0.00016\n",
      "iteration 220200 training loss 0.5951673 lr 0.00016\n",
      "iteration 220300 training loss 0.7576466 lr 0.00016\n",
      "iteration 220400 training loss 0.67087835 lr 0.00016\n",
      "iteration 220500 training loss 0.60092396 lr 0.00016\n",
      "iteration 220600 training loss 0.73585 lr 0.00016\n",
      "iteration 220700 training loss 0.6963211 lr 0.00016\n",
      "iteration 220800 training loss 0.675754 lr 0.00016\n",
      "iteration 220900 training loss 0.7039923 lr 0.00016\n",
      "iteration 221000 training loss 0.6260853 lr 0.00016\n",
      "iteration 221100 training loss 0.7535947 lr 0.00016\n",
      "iteration 221200 training loss 0.6499213 lr 0.00016\n",
      "iteration 221300 training loss 0.6151263 lr 0.00016\n",
      "iteration 221400 training loss 1.0347495 lr 0.00016\n",
      "iteration 221500 training loss 0.7044097 lr 0.00016\n",
      "iteration 221600 training loss 0.7401371 lr 0.00016\n",
      "iteration 221700 training loss 0.56865305 lr 0.00016\n",
      "iteration 221800 training loss 0.59787506 lr 0.00016\n",
      "iteration 221900 training loss 0.77586716 lr 0.00016\n",
      "iteration 222000 training loss 0.99991876 lr 0.00016\n",
      "iteration 222100 training loss 0.7771327 lr 0.00016\n",
      "iteration 222200 training loss 0.78089243 lr 0.00016\n",
      "iteration 222300 training loss 0.53226113 lr 0.00016\n",
      "iteration 222400 training loss 0.7403085 lr 0.00016\n",
      "iteration 222500 training loss 0.820741 lr 0.00016\n",
      "iteration 222600 training loss 0.70925057 lr 0.00016\n",
      "iteration 222700 training loss 0.7234252 lr 0.00016\n",
      "iteration 222800 training loss 0.7088547 lr 0.00016\n",
      "iteration 222900 training loss 0.71240085 lr 0.00016\n",
      "iteration 223000 training loss 0.9299643 lr 0.00016\n",
      "iteration 223100 training loss 0.72059095 lr 0.00016\n",
      "iteration 223200 training loss 0.67098117 lr 0.00016\n",
      "iteration 223300 training loss 0.7346936 lr 0.00016\n",
      "iteration 223400 training loss 0.67489135 lr 0.00016\n",
      "iteration 223500 training loss 0.7093034 lr 0.00016\n",
      "iteration 223600 training loss 0.7865849 lr 0.00016\n",
      "iteration 223700 training loss 0.78003246 lr 0.00016\n",
      "iteration 223800 training loss 0.65823567 lr 0.00016\n",
      "iteration 223900 training loss 0.7988748 lr 0.00016\n",
      "iteration 224000 training loss 0.7387371 lr 0.00016\n",
      "iteration 224100 training loss 0.67978054 lr 0.00016\n",
      "iteration 224200 training loss 0.6366532 lr 0.00016\n",
      "iteration 224300 training loss 0.67736346 lr 0.00016\n",
      "iteration 224400 training loss 0.76311666 lr 0.00016\n",
      "iteration 224500 training loss 0.56986153 lr 0.00016\n",
      "iteration 224600 training loss 0.7218968 lr 0.00016\n",
      "iteration 224700 training loss 0.8122673 lr 0.00016\n",
      "iteration 224800 training loss 0.7401533 lr 0.00016\n",
      "iteration 224900 training loss 0.9288558 lr 0.00016\n",
      "iteration 225000 training loss 0.6491967 lr 0.00016\n",
      "iteration 225100 training loss 0.7918963 lr 0.00016\n",
      "iteration 225200 training loss 0.65228087 lr 0.00016\n",
      "iteration 225300 training loss 0.82836294 lr 0.00016\n",
      "iteration 225400 training loss 0.5786441 lr 0.00016\n",
      "iteration 225500 training loss 0.6136901 lr 0.00016\n",
      "iteration 225600 training loss 0.7384687 lr 0.00016\n",
      "iteration 225700 training loss 0.57992554 lr 0.00016\n",
      "iteration 225800 training loss 0.7089904 lr 0.00016\n",
      "iteration 225900 training loss 0.7386216 lr 0.00016\n",
      "iteration 226000 training loss 0.7419186 lr 0.00016\n",
      "iteration 226100 training loss 0.7860561 lr 0.00016\n",
      "iteration 226200 training loss 0.6166349 lr 0.00016\n",
      "iteration 226300 training loss 0.93368816 lr 0.00016\n",
      "iteration 226400 training loss 0.8199425 lr 0.00016\n",
      "iteration 226500 training loss 0.7844851 lr 0.00016\n",
      "iteration 226600 training loss 0.9033315 lr 0.00016\n",
      "iteration 226700 training loss 0.71308774 lr 0.00016\n",
      "iteration 226800 training loss 0.82259786 lr 0.00016\n",
      "iteration 226900 training loss 0.7610239 lr 0.00016\n",
      "iteration 227000 training loss 0.81189317 lr 0.00016\n",
      "iteration 227100 training loss 0.62661356 lr 0.00016\n",
      "iteration 227200 training loss 0.8911222 lr 0.00016\n",
      "iteration 227300 training loss 0.6864836 lr 0.00016\n",
      "iteration 227400 training loss 0.5635828 lr 0.00016\n",
      "iteration 227500 training loss 0.6797897 lr 0.00016\n",
      "iteration 227600 training loss 0.6381884 lr 0.00016\n",
      "iteration 227700 training loss 0.62561 lr 0.00016\n",
      "iteration 227800 training loss 0.6423966 lr 0.00016\n",
      "iteration 227900 training loss 0.86690575 lr 0.00016\n",
      "iteration 228000 training loss 0.7697175 lr 0.00016\n",
      "iteration 228100 training loss 0.77698237 lr 0.00016\n",
      "iteration 228200 training loss 0.6889126 lr 0.00016\n",
      "iteration 228300 training loss 0.585611 lr 0.00016\n",
      "iteration 228400 training loss 0.66207296 lr 0.00016\n",
      "iteration 228500 training loss 0.68615913 lr 0.00016\n",
      "iteration 228600 training loss 0.581393 lr 0.00016\n",
      "iteration 228700 training loss 0.49641034 lr 0.00016\n",
      "iteration 228800 training loss 0.6264172 lr 0.00016\n",
      "iteration 228900 training loss 0.81321627 lr 0.00016\n",
      "iteration 229000 training loss 0.64058185 lr 0.00016\n",
      "iteration 229100 training loss 0.60407174 lr 0.00016\n",
      "iteration 229200 training loss 0.67253596 lr 0.00016\n",
      "iteration 229300 training loss 0.79673123 lr 0.00016\n",
      "iteration 229400 training loss 0.65410936 lr 0.00016\n",
      "iteration 229500 training loss 0.6603617 lr 0.00016\n",
      "iteration 229600 training loss 0.52961004 lr 0.00016\n",
      "iteration 229700 training loss 0.67582923 lr 0.00016\n",
      "iteration 229800 training loss 0.7030336 lr 0.00016\n",
      "iteration 229900 training loss 0.70028156 lr 0.00016\n",
      "iteration 230000 training loss 0.7373635 lr 0.00016\n",
      "iteration 230100 training loss 0.6548475 lr 0.00016\n",
      "iteration 230200 training loss 0.6951846 lr 0.00016\n",
      "iteration 230300 training loss 0.7430977 lr 0.00016\n",
      "iteration 230400 training loss 0.5831786 lr 0.00016\n",
      "iteration 230500 training loss 0.5553856 lr 0.00016\n",
      "iteration 230600 training loss 0.69655955 lr 0.00016\n",
      "iteration 230700 training loss 0.59824586 lr 0.00016\n",
      "iteration 230800 training loss 0.5786327 lr 0.00016\n",
      "iteration 230900 training loss 0.82057595 lr 0.00016\n",
      "iteration 231000 training loss 0.6890118 lr 0.00016\n",
      "iteration 231100 training loss 0.67004365 lr 0.00016\n",
      "iteration 231200 training loss 0.72757584 lr 0.00016\n",
      "iteration 231300 training loss 0.77144116 lr 0.00016\n",
      "iteration 231400 training loss 0.8001655 lr 0.00016\n",
      "iteration 231500 training loss 0.868454 lr 0.00016\n",
      "iteration 231600 training loss 0.75840616 lr 0.00016\n",
      "iteration 231700 training loss 0.6789526 lr 0.00016\n",
      "iteration 231800 training loss 0.61365914 lr 0.00016\n",
      "iteration 231900 training loss 0.74552584 lr 0.00016\n",
      "iteration 232000 training loss 0.6456753 lr 0.00016\n",
      "iteration 232100 training loss 0.6660651 lr 0.00016\n",
      "iteration 232200 training loss 0.6911268 lr 0.00016\n",
      "iteration 232300 training loss 0.65867686 lr 0.00016\n",
      "iteration 232400 training loss 0.751486 lr 0.00016\n",
      "iteration 232500 training loss 0.7200696 lr 0.00016\n",
      "iteration 232600 training loss 0.57228684 lr 0.00016\n",
      "iteration 232700 training loss 0.6643347 lr 0.00016\n",
      "iteration 232800 training loss 0.67140186 lr 0.00016\n",
      "iteration 232900 training loss 0.67919797 lr 0.00016\n",
      "iteration 233000 training loss 0.8397645 lr 0.00016\n",
      "iteration 233100 training loss 0.7598024 lr 0.00016\n",
      "iteration 233200 training loss 0.55036634 lr 0.00016\n",
      "iteration 233300 training loss 0.8710267 lr 0.00016\n",
      "iteration 233400 training loss 0.76332814 lr 0.00016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 233500 training loss 0.66868067 lr 0.00016\n",
      "iteration 233600 training loss 0.6746365 lr 0.00016\n",
      "iteration 233700 training loss 0.7748094 lr 0.00016\n",
      "iteration 233800 training loss 0.54660636 lr 0.00016\n",
      "iteration 233900 training loss 0.82056105 lr 0.00016\n",
      "iteration 234000 training loss 0.5059816 lr 0.00016\n",
      "iteration 234100 training loss 0.5493702 lr 0.00016\n",
      "iteration 234200 training loss 0.7074676 lr 0.00016\n",
      "iteration 234300 training loss 0.46292925 lr 0.00016\n",
      "iteration 234400 training loss 0.6662003 lr 0.00016\n",
      "iteration 234500 training loss 0.763173 lr 0.00016\n",
      "iteration 234600 training loss 0.6416775 lr 0.00016\n",
      "iteration 234700 training loss 0.67820543 lr 0.00016\n",
      "iteration 234800 training loss 0.88025653 lr 0.00016\n",
      "iteration 234900 training loss 0.75837094 lr 0.00016\n",
      "iteration 235000 training loss 0.6377678 lr 0.00016\n",
      "iteration 235100 training loss 0.7229414 lr 0.00016\n",
      "iteration 235200 training loss 0.80711323 lr 0.00016\n",
      "iteration 235300 training loss 0.7453455 lr 0.00016\n",
      "iteration 235400 training loss 0.61937046 lr 0.00016\n",
      "iteration 235500 training loss 0.6332083 lr 0.00016\n",
      "iteration 235600 training loss 0.65793264 lr 0.00016\n",
      "iteration 235700 training loss 0.6518442 lr 0.00016\n",
      "iteration 235800 training loss 0.6806708 lr 0.00016\n",
      "iteration 235900 training loss 0.8192424 lr 0.00016\n",
      "iteration 236000 training loss 0.8734687 lr 0.00016\n",
      "iteration 236100 training loss 0.71715075 lr 0.00016\n",
      "iteration 236200 training loss 0.81402004 lr 0.00016\n",
      "iteration 236300 training loss 0.7065289 lr 0.00016\n",
      "iteration 236400 training loss 0.7802162 lr 0.00016\n",
      "iteration 236500 training loss 0.85795045 lr 0.00016\n",
      "iteration 236600 training loss 0.7338315 lr 0.00016\n",
      "iteration 236700 training loss 0.667959 lr 0.00016\n",
      "iteration 236800 training loss 0.5166439 lr 0.00016\n",
      "iteration 236900 training loss 0.7168516 lr 0.00016\n",
      "iteration 237000 training loss 0.83670604 lr 0.00016\n",
      "iteration 237100 training loss 0.55356157 lr 0.00016\n",
      "iteration 237200 training loss 0.5887669 lr 0.00016\n",
      "iteration 237300 training loss 0.86693335 lr 0.00016\n",
      "iteration 237400 training loss 0.95904636 lr 0.00016\n",
      "iteration 237500 training loss 0.6734419 lr 0.00016\n",
      "iteration 237600 training loss 0.6929712 lr 0.00016\n",
      "iteration 237700 training loss 0.7357623 lr 0.00016\n",
      "iteration 237800 training loss 0.4252523 lr 0.00016\n",
      "iteration 237900 training loss 0.708199 lr 0.00016\n",
      "iteration 238000 training loss 0.6582489 lr 0.00016\n",
      "iteration 238100 training loss 0.7499662 lr 0.00016\n",
      "iteration 238200 training loss 0.73463464 lr 0.00016\n",
      "iteration 238300 training loss 0.54990804 lr 0.00016\n",
      "iteration 238400 training loss 0.66767246 lr 0.00016\n",
      "iteration 238500 training loss 0.7282095 lr 0.00016\n",
      "iteration 238600 training loss 0.8694431 lr 0.00016\n",
      "iteration 238700 training loss 0.5332443 lr 0.00016\n",
      "iteration 238800 training loss 0.7116215 lr 0.00016\n",
      "iteration 238900 training loss 0.744444 lr 0.00016\n",
      "iteration 239000 training loss 0.8384257 lr 0.00016\n",
      "iteration 239100 training loss 0.6602905 lr 0.00016\n",
      "iteration 239200 training loss 0.7378308 lr 0.00016\n",
      "iteration 239300 training loss 0.8207288 lr 0.00016\n",
      "iteration 239400 training loss 0.5738889 lr 0.00016\n",
      "iteration 239500 training loss 0.47170192 lr 0.00016\n",
      "iteration 239600 training loss 0.6037565 lr 0.00016\n",
      "iteration 239700 training loss 0.52549034 lr 0.00016\n",
      "iteration 239800 training loss 0.5515497 lr 0.00016\n",
      "iteration 239900 training loss 0.6343646 lr 0.00016\n",
      "iteration 240000 training loss 0.8640198 lr 0.00016\n",
      "layout:nlp:random 0.800695382309668\n",
      "layout:nlp:default 0.41575735183952256\n",
      "layout:xla:random 0.4532722664859295\n",
      "layout:xla:default 0.2016439833892818\n",
      "epoch 0, it 240000 validation loss -0.468\n",
      "iteration 240100 training loss 0.624375 lr 0.00014\n",
      "iteration 240200 training loss 0.91571546 lr 0.00014\n",
      "iteration 240300 training loss 0.6966388 lr 0.00014\n",
      "iteration 240400 training loss 0.6381376 lr 0.00014\n",
      "iteration 240500 training loss 0.73739684 lr 0.00014\n",
      "iteration 240600 training loss 0.7157436 lr 0.00014\n",
      "iteration 240700 training loss 0.55071276 lr 0.00014\n",
      "iteration 240800 training loss 0.6231966 lr 0.00014\n",
      "iteration 240900 training loss 0.764287 lr 0.00014\n",
      "iteration 241000 training loss 0.7103269 lr 0.00014\n",
      "iteration 241100 training loss 0.60726464 lr 0.00014\n",
      "iteration 241200 training loss 0.663297 lr 0.00014\n",
      "iteration 241300 training loss 0.5685565 lr 0.00014\n",
      "iteration 241400 training loss 0.6753576 lr 0.00014\n",
      "iteration 241500 training loss 0.5771308 lr 0.00014\n",
      "iteration 241600 training loss 0.5964968 lr 0.00014\n",
      "iteration 241700 training loss 0.61330557 lr 0.00014\n",
      "iteration 241800 training loss 0.73403144 lr 0.00014\n",
      "iteration 241900 training loss 0.62385 lr 0.00014\n",
      "iteration 242000 training loss 0.603588 lr 0.00014\n",
      "iteration 242100 training loss 0.67308795 lr 0.00014\n",
      "iteration 242200 training loss 0.75982904 lr 0.00014\n",
      "iteration 242300 training loss 0.82380086 lr 0.00014\n",
      "iteration 242400 training loss 0.59504414 lr 0.00014\n",
      "iteration 242500 training loss 0.7525038 lr 0.00014\n",
      "iteration 242600 training loss 0.6652716 lr 0.00014\n",
      "iteration 242700 training loss 0.6933618 lr 0.00014\n",
      "iteration 242800 training loss 0.6088293 lr 0.00014\n",
      "iteration 242900 training loss 0.7901208 lr 0.00014\n",
      "iteration 243000 training loss 0.69791204 lr 0.00014\n",
      "iteration 243100 training loss 0.7553611 lr 0.00014\n",
      "iteration 243200 training loss 0.6287449 lr 0.00014\n",
      "iteration 243300 training loss 0.6389992 lr 0.00014\n",
      "iteration 243400 training loss 0.57450706 lr 0.00014\n",
      "iteration 243500 training loss 0.7544371 lr 0.00014\n",
      "iteration 243600 training loss 0.79091233 lr 0.00014\n",
      "iteration 243700 training loss 0.6188005 lr 0.00014\n",
      "iteration 243800 training loss 0.7410089 lr 0.00014\n",
      "iteration 243900 training loss 0.8574728 lr 0.00014\n",
      "iteration 244000 training loss 0.794106 lr 0.00014\n",
      "iteration 244100 training loss 0.87357163 lr 0.00014\n",
      "iteration 244200 training loss 0.83341837 lr 0.00014\n",
      "iteration 244300 training loss 0.6128467 lr 0.00014\n",
      "iteration 244400 training loss 0.61056757 lr 0.00014\n",
      "iteration 244500 training loss 0.6678503 lr 0.00014\n",
      "iteration 244600 training loss 0.70012146 lr 0.00014\n",
      "iteration 244700 training loss 0.67226666 lr 0.00014\n",
      "iteration 244800 training loss 0.6506858 lr 0.00014\n",
      "iteration 244900 training loss 0.7884041 lr 0.00014\n",
      "iteration 245000 training loss 0.6832825 lr 0.00014\n",
      "iteration 245100 training loss 0.530175 lr 0.00014\n",
      "iteration 245200 training loss 0.6964561 lr 0.00014\n",
      "iteration 245300 training loss 0.5772868 lr 0.00014\n",
      "iteration 245400 training loss 0.73193973 lr 0.00014\n",
      "iteration 245500 training loss 0.8685037 lr 0.00014\n",
      "iteration 245600 training loss 0.8224567 lr 0.00014\n",
      "iteration 245700 training loss 0.6956613 lr 0.00014\n",
      "iteration 245800 training loss 0.726769 lr 0.00014\n",
      "iteration 245900 training loss 0.74250925 lr 0.00014\n",
      "iteration 246000 training loss 0.5408276 lr 0.00014\n",
      "iteration 246100 training loss 0.7919925 lr 0.00014\n",
      "iteration 246200 training loss 0.6701187 lr 0.00014\n",
      "iteration 246300 training loss 0.68884677 lr 0.00014\n",
      "iteration 246400 training loss 0.8526881 lr 0.00014\n",
      "iteration 246500 training loss 0.7221266 lr 0.00014\n",
      "iteration 246600 training loss 0.87522036 lr 0.00014\n",
      "iteration 246700 training loss 0.70011234 lr 0.00014\n",
      "iteration 246800 training loss 0.6349392 lr 0.00014\n",
      "iteration 246900 training loss 0.70851034 lr 0.00014\n",
      "iteration 247000 training loss 0.74279326 lr 0.00014\n",
      "iteration 247100 training loss 0.52281374 lr 0.00014\n",
      "iteration 247200 training loss 0.61412066 lr 0.00014\n",
      "iteration 247300 training loss 0.596549 lr 0.00014\n",
      "iteration 247400 training loss 0.84589463 lr 0.00014\n",
      "iteration 247500 training loss 0.72577304 lr 0.00014\n",
      "iteration 247600 training loss 0.76874995 lr 0.00014\n",
      "iteration 247700 training loss 0.7722264 lr 0.00014\n",
      "iteration 247800 training loss 0.6435191 lr 0.00014\n",
      "iteration 247900 training loss 0.76616216 lr 0.00014\n",
      "iteration 248000 training loss 0.6730381 lr 0.00014\n",
      "iteration 248100 training loss 0.59034103 lr 0.00014\n",
      "iteration 248200 training loss 0.76263946 lr 0.00014\n",
      "iteration 248300 training loss 0.619388 lr 0.00014\n",
      "iteration 248400 training loss 0.9339218 lr 0.00014\n",
      "iteration 248500 training loss 0.7922161 lr 0.00014\n",
      "iteration 248600 training loss 0.63534456 lr 0.00014\n",
      "iteration 248700 training loss 0.5807701 lr 0.00014\n",
      "iteration 248800 training loss 0.65844417 lr 0.00014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 248900 training loss 0.5969861 lr 0.00014\n",
      "iteration 249000 training loss 0.88070893 lr 0.00014\n",
      "iteration 249100 training loss 0.77142155 lr 0.00014\n",
      "iteration 249200 training loss 0.75334865 lr 0.00014\n",
      "iteration 249300 training loss 0.7683218 lr 0.00014\n",
      "iteration 249400 training loss 0.67667586 lr 0.00014\n",
      "iteration 249500 training loss 0.5646233 lr 0.00014\n",
      "iteration 249600 training loss 0.63680005 lr 0.00014\n",
      "iteration 249700 training loss 0.7263418 lr 0.00014\n",
      "iteration 249800 training loss 0.6725487 lr 0.00014\n",
      "iteration 249900 training loss 0.6992765 lr 0.00014\n",
      "iteration 250000 training loss 0.7346573 lr 0.00014\n",
      "iteration 250100 training loss 0.9181316 lr 0.00014\n",
      "iteration 250200 training loss 0.81824946 lr 0.00014\n",
      "iteration 250300 training loss 0.7017255 lr 0.00014\n",
      "iteration 250400 training loss 0.708559 lr 0.00014\n",
      "iteration 250500 training loss 0.51054454 lr 0.00014\n",
      "iteration 250600 training loss 0.90427256 lr 0.00014\n",
      "iteration 250700 training loss 0.6312002 lr 0.00014\n",
      "iteration 250800 training loss 0.73378897 lr 0.00014\n",
      "iteration 250900 training loss 0.6704013 lr 0.00014\n",
      "iteration 251000 training loss 0.66120946 lr 0.00014\n",
      "iteration 251100 training loss 0.63033926 lr 0.00014\n",
      "iteration 251200 training loss 0.8032589 lr 0.00014\n",
      "iteration 251300 training loss 0.60777736 lr 0.00014\n",
      "iteration 251400 training loss 0.70612764 lr 0.00014\n",
      "iteration 251500 training loss 0.6280925 lr 0.00014\n",
      "iteration 251600 training loss 0.45313495 lr 0.00014\n",
      "iteration 251700 training loss 0.69273 lr 0.00014\n",
      "iteration 251800 training loss 0.73988706 lr 0.00014\n",
      "iteration 251900 training loss 0.55130345 lr 0.00014\n",
      "iteration 252000 training loss 0.7052675 lr 0.00014\n",
      "iteration 252100 training loss 0.6752871 lr 0.00014\n",
      "iteration 252200 training loss 0.48589897 lr 0.00014\n",
      "iteration 252300 training loss 0.6955046 lr 0.00014\n",
      "iteration 252400 training loss 0.7016951 lr 0.00014\n",
      "iteration 252500 training loss 0.7631022 lr 0.00014\n",
      "iteration 252600 training loss 0.6846007 lr 0.00014\n",
      "iteration 252700 training loss 0.774884 lr 0.00014\n",
      "iteration 252800 training loss 0.7645927 lr 0.00014\n",
      "iteration 252900 training loss 0.5483978 lr 0.00014\n",
      "iteration 253000 training loss 0.68198305 lr 0.00014\n",
      "iteration 253100 training loss 0.6288848 lr 0.00014\n",
      "iteration 253200 training loss 0.6113659 lr 0.00014\n",
      "iteration 253300 training loss 0.6325852 lr 0.00014\n",
      "iteration 253400 training loss 0.6150119 lr 0.00014\n",
      "iteration 253500 training loss 0.6618685 lr 0.00014\n",
      "iteration 253600 training loss 0.7856721 lr 0.00014\n",
      "iteration 253700 training loss 0.6939231 lr 0.00014\n",
      "iteration 253800 training loss 0.6607635 lr 0.00014\n",
      "iteration 253900 training loss 0.7530916 lr 0.00014\n",
      "iteration 254000 training loss 0.72039336 lr 0.00014\n",
      "iteration 254100 training loss 0.93021744 lr 0.00014\n",
      "iteration 254200 training loss 0.64137214 lr 0.00014\n",
      "iteration 254300 training loss 0.5517748 lr 0.00014\n",
      "iteration 254400 training loss 0.7888265 lr 0.00014\n",
      "iteration 254500 training loss 0.82652915 lr 0.00014\n",
      "iteration 254600 training loss 0.88320506 lr 0.00014\n",
      "iteration 254700 training loss 0.7671108 lr 0.00014\n",
      "iteration 254800 training loss 0.7034725 lr 0.00014\n",
      "iteration 254900 training loss 0.6857707 lr 0.00014\n",
      "iteration 255000 training loss 0.6789094 lr 0.00014\n",
      "iteration 255100 training loss 0.63900566 lr 0.00014\n",
      "iteration 255200 training loss 0.6540444 lr 0.00014\n",
      "iteration 255300 training loss 0.67927814 lr 0.00014\n",
      "iteration 255400 training loss 0.6324673 lr 0.00014\n",
      "iteration 255500 training loss 0.80793834 lr 0.00014\n",
      "iteration 255600 training loss 0.78060466 lr 0.00014\n",
      "iteration 255700 training loss 0.7017242 lr 0.00014\n",
      "iteration 255800 training loss 0.5627841 lr 0.00014\n",
      "iteration 255900 training loss 0.57968575 lr 0.00014\n",
      "iteration 256000 training loss 0.6650677 lr 0.00014\n",
      "iteration 256100 training loss 0.77250475 lr 0.00014\n",
      "iteration 256200 training loss 0.73114645 lr 0.00014\n",
      "iteration 256300 training loss 0.55073166 lr 0.00014\n",
      "iteration 256400 training loss 0.7784762 lr 0.00014\n",
      "iteration 256500 training loss 0.75580525 lr 0.00014\n",
      "iteration 256600 training loss 0.8180562 lr 0.00014\n",
      "iteration 256700 training loss 0.7002382 lr 0.00014\n",
      "iteration 256800 training loss 0.78866863 lr 0.00014\n",
      "iteration 256900 training loss 0.75344336 lr 0.00014\n",
      "iteration 257000 training loss 0.826151 lr 0.00014\n",
      "iteration 257100 training loss 0.47948858 lr 0.00014\n",
      "iteration 257200 training loss 0.64245784 lr 0.00014\n",
      "iteration 257300 training loss 0.5134437 lr 0.00014\n",
      "iteration 257400 training loss 0.58948815 lr 0.00014\n",
      "iteration 257500 training loss 0.701095 lr 0.00014\n",
      "iteration 257600 training loss 0.97108924 lr 0.00014\n",
      "iteration 257700 training loss 0.6216518 lr 0.00014\n",
      "iteration 257800 training loss 0.7379964 lr 0.00014\n",
      "iteration 257900 training loss 0.5789619 lr 0.00014\n",
      "iteration 258000 training loss 0.5137015 lr 0.00014\n",
      "iteration 258100 training loss 0.6994944 lr 0.00014\n",
      "iteration 258200 training loss 0.5823687 lr 0.00014\n",
      "iteration 258300 training loss 0.53396714 lr 0.00014\n",
      "iteration 258400 training loss 0.74079597 lr 0.00014\n",
      "iteration 258500 training loss 0.58882177 lr 0.00014\n",
      "iteration 258600 training loss 0.60259914 lr 0.00014\n",
      "iteration 258700 training loss 0.64117813 lr 0.00014\n",
      "iteration 258800 training loss 0.52122474 lr 0.00014\n",
      "iteration 258900 training loss 0.691718 lr 0.00014\n",
      "iteration 259000 training loss 0.49165544 lr 0.00014\n",
      "iteration 259100 training loss 0.5598041 lr 0.00014\n",
      "iteration 259200 training loss 0.62409735 lr 0.00014\n",
      "iteration 259300 training loss 0.542071 lr 0.00014\n",
      "iteration 259400 training loss 0.65295494 lr 0.00014\n",
      "iteration 259500 training loss 0.6340373 lr 0.00014\n",
      "iteration 259600 training loss 0.61914825 lr 0.00014\n",
      "iteration 259700 training loss 0.6090861 lr 0.00014\n",
      "iteration 259800 training loss 0.6707367 lr 0.00014\n",
      "iteration 259900 training loss 0.67826784 lr 0.00014\n",
      "iteration 260000 training loss 0.59589076 lr 0.00014\n",
      "layout:nlp:random 0.8104664165766261\n",
      "layout:nlp:default 0.42245118077031385\n",
      "layout:xla:random 0.47429497515731267\n",
      "layout:xla:default 0.18646078513936049\n",
      "epoch 0, it 260000 validation loss -0.473\n",
      "iteration 260100 training loss 0.7837818 lr 0.00013\n",
      "iteration 260200 training loss 0.608427 lr 0.00013\n",
      "iteration 260300 training loss 0.83519787 lr 0.00013\n",
      "iteration 260400 training loss 0.6620584 lr 0.00013\n",
      "iteration 260500 training loss 0.505597 lr 0.00013\n",
      "iteration 260600 training loss 0.9254312 lr 0.00013\n",
      "iteration 260700 training loss 0.6588053 lr 0.00013\n",
      "iteration 260800 training loss 0.73943764 lr 0.00013\n",
      "iteration 260900 training loss 0.5670496 lr 0.00013\n",
      "iteration 261000 training loss 0.5961755 lr 0.00013\n",
      "iteration 261100 training loss 0.83272535 lr 0.00013\n",
      "iteration 261200 training loss 0.65255183 lr 0.00013\n",
      "iteration 261300 training loss 0.7134923 lr 0.00013\n",
      "iteration 261400 training loss 0.6248134 lr 0.00013\n",
      "iteration 261500 training loss 0.599776 lr 0.00013\n",
      "iteration 261600 training loss 0.6431496 lr 0.00013\n",
      "iteration 261700 training loss 0.6847233 lr 0.00013\n",
      "iteration 261800 training loss 0.8091045 lr 0.00013\n",
      "iteration 261900 training loss 0.75569373 lr 0.00013\n",
      "iteration 262000 training loss 0.5125873 lr 0.00013\n",
      "iteration 262100 training loss 0.7021283 lr 0.00013\n",
      "iteration 262200 training loss 0.8716644 lr 0.00013\n",
      "iteration 262300 training loss 0.8132827 lr 0.00013\n",
      "iteration 262400 training loss 0.6916718 lr 0.00013\n",
      "iteration 262500 training loss 0.71321756 lr 0.00013\n",
      "iteration 262600 training loss 0.70278925 lr 0.00013\n",
      "iteration 262700 training loss 0.6828648 lr 0.00013\n",
      "iteration 262800 training loss 0.7774081 lr 0.00013\n",
      "iteration 262900 training loss 0.7138514 lr 0.00013\n",
      "iteration 263000 training loss 0.79414624 lr 0.00013\n",
      "iteration 263100 training loss 0.8267335 lr 0.00013\n",
      "iteration 263200 training loss 0.69496876 lr 0.00013\n",
      "iteration 263300 training loss 0.704212 lr 0.00013\n",
      "iteration 263400 training loss 0.82292646 lr 0.00013\n",
      "iteration 263500 training loss 0.7246121 lr 0.00013\n",
      "iteration 263600 training loss 0.6975513 lr 0.00013\n",
      "iteration 263700 training loss 0.840675 lr 0.00013\n",
      "iteration 263800 training loss 0.6294438 lr 0.00013\n",
      "iteration 263900 training loss 0.725619 lr 0.00013\n",
      "iteration 264000 training loss 0.826668 lr 0.00013\n",
      "iteration 264100 training loss 0.68465 lr 0.00013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 264200 training loss 0.6927472 lr 0.00013\n",
      "iteration 264300 training loss 0.7470523 lr 0.00013\n",
      "iteration 264400 training loss 0.6324262 lr 0.00013\n",
      "iteration 264500 training loss 0.6494667 lr 0.00013\n",
      "iteration 264600 training loss 0.67532814 lr 0.00013\n",
      "iteration 264700 training loss 0.59133273 lr 0.00013\n",
      "iteration 264800 training loss 0.6913521 lr 0.00013\n",
      "iteration 264900 training loss 0.7429007 lr 0.00013\n",
      "iteration 265000 training loss 0.48391733 lr 0.00013\n",
      "iteration 265100 training loss 0.6335089 lr 0.00013\n",
      "iteration 265200 training loss 0.704682 lr 0.00013\n",
      "iteration 265300 training loss 0.6256661 lr 0.00013\n",
      "iteration 265400 training loss 0.7664447 lr 0.00013\n",
      "iteration 265500 training loss 0.74750954 lr 0.00013\n",
      "iteration 265600 training loss 0.7261769 lr 0.00013\n",
      "iteration 265700 training loss 0.6638742 lr 0.00013\n",
      "iteration 265800 training loss 0.6630368 lr 0.00013\n",
      "iteration 265900 training loss 0.57877713 lr 0.00013\n",
      "iteration 266000 training loss 0.8419664 lr 0.00013\n",
      "iteration 266100 training loss 0.7139579 lr 0.00013\n",
      "iteration 266200 training loss 0.66871643 lr 0.00013\n",
      "iteration 266300 training loss 0.8375164 lr 0.00013\n",
      "iteration 266400 training loss 0.6100594 lr 0.00013\n",
      "iteration 266500 training loss 0.7023435 lr 0.00013\n",
      "iteration 266600 training loss 0.69509465 lr 0.00013\n",
      "iteration 266700 training loss 0.6382244 lr 0.00013\n",
      "iteration 266800 training loss 0.72190106 lr 0.00013\n",
      "iteration 266900 training loss 0.61264735 lr 0.00013\n",
      "iteration 267000 training loss 0.7877033 lr 0.00013\n",
      "iteration 267100 training loss 0.6634188 lr 0.00013\n",
      "iteration 267200 training loss 0.47723985 lr 0.00013\n",
      "iteration 267300 training loss 0.76265675 lr 0.00013\n",
      "iteration 267400 training loss 0.8230547 lr 0.00013\n",
      "iteration 267500 training loss 0.91278976 lr 0.00013\n",
      "iteration 267600 training loss 0.54303163 lr 0.00013\n",
      "iteration 267700 training loss 0.54493254 lr 0.00013\n",
      "iteration 267800 training loss 0.60869145 lr 0.00013\n",
      "iteration 267900 training loss 0.5368707 lr 0.00013\n",
      "iteration 268000 training loss 0.711521 lr 0.00013\n",
      "iteration 268100 training loss 0.7273563 lr 0.00013\n",
      "iteration 268200 training loss 0.6048332 lr 0.00013\n",
      "iteration 268300 training loss 0.49645564 lr 0.00013\n",
      "iteration 268400 training loss 0.5393881 lr 0.00013\n",
      "iteration 268500 training loss 0.69135994 lr 0.00013\n",
      "iteration 268600 training loss 0.6528429 lr 0.00013\n",
      "iteration 268700 training loss 0.5672332 lr 0.00013\n",
      "iteration 268800 training loss 0.6680244 lr 0.00013\n",
      "iteration 268900 training loss 0.6782466 lr 0.00013\n",
      "iteration 269000 training loss 0.6731808 lr 0.00013\n",
      "iteration 269100 training loss 0.65367895 lr 0.00013\n",
      "iteration 269200 training loss 0.77933097 lr 0.00013\n",
      "iteration 269300 training loss 0.693483 lr 0.00013\n",
      "iteration 269400 training loss 0.8345783 lr 0.00013\n",
      "iteration 269500 training loss 0.5143091 lr 0.00013\n",
      "iteration 269600 training loss 0.6647744 lr 0.00013\n",
      "iteration 269700 training loss 0.53928345 lr 0.00013\n",
      "iteration 269800 training loss 0.6962374 lr 0.00013\n",
      "iteration 269900 training loss 0.66104835 lr 0.00013\n",
      "iteration 270000 training loss 0.69193643 lr 0.00013\n",
      "iteration 270100 training loss 0.8090317 lr 0.00013\n",
      "iteration 270200 training loss 0.5845292 lr 0.00013\n",
      "iteration 270300 training loss 0.7839808 lr 0.00013\n",
      "iteration 270400 training loss 0.79074985 lr 0.00013\n",
      "iteration 270500 training loss 0.57364005 lr 0.00013\n",
      "iteration 270600 training loss 0.5362387 lr 0.00013\n",
      "iteration 270700 training loss 0.5011996 lr 0.00013\n",
      "iteration 270800 training loss 0.6391239 lr 0.00013\n",
      "iteration 270900 training loss 0.6550959 lr 0.00013\n",
      "iteration 271000 training loss 0.5504371 lr 0.00013\n",
      "iteration 271100 training loss 0.73550725 lr 0.00013\n",
      "iteration 271200 training loss 0.65977997 lr 0.00013\n",
      "iteration 271300 training loss 0.6366573 lr 0.00013\n",
      "iteration 271400 training loss 0.6260549 lr 0.00013\n",
      "iteration 271500 training loss 0.72719014 lr 0.00013\n",
      "iteration 271600 training loss 0.7402749 lr 0.00013\n",
      "iteration 271700 training loss 0.538871 lr 0.00013\n",
      "iteration 271800 training loss 0.61038744 lr 0.00013\n",
      "iteration 271900 training loss 0.5413668 lr 0.00013\n",
      "iteration 272000 training loss 0.74582374 lr 0.00013\n",
      "iteration 272100 training loss 0.6226951 lr 0.00013\n",
      "iteration 272200 training loss 0.5809395 lr 0.00013\n",
      "iteration 272300 training loss 0.75469905 lr 0.00013\n",
      "iteration 272400 training loss 0.6625995 lr 0.00013\n",
      "iteration 272500 training loss 0.7457007 lr 0.00013\n",
      "iteration 272600 training loss 0.6259341 lr 0.00013\n",
      "iteration 272700 training loss 0.905377 lr 0.00013\n",
      "iteration 272800 training loss 0.756088 lr 0.00013\n",
      "iteration 272900 training loss 0.6347833 lr 0.00013\n",
      "iteration 273000 training loss 0.68402666 lr 0.00013\n",
      "iteration 273100 training loss 0.67014885 lr 0.00013\n",
      "iteration 273200 training loss 0.67975736 lr 0.00013\n",
      "iteration 273300 training loss 0.59836483 lr 0.00013\n",
      "iteration 273400 training loss 0.5882616 lr 0.00013\n",
      "iteration 273500 training loss 0.50241417 lr 0.00013\n",
      "iteration 273600 training loss 0.6178613 lr 0.00013\n",
      "iteration 273700 training loss 0.50641906 lr 0.00013\n",
      "iteration 273800 training loss 0.57226187 lr 0.00013\n",
      "iteration 273900 training loss 0.6164451 lr 0.00013\n",
      "iteration 274000 training loss 0.7099519 lr 0.00013\n",
      "iteration 274100 training loss 0.5199108 lr 0.00013\n",
      "iteration 274200 training loss 0.69297945 lr 0.00013\n",
      "iteration 274300 training loss 0.76060647 lr 0.00013\n",
      "iteration 274400 training loss 0.6465518 lr 0.00013\n",
      "iteration 274500 training loss 0.52126545 lr 0.00013\n",
      "iteration 274600 training loss 0.5863712 lr 0.00013\n",
      "iteration 274700 training loss 0.60019535 lr 0.00013\n",
      "iteration 274800 training loss 0.70833343 lr 0.00013\n",
      "iteration 274900 training loss 0.7732643 lr 0.00013\n",
      "iteration 275000 training loss 0.93944323 lr 0.00013\n",
      "iteration 275100 training loss 0.622199 lr 0.00013\n",
      "iteration 275200 training loss 0.7401655 lr 0.00013\n",
      "iteration 275300 training loss 0.9554522 lr 0.00013\n",
      "iteration 275400 training loss 0.99515635 lr 0.00013\n",
      "iteration 275500 training loss 0.849135 lr 0.00013\n",
      "iteration 275600 training loss 0.5620526 lr 0.00013\n",
      "iteration 275700 training loss 0.66460735 lr 0.00013\n",
      "iteration 275800 training loss 0.89762187 lr 0.00013\n",
      "iteration 275900 training loss 0.6291022 lr 0.00013\n",
      "iteration 276000 training loss 0.7139534 lr 0.00013\n",
      "iteration 276100 training loss 0.7661247 lr 0.00013\n",
      "iteration 276200 training loss 0.5889916 lr 0.00013\n",
      "iteration 276300 training loss 0.68534 lr 0.00013\n",
      "iteration 276400 training loss 0.65567267 lr 0.00013\n",
      "iteration 276500 training loss 0.55805653 lr 0.00013\n",
      "iteration 276600 training loss 0.7259172 lr 0.00013\n",
      "iteration 276700 training loss 0.6231814 lr 0.00013\n",
      "iteration 276800 training loss 0.7714319 lr 0.00013\n",
      "iteration 276900 training loss 0.7122882 lr 0.00013\n",
      "iteration 277000 training loss 0.621492 lr 0.00013\n",
      "iteration 277100 training loss 0.6858685 lr 0.00013\n",
      "iteration 277200 training loss 0.7942589 lr 0.00013\n",
      "iteration 277300 training loss 0.6495038 lr 0.00013\n",
      "iteration 277400 training loss 0.81114715 lr 0.00013\n",
      "iteration 277500 training loss 0.7351053 lr 0.00013\n",
      "iteration 277600 training loss 0.817991 lr 0.00013\n",
      "iteration 277700 training loss 0.5959584 lr 0.00013\n",
      "iteration 277800 training loss 0.6998857 lr 0.00013\n",
      "iteration 277900 training loss 0.91065115 lr 0.00013\n",
      "iteration 278000 training loss 0.5689194 lr 0.00013\n",
      "iteration 278100 training loss 0.7093239 lr 0.00013\n",
      "iteration 278200 training loss 0.6614873 lr 0.00013\n",
      "iteration 278300 training loss 0.82501656 lr 0.00013\n",
      "iteration 278400 training loss 0.5308932 lr 0.00013\n",
      "iteration 278500 training loss 0.73025984 lr 0.00013\n",
      "iteration 278600 training loss 0.8095551 lr 0.00013\n",
      "iteration 278700 training loss 0.7520279 lr 0.00013\n",
      "iteration 278800 training loss 0.6216504 lr 0.00013\n",
      "iteration 278900 training loss 0.80171824 lr 0.00013\n",
      "iteration 279000 training loss 0.8605309 lr 0.00013\n",
      "iteration 279100 training loss 0.63802516 lr 0.00013\n",
      "iteration 279200 training loss 0.83626056 lr 0.00013\n",
      "iteration 279300 training loss 0.68001384 lr 0.00013\n",
      "iteration 279400 training loss 0.74101853 lr 0.00013\n",
      "iteration 279500 training loss 0.6863781 lr 0.00013\n",
      "iteration 279600 training loss 0.5854489 lr 0.00013\n",
      "iteration 279700 training loss 0.91737455 lr 0.00013\n",
      "iteration 279800 training loss 0.64779764 lr 0.00013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 279900 training loss 0.6465333 lr 0.00013\n",
      "iteration 280000 training loss 0.6236445 lr 0.00013\n",
      "layout:nlp:random 0.8144719223471562\n",
      "layout:nlp:default 0.4240587341710505\n",
      "layout:xla:random 0.4947962214761545\n",
      "layout:xla:default 0.2581795786821175\n",
      "epoch 0, it 280000 validation loss -0.498\n",
      "iteration 280100 training loss 0.6755048 lr 0.00011\n",
      "iteration 280200 training loss 0.81842834 lr 0.00011\n",
      "iteration 280300 training loss 0.7838459 lr 0.00011\n",
      "iteration 280400 training loss 0.62770766 lr 0.00011\n",
      "iteration 280500 training loss 0.7110764 lr 0.00011\n",
      "iteration 280600 training loss 0.8005209 lr 0.00011\n",
      "iteration 280700 training loss 0.58564734 lr 0.00011\n",
      "iteration 280800 training loss 0.58182245 lr 0.00011\n",
      "iteration 280900 training loss 0.6666937 lr 0.00011\n",
      "iteration 281000 training loss 0.7603438 lr 0.00011\n",
      "iteration 281100 training loss 0.64069927 lr 0.00011\n",
      "iteration 281200 training loss 0.80714667 lr 0.00011\n",
      "iteration 281300 training loss 0.5480736 lr 0.00011\n",
      "iteration 281400 training loss 0.7478398 lr 0.00011\n",
      "iteration 281500 training loss 0.68914557 lr 0.00011\n",
      "iteration 281600 training loss 0.47753322 lr 0.00011\n",
      "iteration 281700 training loss 0.6950618 lr 0.00011\n",
      "iteration 281800 training loss 0.7200194 lr 0.00011\n",
      "iteration 281900 training loss 0.6908005 lr 0.00011\n",
      "iteration 282000 training loss 0.62831104 lr 0.00011\n",
      "iteration 282100 training loss 0.62580323 lr 0.00011\n",
      "iteration 282200 training loss 0.6563748 lr 0.00011\n",
      "iteration 282300 training loss 0.6803083 lr 0.00011\n",
      "iteration 282400 training loss 0.57209057 lr 0.00011\n",
      "iteration 282500 training loss 0.6005285 lr 0.00011\n",
      "iteration 282600 training loss 0.543722 lr 0.00011\n",
      "iteration 282700 training loss 0.61884487 lr 0.00011\n",
      "iteration 282800 training loss 0.8024562 lr 0.00011\n",
      "iteration 282900 training loss 0.962431 lr 0.00011\n",
      "iteration 283000 training loss 0.6771793 lr 0.00011\n",
      "iteration 283100 training loss 0.6424209 lr 0.00011\n",
      "iteration 283200 training loss 0.65213776 lr 0.00011\n",
      "iteration 283300 training loss 0.7376677 lr 0.00011\n",
      "iteration 283400 training loss 0.6693942 lr 0.00011\n",
      "iteration 283500 training loss 0.675614 lr 0.00011\n",
      "iteration 283600 training loss 0.4557244 lr 0.00011\n",
      "iteration 283700 training loss 0.52274597 lr 0.00011\n",
      "iteration 283800 training loss 1.0096124 lr 0.00011\n",
      "iteration 283900 training loss 0.5885586 lr 0.00011\n",
      "iteration 284000 training loss 0.7572067 lr 0.00011\n",
      "iteration 284100 training loss 0.81560284 lr 0.00011\n",
      "iteration 284200 training loss 0.83478045 lr 0.00011\n",
      "iteration 284300 training loss 0.8009623 lr 0.00011\n",
      "iteration 284400 training loss 1.0121605 lr 0.00011\n",
      "iteration 284500 training loss 0.5715064 lr 0.00011\n",
      "iteration 284600 training loss 0.58419955 lr 0.00011\n",
      "iteration 284700 training loss 0.81094295 lr 0.00011\n",
      "iteration 284800 training loss 0.66978174 lr 0.00011\n",
      "iteration 284900 training loss 0.91133016 lr 0.00011\n",
      "iteration 285000 training loss 0.6341879 lr 0.00011\n",
      "iteration 285100 training loss 0.6365063 lr 0.00011\n",
      "iteration 285200 training loss 0.5335123 lr 0.00011\n",
      "iteration 285300 training loss 0.71803725 lr 0.00011\n",
      "iteration 285400 training loss 0.45259404 lr 0.00011\n",
      "iteration 285500 training loss 0.7796523 lr 0.00011\n",
      "iteration 285600 training loss 0.6447484 lr 0.00011\n",
      "iteration 285700 training loss 0.6226754 lr 0.00011\n",
      "iteration 285800 training loss 0.5799234 lr 0.00011\n",
      "iteration 285900 training loss 0.80643207 lr 0.00011\n",
      "iteration 286000 training loss 0.5552791 lr 0.00011\n",
      "iteration 286100 training loss 0.513784 lr 0.00011\n",
      "iteration 286200 training loss 0.68170875 lr 0.00011\n",
      "iteration 286300 training loss 0.6217958 lr 0.00011\n",
      "iteration 286400 training loss 0.84789175 lr 0.00011\n",
      "iteration 286500 training loss 0.7166342 lr 0.00011\n",
      "iteration 286600 training loss 0.55841684 lr 0.00011\n",
      "iteration 286700 training loss 0.6861921 lr 0.00011\n",
      "iteration 286800 training loss 0.6897138 lr 0.00011\n",
      "iteration 286900 training loss 0.60729873 lr 0.00011\n",
      "iteration 287000 training loss 0.8302543 lr 0.00011\n",
      "iteration 287100 training loss 0.6592631 lr 0.00011\n",
      "iteration 287200 training loss 0.59607375 lr 0.00011\n",
      "iteration 287300 training loss 0.64243525 lr 0.00011\n",
      "iteration 287400 training loss 0.6902572 lr 0.00011\n",
      "iteration 287500 training loss 0.72508013 lr 0.00011\n",
      "iteration 287600 training loss 0.8863544 lr 0.00011\n",
      "iteration 287700 training loss 0.6434863 lr 0.00011\n",
      "iteration 287800 training loss 0.46770328 lr 0.00011\n",
      "iteration 287900 training loss 0.61856633 lr 0.00011\n",
      "iteration 288000 training loss 0.60046655 lr 0.00011\n",
      "iteration 288100 training loss 0.50475544 lr 0.00011\n",
      "iteration 288200 training loss 0.5622275 lr 0.00011\n",
      "iteration 288300 training loss 0.62441444 lr 0.00011\n",
      "iteration 288400 training loss 0.56797045 lr 0.00011\n",
      "iteration 288500 training loss 0.7247456 lr 0.00011\n",
      "iteration 288600 training loss 0.698011 lr 0.00011\n",
      "iteration 288700 training loss 0.6047316 lr 0.00011\n",
      "iteration 288800 training loss 0.7323908 lr 0.00011\n",
      "iteration 288900 training loss 0.7172752 lr 0.00011\n",
      "iteration 289000 training loss 0.7013959 lr 0.00011\n",
      "iteration 289100 training loss 0.5231729 lr 0.00011\n",
      "iteration 289200 training loss 0.708489 lr 0.00011\n",
      "iteration 289300 training loss 0.744126 lr 0.00011\n",
      "iteration 289400 training loss 0.6369575 lr 0.00011\n",
      "iteration 289500 training loss 0.7239407 lr 0.00011\n",
      "iteration 289600 training loss 0.5612575 lr 0.00011\n",
      "iteration 289700 training loss 0.7737403 lr 0.00011\n",
      "iteration 289800 training loss 0.7878741 lr 0.00011\n",
      "iteration 289900 training loss 0.8411838 lr 0.00011\n",
      "iteration 290000 training loss 0.60201085 lr 0.00011\n",
      "iteration 290100 training loss 0.57921994 lr 0.00011\n",
      "iteration 290200 training loss 0.6658588 lr 0.00011\n",
      "iteration 290300 training loss 0.6216337 lr 0.00011\n",
      "iteration 290400 training loss 0.45035666 lr 0.00011\n",
      "iteration 290500 training loss 0.5270737 lr 0.00011\n",
      "iteration 290600 training loss 0.5227913 lr 0.00011\n",
      "iteration 290700 training loss 0.6549217 lr 0.00011\n",
      "iteration 290800 training loss 0.5398406 lr 0.00011\n",
      "iteration 290900 training loss 0.67708397 lr 0.00011\n",
      "iteration 291000 training loss 0.6586602 lr 0.00011\n",
      "iteration 291100 training loss 0.71759593 lr 0.00011\n",
      "iteration 291200 training loss 0.80235857 lr 0.00011\n",
      "iteration 291300 training loss 0.7048768 lr 0.00011\n",
      "iteration 291400 training loss 0.7677978 lr 0.00011\n",
      "iteration 291500 training loss 0.66014946 lr 0.00011\n",
      "iteration 291600 training loss 0.49357438 lr 0.00011\n",
      "iteration 291700 training loss 0.62837195 lr 0.00011\n",
      "iteration 291800 training loss 0.71088755 lr 0.00011\n",
      "iteration 291900 training loss 0.6258333 lr 0.00011\n",
      "iteration 292000 training loss 0.6913909 lr 0.00011\n",
      "iteration 292100 training loss 0.7334054 lr 0.00011\n",
      "iteration 292200 training loss 0.907816 lr 0.00011\n",
      "iteration 292300 training loss 0.80088395 lr 0.00011\n",
      "iteration 292400 training loss 0.70788956 lr 0.00011\n",
      "iteration 292500 training loss 0.5317145 lr 0.00011\n",
      "iteration 292600 training loss 0.7474605 lr 0.00011\n",
      "iteration 292700 training loss 0.724968 lr 0.00011\n",
      "iteration 292800 training loss 0.7193636 lr 0.00011\n",
      "iteration 292900 training loss 0.62492627 lr 0.00011\n",
      "iteration 293000 training loss 0.70194453 lr 0.00011\n",
      "iteration 293100 training loss 0.68942046 lr 0.00011\n",
      "iteration 293200 training loss 0.58640116 lr 0.00011\n",
      "iteration 293300 training loss 0.7588365 lr 0.00011\n",
      "iteration 293400 training loss 0.55342513 lr 0.00011\n",
      "iteration 293500 training loss 0.68779147 lr 0.00011\n",
      "iteration 293600 training loss 0.7217915 lr 0.00011\n",
      "iteration 293700 training loss 0.6002512 lr 0.00011\n",
      "iteration 293800 training loss 0.6790605 lr 0.00011\n",
      "iteration 293900 training loss 0.6551758 lr 0.00011\n",
      "iteration 294000 training loss 0.8822219 lr 0.00011\n",
      "iteration 294100 training loss 0.70821476 lr 0.00011\n",
      "iteration 294200 training loss 0.86915725 lr 0.00011\n",
      "iteration 294300 training loss 0.4581511 lr 0.00011\n",
      "iteration 294400 training loss 0.60180515 lr 0.00011\n",
      "iteration 294500 training loss 0.81467277 lr 0.00011\n",
      "iteration 294600 training loss 0.6534243 lr 0.00011\n",
      "iteration 294700 training loss 0.7972218 lr 0.00011\n",
      "iteration 294800 training loss 0.42166087 lr 0.00011\n",
      "iteration 294900 training loss 0.99127537 lr 0.00011\n",
      "iteration 295000 training loss 0.59794164 lr 0.00011\n",
      "iteration 295100 training loss 0.7045071 lr 0.00011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 295200 training loss 0.6188822 lr 0.00011\n",
      "iteration 295300 training loss 0.76640624 lr 0.00011\n",
      "iteration 295400 training loss 0.6201201 lr 0.00011\n",
      "iteration 295500 training loss 0.7506011 lr 0.00011\n",
      "iteration 295600 training loss 0.71673757 lr 0.00011\n",
      "iteration 295700 training loss 0.6974342 lr 0.00011\n",
      "iteration 295800 training loss 0.7669331 lr 0.00011\n",
      "iteration 295900 training loss 0.53327805 lr 0.00011\n",
      "iteration 296000 training loss 0.64025164 lr 0.00011\n",
      "iteration 296100 training loss 0.822104 lr 0.00011\n",
      "iteration 296200 training loss 0.56817317 lr 0.00011\n",
      "iteration 296300 training loss 0.71893215 lr 0.00011\n",
      "iteration 296400 training loss 0.6651301 lr 0.00011\n",
      "iteration 296500 training loss 0.69706726 lr 0.00011\n",
      "iteration 296600 training loss 0.68725836 lr 0.00011\n",
      "iteration 296700 training loss 0.78739196 lr 0.00011\n",
      "iteration 296800 training loss 0.81816226 lr 0.00011\n",
      "iteration 296900 training loss 0.6837632 lr 0.00011\n",
      "iteration 297000 training loss 0.533373 lr 0.00011\n",
      "iteration 297100 training loss 0.47344032 lr 0.00011\n",
      "iteration 297200 training loss 0.79585826 lr 0.00011\n",
      "iteration 297300 training loss 0.68307936 lr 0.00011\n",
      "iteration 297400 training loss 0.6984683 lr 0.00011\n",
      "iteration 297500 training loss 0.6204391 lr 0.00011\n",
      "iteration 297600 training loss 0.6210544 lr 0.00011\n",
      "iteration 297700 training loss 0.6665844 lr 0.00011\n",
      "iteration 297800 training loss 0.65807796 lr 0.00011\n",
      "iteration 297900 training loss 0.6436482 lr 0.00011\n",
      "iteration 298000 training loss 0.7098939 lr 0.00011\n",
      "iteration 298100 training loss 0.68419135 lr 0.00011\n",
      "iteration 298200 training loss 0.61465436 lr 0.00011\n",
      "iteration 298300 training loss 0.8300348 lr 0.00011\n",
      "iteration 298400 training loss 0.65349746 lr 0.00011\n",
      "iteration 298500 training loss 0.8011024 lr 0.00011\n",
      "iteration 298600 training loss 0.6487504 lr 0.00011\n",
      "iteration 298700 training loss 0.84594345 lr 0.00011\n",
      "iteration 298800 training loss 0.6850783 lr 0.00011\n",
      "iteration 298900 training loss 0.6360872 lr 0.00011\n",
      "iteration 299000 training loss 0.8065359 lr 0.00011\n",
      "iteration 299100 training loss 0.8115075 lr 0.00011\n",
      "iteration 299200 training loss 0.80119807 lr 0.00011\n",
      "iteration 299300 training loss 0.42161697 lr 0.00011\n",
      "iteration 299400 training loss 0.6914371 lr 0.00011\n",
      "iteration 299500 training loss 0.57964885 lr 0.00011\n",
      "iteration 299600 training loss 0.6075728 lr 0.00011\n",
      "iteration 299700 training loss 0.6805318 lr 0.00011\n",
      "iteration 299800 training loss 0.673171 lr 0.00011\n",
      "iteration 299900 training loss 0.47522658 lr 0.00011\n",
      "iteration 300000 training loss 0.5619314 lr 0.00011\n",
      "layout:nlp:random 0.8220999551469881\n",
      "layout:nlp:default 0.4218672135692321\n",
      "layout:xla:random 0.5263769003072106\n",
      "layout:xla:default 0.23578340867675712\n",
      "epoch 0, it 300000 validation loss -0.502\n",
      "iteration 300100 training loss 0.67031735 lr 0.00010\n",
      "iteration 300200 training loss 0.737181 lr 0.00010\n",
      "iteration 300300 training loss 0.79680365 lr 0.00010\n",
      "iteration 300400 training loss 0.63802296 lr 0.00010\n",
      "iteration 300500 training loss 0.8219864 lr 0.00010\n",
      "iteration 300600 training loss 0.6049958 lr 0.00010\n",
      "iteration 300700 training loss 0.7211161 lr 0.00010\n",
      "iteration 300800 training loss 0.6625426 lr 0.00010\n",
      "iteration 300900 training loss 0.5137353 lr 0.00010\n",
      "iteration 301000 training loss 0.65548396 lr 0.00010\n",
      "iteration 301100 training loss 0.6492591 lr 0.00010\n",
      "iteration 301200 training loss 0.44574112 lr 0.00010\n",
      "iteration 301300 training loss 0.6788652 lr 0.00010\n",
      "iteration 301400 training loss 0.7546929 lr 0.00010\n",
      "iteration 301500 training loss 0.47726417 lr 0.00010\n",
      "iteration 301600 training loss 0.6685105 lr 0.00010\n",
      "iteration 301700 training loss 0.6398204 lr 0.00010\n",
      "iteration 301800 training loss 0.6450214 lr 0.00010\n",
      "iteration 301900 training loss 0.73848623 lr 0.00010\n",
      "iteration 302000 training loss 0.65650916 lr 0.00010\n",
      "iteration 302100 training loss 0.6121324 lr 0.00010\n",
      "iteration 302200 training loss 0.8152711 lr 0.00010\n",
      "iteration 302300 training loss 0.64302933 lr 0.00010\n",
      "iteration 302400 training loss 0.5397952 lr 0.00010\n",
      "iteration 302500 training loss 0.73719484 lr 0.00010\n",
      "iteration 302600 training loss 0.48863423 lr 0.00010\n",
      "iteration 302700 training loss 0.6695313 lr 0.00010\n",
      "iteration 302800 training loss 0.47980666 lr 0.00010\n",
      "iteration 302900 training loss 0.55243874 lr 0.00010\n",
      "iteration 303000 training loss 0.5755278 lr 0.00010\n",
      "iteration 303100 training loss 0.503892 lr 0.00010\n",
      "iteration 303200 training loss 0.51315606 lr 0.00010\n",
      "iteration 303300 training loss 0.54167265 lr 0.00010\n",
      "iteration 303400 training loss 0.514006 lr 0.00010\n",
      "iteration 303500 training loss 0.57795507 lr 0.00010\n",
      "iteration 303600 training loss 0.5445951 lr 0.00010\n",
      "iteration 303700 training loss 0.6361232 lr 0.00010\n",
      "iteration 303800 training loss 0.8458077 lr 0.00010\n",
      "iteration 303900 training loss 0.68332607 lr 0.00010\n",
      "iteration 304000 training loss 0.7252501 lr 0.00010\n",
      "iteration 304100 training loss 0.75979686 lr 0.00010\n",
      "iteration 304200 training loss 0.46734855 lr 0.00010\n",
      "iteration 304300 training loss 0.82501817 lr 0.00010\n",
      "iteration 304400 training loss 0.6263721 lr 0.00010\n",
      "iteration 304500 training loss 0.43691385 lr 0.00010\n",
      "iteration 304600 training loss 0.7916032 lr 0.00010\n",
      "iteration 304700 training loss 0.7847789 lr 0.00010\n",
      "iteration 304800 training loss 0.7120433 lr 0.00010\n",
      "iteration 304900 training loss 0.5621368 lr 0.00010\n",
      "iteration 305000 training loss 0.5948336 lr 0.00010\n",
      "iteration 305100 training loss 0.6624365 lr 0.00010\n",
      "iteration 305200 training loss 0.82053155 lr 0.00010\n",
      "iteration 305300 training loss 0.69256103 lr 0.00010\n",
      "iteration 305400 training loss 0.58956647 lr 0.00010\n",
      "iteration 305500 training loss 0.5877756 lr 0.00010\n",
      "iteration 305600 training loss 0.8761635 lr 0.00010\n",
      "iteration 305700 training loss 0.74311244 lr 0.00010\n",
      "iteration 305800 training loss 0.6476292 lr 0.00010\n",
      "iteration 305900 training loss 0.6675522 lr 0.00010\n",
      "iteration 306000 training loss 0.7287157 lr 0.00010\n",
      "iteration 306100 training loss 0.6479707 lr 0.00010\n",
      "iteration 306200 training loss 0.7460669 lr 0.00010\n",
      "iteration 306300 training loss 0.73248196 lr 0.00010\n",
      "iteration 306400 training loss 0.77376366 lr 0.00010\n",
      "iteration 306500 training loss 0.7519203 lr 0.00010\n",
      "iteration 306600 training loss 0.7859289 lr 0.00010\n",
      "iteration 306700 training loss 0.7177229 lr 0.00010\n",
      "iteration 306800 training loss 0.75780165 lr 0.00010\n",
      "iteration 306900 training loss 0.8135863 lr 0.00010\n",
      "iteration 307000 training loss 0.69273895 lr 0.00010\n",
      "iteration 307100 training loss 0.59595484 lr 0.00010\n",
      "iteration 307200 training loss 0.75676316 lr 0.00010\n",
      "iteration 307300 training loss 0.70553654 lr 0.00010\n",
      "iteration 307400 training loss 0.73168576 lr 0.00010\n",
      "iteration 307500 training loss 0.79268235 lr 0.00010\n",
      "iteration 307600 training loss 0.73965436 lr 0.00010\n",
      "iteration 307700 training loss 0.70266044 lr 0.00010\n",
      "iteration 307800 training loss 0.7814548 lr 0.00010\n",
      "iteration 307900 training loss 0.6695719 lr 0.00010\n",
      "iteration 308000 training loss 0.6400656 lr 0.00010\n",
      "iteration 308100 training loss 0.76231176 lr 0.00010\n",
      "iteration 308200 training loss 0.70351785 lr 0.00010\n",
      "iteration 308300 training loss 0.8459709 lr 0.00010\n",
      "iteration 308400 training loss 0.7289395 lr 0.00010\n",
      "iteration 308500 training loss 0.7604482 lr 0.00010\n",
      "iteration 308600 training loss 0.72678053 lr 0.00010\n",
      "iteration 308700 training loss 0.801424 lr 0.00010\n",
      "iteration 308800 training loss 0.573434 lr 0.00010\n",
      "iteration 308900 training loss 0.7687023 lr 0.00010\n",
      "iteration 309000 training loss 0.8378434 lr 0.00010\n",
      "iteration 309100 training loss 0.64108396 lr 0.00010\n",
      "iteration 309200 training loss 0.9061311 lr 0.00010\n",
      "iteration 309300 training loss 0.64660984 lr 0.00010\n",
      "iteration 309400 training loss 0.64525026 lr 0.00010\n",
      "iteration 309500 training loss 0.7701415 lr 0.00010\n",
      "iteration 309600 training loss 0.49202406 lr 0.00010\n",
      "iteration 309700 training loss 0.7415841 lr 0.00010\n",
      "iteration 309800 training loss 0.6989827 lr 0.00010\n",
      "iteration 309900 training loss 0.7948948 lr 0.00010\n",
      "iteration 310000 training loss 0.6325548 lr 0.00010\n",
      "iteration 310100 training loss 0.6565765 lr 0.00010\n",
      "iteration 310200 training loss 0.6075072 lr 0.00010\n",
      "iteration 310300 training loss 0.72265744 lr 0.00010\n",
      "iteration 310400 training loss 0.92235726 lr 0.00010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 310500 training loss 0.72628784 lr 0.00010\n",
      "iteration 310600 training loss 0.92440534 lr 0.00010\n",
      "iteration 310700 training loss 0.65303946 lr 0.00010\n",
      "iteration 310800 training loss 0.86788857 lr 0.00010\n",
      "iteration 310900 training loss 0.556035 lr 0.00010\n",
      "iteration 311000 training loss 0.8505624 lr 0.00010\n",
      "iteration 311100 training loss 0.7228555 lr 0.00010\n",
      "iteration 311200 training loss 0.7127622 lr 0.00010\n",
      "iteration 311300 training loss 0.7572656 lr 0.00010\n",
      "iteration 311400 training loss 0.56086457 lr 0.00010\n",
      "iteration 311500 training loss 0.77393883 lr 0.00010\n",
      "iteration 311600 training loss 0.7783854 lr 0.00010\n",
      "iteration 311700 training loss 0.63575596 lr 0.00010\n",
      "iteration 311800 training loss 0.65326524 lr 0.00010\n",
      "iteration 311900 training loss 0.65909535 lr 0.00010\n",
      "iteration 312000 training loss 0.4890662 lr 0.00010\n",
      "iteration 312100 training loss 0.5283325 lr 0.00010\n",
      "iteration 312200 training loss 0.5238804 lr 0.00010\n",
      "iteration 312300 training loss 0.8664542 lr 0.00010\n",
      "iteration 312400 training loss 0.60331887 lr 0.00010\n",
      "iteration 312500 training loss 0.67817205 lr 0.00010\n",
      "iteration 312600 training loss 0.49130192 lr 0.00010\n",
      "iteration 312700 training loss 0.6672937 lr 0.00010\n",
      "iteration 312800 training loss 0.5452394 lr 0.00010\n",
      "iteration 312900 training loss 0.57662666 lr 0.00010\n",
      "iteration 313000 training loss 0.7419633 lr 0.00010\n",
      "iteration 313100 training loss 0.67561597 lr 0.00010\n",
      "iteration 313200 training loss 0.8121778 lr 0.00010\n",
      "iteration 313300 training loss 0.7794794 lr 0.00010\n",
      "iteration 313400 training loss 0.7691778 lr 0.00010\n",
      "iteration 313500 training loss 0.7943311 lr 0.00010\n",
      "iteration 313600 training loss 0.78147167 lr 0.00010\n",
      "iteration 313700 training loss 0.59011364 lr 0.00010\n",
      "iteration 313800 training loss 0.6120037 lr 0.00010\n",
      "iteration 313900 training loss 0.61357206 lr 0.00010\n",
      "iteration 314000 training loss 0.6660657 lr 0.00010\n",
      "iteration 314100 training loss 0.79943603 lr 0.00010\n",
      "iteration 314200 training loss 0.5944733 lr 0.00010\n",
      "iteration 314300 training loss 0.85418767 lr 0.00010\n",
      "iteration 314400 training loss 0.52499306 lr 0.00010\n",
      "iteration 314500 training loss 0.62069243 lr 0.00010\n",
      "iteration 314600 training loss 0.5454146 lr 0.00010\n",
      "iteration 314700 training loss 0.49302068 lr 0.00010\n",
      "iteration 314800 training loss 0.58612114 lr 0.00010\n",
      "iteration 314900 training loss 0.5779131 lr 0.00010\n",
      "iteration 315000 training loss 1.0047354 lr 0.00010\n",
      "iteration 315100 training loss 0.6377387 lr 0.00010\n",
      "iteration 315200 training loss 0.64878637 lr 0.00010\n",
      "iteration 315300 training loss 0.5184989 lr 0.00010\n",
      "iteration 315400 training loss 0.6069325 lr 0.00010\n",
      "iteration 315500 training loss 0.65594417 lr 0.00010\n",
      "iteration 315600 training loss 0.7465029 lr 0.00010\n",
      "iteration 315700 training loss 0.6252095 lr 0.00010\n",
      "iteration 315800 training loss 0.72281635 lr 0.00010\n",
      "iteration 315900 training loss 0.6603589 lr 0.00010\n",
      "iteration 316000 training loss 0.770781 lr 0.00010\n",
      "iteration 316100 training loss 0.6776538 lr 0.00010\n",
      "iteration 316200 training loss 0.66478586 lr 0.00010\n",
      "iteration 316300 training loss 0.7141152 lr 0.00010\n",
      "iteration 316400 training loss 0.56081027 lr 0.00010\n",
      "iteration 316500 training loss 0.6043188 lr 0.00010\n",
      "iteration 316600 training loss 0.6455197 lr 0.00010\n",
      "iteration 316700 training loss 0.80956316 lr 0.00010\n",
      "iteration 316800 training loss 0.66063017 lr 0.00010\n",
      "iteration 316900 training loss 0.75845957 lr 0.00010\n",
      "iteration 317000 training loss 0.57515186 lr 0.00010\n",
      "iteration 317100 training loss 0.6156271 lr 0.00010\n",
      "iteration 317200 training loss 0.6426607 lr 0.00010\n",
      "iteration 317300 training loss 0.6644012 lr 0.00010\n",
      "iteration 317400 training loss 0.51748264 lr 0.00010\n",
      "iteration 317500 training loss 0.692751 lr 0.00010\n",
      "iteration 317600 training loss 0.63535607 lr 0.00010\n",
      "iteration 317700 training loss 0.6208167 lr 0.00010\n",
      "iteration 317800 training loss 0.64995253 lr 0.00010\n",
      "iteration 317900 training loss 0.6115193 lr 0.00010\n",
      "iteration 318000 training loss 0.587735 lr 0.00010\n",
      "iteration 318100 training loss 0.9282967 lr 0.00010\n",
      "iteration 318200 training loss 0.7184876 lr 0.00010\n",
      "iteration 318300 training loss 0.68162984 lr 0.00010\n",
      "iteration 318400 training loss 0.64200425 lr 0.00010\n",
      "iteration 318500 training loss 0.7787107 lr 0.00010\n",
      "iteration 318600 training loss 0.69277847 lr 0.00010\n",
      "iteration 318700 training loss 0.5765469 lr 0.00010\n",
      "iteration 318800 training loss 0.825325 lr 0.00010\n",
      "iteration 318900 training loss 0.66762507 lr 0.00010\n",
      "iteration 319000 training loss 0.640772 lr 0.00010\n",
      "iteration 319100 training loss 0.6834107 lr 0.00010\n",
      "iteration 319200 training loss 0.5915382 lr 0.00010\n",
      "iteration 319300 training loss 0.70508116 lr 0.00010\n",
      "iteration 319400 training loss 0.64504397 lr 0.00010\n",
      "iteration 319500 training loss 0.801187 lr 0.00010\n",
      "iteration 319600 training loss 0.63199884 lr 0.00010\n",
      "iteration 319700 training loss 0.7778279 lr 0.00010\n",
      "iteration 319800 training loss 0.61021036 lr 0.00010\n",
      "iteration 319900 training loss 0.7649688 lr 0.00010\n",
      "iteration 320000 training loss 0.68217164 lr 0.00010\n",
      "layout:nlp:random 0.8242258101971205\n",
      "layout:nlp:default 0.43843682056945166\n",
      "layout:xla:random 0.5529744175742267\n",
      "layout:xla:default 0.24270537521413602\n",
      "epoch 0, it 320000 validation loss -0.515\n",
      "iteration 320100 training loss 0.7203276 lr 0.00009\n",
      "iteration 320200 training loss 0.67587745 lr 0.00009\n",
      "iteration 320300 training loss 0.70398766 lr 0.00009\n",
      "iteration 320400 training loss 0.6673516 lr 0.00009\n",
      "iteration 320500 training loss 0.6797801 lr 0.00009\n",
      "iteration 320600 training loss 0.7461895 lr 0.00009\n",
      "iteration 320700 training loss 0.6265984 lr 0.00009\n",
      "iteration 320800 training loss 0.5893227 lr 0.00009\n",
      "iteration 320900 training loss 0.6599723 lr 0.00009\n",
      "iteration 321000 training loss 0.687222 lr 0.00009\n",
      "iteration 321100 training loss 0.60275584 lr 0.00009\n",
      "iteration 321200 training loss 0.6800933 lr 0.00009\n",
      "iteration 321300 training loss 0.7028091 lr 0.00009\n",
      "iteration 321400 training loss 0.7298199 lr 0.00009\n",
      "iteration 321500 training loss 0.538747 lr 0.00009\n",
      "iteration 321600 training loss 0.62610483 lr 0.00009\n",
      "iteration 321700 training loss 0.7779523 lr 0.00009\n",
      "iteration 321800 training loss 0.6557041 lr 0.00009\n",
      "iteration 321900 training loss 0.6158463 lr 0.00009\n",
      "iteration 322000 training loss 0.675963 lr 0.00009\n",
      "iteration 322100 training loss 0.65947133 lr 0.00009\n",
      "iteration 322200 training loss 0.70274377 lr 0.00009\n",
      "iteration 322300 training loss 0.62358576 lr 0.00009\n",
      "iteration 322400 training loss 0.6156048 lr 0.00009\n",
      "iteration 322500 training loss 0.8231075 lr 0.00009\n",
      "iteration 322600 training loss 0.7519306 lr 0.00009\n",
      "iteration 322700 training loss 0.8039848 lr 0.00009\n",
      "iteration 322800 training loss 0.807928 lr 0.00009\n",
      "iteration 322900 training loss 0.5771462 lr 0.00009\n",
      "iteration 323000 training loss 0.82097596 lr 0.00009\n",
      "iteration 323100 training loss 0.71989715 lr 0.00009\n",
      "iteration 323200 training loss 0.7466949 lr 0.00009\n",
      "iteration 323300 training loss 0.67293924 lr 0.00009\n",
      "iteration 323400 training loss 0.6054829 lr 0.00009\n",
      "iteration 323500 training loss 0.5026422 lr 0.00009\n",
      "iteration 323600 training loss 0.64142805 lr 0.00009\n",
      "iteration 323700 training loss 0.5970186 lr 0.00009\n",
      "iteration 323800 training loss 0.4673193 lr 0.00009\n",
      "iteration 323900 training loss 0.7957686 lr 0.00009\n",
      "iteration 324000 training loss 0.6896245 lr 0.00009\n",
      "iteration 324100 training loss 0.7512422 lr 0.00009\n",
      "iteration 324200 training loss 0.3656123 lr 0.00009\n",
      "iteration 324300 training loss 0.46183 lr 0.00009\n",
      "iteration 324400 training loss 0.70970136 lr 0.00009\n",
      "iteration 324500 training loss 0.63565147 lr 0.00009\n",
      "iteration 324600 training loss 0.6861183 lr 0.00009\n",
      "iteration 324700 training loss 0.5920326 lr 0.00009\n",
      "iteration 324800 training loss 0.6511674 lr 0.00009\n",
      "iteration 324900 training loss 0.6277947 lr 0.00009\n",
      "iteration 325000 training loss 0.5423671 lr 0.00009\n",
      "iteration 325100 training loss 0.6667168 lr 0.00009\n",
      "iteration 325200 training loss 0.64989114 lr 0.00009\n",
      "iteration 325300 training loss 0.6409294 lr 0.00009\n",
      "iteration 325400 training loss 0.5529891 lr 0.00009\n",
      "iteration 325500 training loss 0.72416073 lr 0.00009\n",
      "iteration 325600 training loss 1.0149533 lr 0.00009\n",
      "iteration 325700 training loss 0.8467422 lr 0.00009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 325800 training loss 0.66830325 lr 0.00009\n",
      "iteration 325900 training loss 0.6613913 lr 0.00009\n",
      "iteration 326000 training loss 0.531268 lr 0.00009\n",
      "iteration 326100 training loss 0.5630409 lr 0.00009\n",
      "iteration 326200 training loss 0.6007816 lr 0.00009\n",
      "iteration 326300 training loss 0.61857027 lr 0.00009\n",
      "iteration 326400 training loss 0.6714182 lr 0.00009\n",
      "iteration 326500 training loss 0.67512935 lr 0.00009\n",
      "iteration 326600 training loss 0.7059986 lr 0.00009\n",
      "iteration 326700 training loss 0.50918096 lr 0.00009\n",
      "iteration 326800 training loss 0.57830316 lr 0.00009\n",
      "iteration 326900 training loss 0.6964322 lr 0.00009\n",
      "iteration 327000 training loss 0.5622396 lr 0.00009\n",
      "iteration 327100 training loss 0.6629274 lr 0.00009\n",
      "iteration 327200 training loss 0.59006727 lr 0.00009\n",
      "iteration 327300 training loss 0.79720664 lr 0.00009\n",
      "iteration 327400 training loss 0.74241394 lr 0.00009\n",
      "iteration 327500 training loss 0.705287 lr 0.00009\n",
      "iteration 327600 training loss 0.60478735 lr 0.00009\n",
      "iteration 327700 training loss 0.6269736 lr 0.00009\n",
      "iteration 327800 training loss 0.492915 lr 0.00009\n",
      "iteration 327900 training loss 0.6587772 lr 0.00009\n",
      "iteration 328000 training loss 0.5624635 lr 0.00009\n",
      "iteration 328100 training loss 0.6173064 lr 0.00009\n",
      "iteration 328200 training loss 0.83146983 lr 0.00009\n",
      "iteration 328300 training loss 0.6473903 lr 0.00009\n",
      "iteration 328400 training loss 0.66341585 lr 0.00009\n",
      "iteration 328500 training loss 0.58754706 lr 0.00009\n",
      "iteration 328600 training loss 0.5656511 lr 0.00009\n",
      "iteration 328700 training loss 0.6888394 lr 0.00009\n",
      "iteration 328800 training loss 0.6686674 lr 0.00009\n",
      "iteration 328900 training loss 0.58453864 lr 0.00009\n",
      "iteration 329000 training loss 0.5141216 lr 0.00009\n",
      "iteration 329100 training loss 0.5388778 lr 0.00009\n",
      "iteration 329200 training loss 0.77741694 lr 0.00009\n",
      "iteration 329300 training loss 0.74218655 lr 0.00009\n",
      "iteration 329400 training loss 0.68884534 lr 0.00009\n",
      "iteration 329500 training loss 0.6026717 lr 0.00009\n",
      "iteration 329600 training loss 0.72656024 lr 0.00009\n",
      "iteration 329700 training loss 0.695631 lr 0.00009\n",
      "iteration 329800 training loss 0.7709928 lr 0.00009\n",
      "iteration 329900 training loss 0.60554385 lr 0.00009\n",
      "iteration 330000 training loss 0.52706623 lr 0.00009\n",
      "iteration 330100 training loss 0.50601196 lr 0.00009\n",
      "iteration 330200 training loss 0.43135345 lr 0.00009\n",
      "iteration 330300 training loss 0.6188631 lr 0.00009\n",
      "iteration 330400 training loss 0.59222215 lr 0.00009\n",
      "iteration 330500 training loss 0.36689463 lr 0.00009\n",
      "iteration 330600 training loss 0.53484035 lr 0.00009\n",
      "iteration 330700 training loss 0.48088884 lr 0.00009\n",
      "iteration 330800 training loss 0.6806333 lr 0.00009\n",
      "iteration 330900 training loss 0.736138 lr 0.00009\n",
      "iteration 331000 training loss 0.57743365 lr 0.00009\n",
      "iteration 331100 training loss 0.62995815 lr 0.00009\n",
      "iteration 331200 training loss 0.75664264 lr 0.00009\n",
      "iteration 331300 training loss 0.511956 lr 0.00009\n",
      "iteration 331400 training loss 0.41502473 lr 0.00009\n",
      "iteration 331500 training loss 0.66960835 lr 0.00009\n",
      "iteration 331600 training loss 0.59761226 lr 0.00009\n",
      "iteration 331700 training loss 0.5714319 lr 0.00009\n",
      "iteration 331800 training loss 0.5288801 lr 0.00009\n",
      "iteration 331900 training loss 0.63256955 lr 0.00009\n",
      "iteration 332000 training loss 0.628406 lr 0.00009\n",
      "iteration 332100 training loss 0.47638822 lr 0.00009\n",
      "iteration 332200 training loss 0.571914 lr 0.00009\n",
      "iteration 332300 training loss 0.56719995 lr 0.00009\n",
      "iteration 332400 training loss 0.557782 lr 0.00009\n",
      "iteration 332500 training loss 0.6368235 lr 0.00009\n",
      "iteration 332600 training loss 0.79021734 lr 0.00009\n",
      "iteration 332700 training loss 0.6889032 lr 0.00009\n",
      "iteration 332800 training loss 0.7535902 lr 0.00009\n",
      "iteration 332900 training loss 0.7442737 lr 0.00009\n",
      "iteration 333000 training loss 0.6567717 lr 0.00009\n",
      "iteration 333100 training loss 0.7107526 lr 0.00009\n",
      "iteration 333200 training loss 0.6352751 lr 0.00009\n",
      "iteration 333300 training loss 0.6823415 lr 0.00009\n",
      "iteration 333400 training loss 0.6882312 lr 0.00009\n",
      "iteration 333500 training loss 0.7374876 lr 0.00009\n",
      "iteration 333600 training loss 0.6215712 lr 0.00009\n",
      "iteration 333700 training loss 0.5476357 lr 0.00009\n",
      "iteration 333800 training loss 0.71584624 lr 0.00009\n",
      "iteration 333900 training loss 0.5552361 lr 0.00009\n",
      "iteration 334000 training loss 0.6889444 lr 0.00009\n",
      "iteration 334100 training loss 0.63238925 lr 0.00009\n",
      "iteration 334200 training loss 0.7377193 lr 0.00009\n",
      "iteration 334300 training loss 0.8880211 lr 0.00009\n",
      "iteration 334400 training loss 0.7302456 lr 0.00009\n",
      "iteration 334500 training loss 0.55814695 lr 0.00009\n",
      "iteration 334600 training loss 0.7637011 lr 0.00009\n",
      "iteration 334700 training loss 0.89131975 lr 0.00009\n",
      "iteration 334800 training loss 0.7390818 lr 0.00009\n",
      "iteration 334900 training loss 0.630178 lr 0.00009\n",
      "iteration 335000 training loss 0.6255147 lr 0.00009\n",
      "iteration 335100 training loss 0.63553846 lr 0.00009\n",
      "iteration 335200 training loss 0.79932666 lr 0.00009\n",
      "iteration 335300 training loss 0.574493 lr 0.00009\n",
      "iteration 335400 training loss 0.5206348 lr 0.00009\n",
      "iteration 335500 training loss 0.7864588 lr 0.00009\n",
      "iteration 335600 training loss 0.7768803 lr 0.00009\n",
      "iteration 335700 training loss 0.591816 lr 0.00009\n",
      "iteration 335800 training loss 0.7866628 lr 0.00009\n",
      "iteration 335900 training loss 0.73270154 lr 0.00009\n",
      "iteration 336000 training loss 0.66250634 lr 0.00009\n",
      "iteration 336100 training loss 0.72436744 lr 0.00009\n",
      "iteration 336200 training loss 0.6178095 lr 0.00009\n",
      "iteration 336300 training loss 0.5198297 lr 0.00009\n",
      "iteration 336400 training loss 0.7627005 lr 0.00009\n",
      "iteration 336500 training loss 0.6678457 lr 0.00009\n",
      "iteration 336600 training loss 0.8300033 lr 0.00009\n",
      "iteration 336700 training loss 0.65486854 lr 0.00009\n",
      "iteration 336800 training loss 0.72002953 lr 0.00009\n",
      "iteration 336900 training loss 0.6959124 lr 0.00009\n",
      "iteration 337000 training loss 0.63289094 lr 0.00009\n",
      "iteration 337100 training loss 0.7265518 lr 0.00009\n",
      "iteration 337200 training loss 0.5875638 lr 0.00009\n",
      "iteration 337300 training loss 0.7651952 lr 0.00009\n",
      "iteration 337400 training loss 0.69615614 lr 0.00009\n",
      "iteration 337500 training loss 0.739288 lr 0.00009\n",
      "iteration 337600 training loss 0.68231845 lr 0.00009\n",
      "iteration 337700 training loss 0.7262365 lr 0.00009\n",
      "iteration 337800 training loss 0.7254692 lr 0.00009\n",
      "iteration 337900 training loss 0.7768193 lr 0.00009\n",
      "iteration 338000 training loss 0.7828857 lr 0.00009\n",
      "iteration 338100 training loss 0.72123486 lr 0.00009\n",
      "iteration 338200 training loss 0.6737831 lr 0.00009\n",
      "iteration 338300 training loss 0.7594129 lr 0.00009\n",
      "iteration 338400 training loss 0.74335676 lr 0.00009\n",
      "iteration 338500 training loss 0.5938643 lr 0.00009\n",
      "iteration 338600 training loss 0.6722321 lr 0.00009\n",
      "iteration 338700 training loss 0.6009277 lr 0.00009\n",
      "iteration 338800 training loss 0.6534738 lr 0.00009\n",
      "iteration 338900 training loss 0.48784187 lr 0.00009\n",
      "iteration 339000 training loss 0.7651176 lr 0.00009\n",
      "iteration 339100 training loss 0.60989296 lr 0.00009\n",
      "iteration 339200 training loss 0.8788538 lr 0.00009\n",
      "iteration 339300 training loss 0.6820588 lr 0.00009\n",
      "iteration 339400 training loss 0.74638736 lr 0.00009\n",
      "iteration 339500 training loss 0.7550688 lr 0.00009\n",
      "iteration 339600 training loss 0.77170104 lr 0.00009\n",
      "iteration 339700 training loss 0.80247754 lr 0.00009\n",
      "iteration 339800 training loss 0.81369025 lr 0.00009\n",
      "iteration 339900 training loss 0.5964796 lr 0.00009\n",
      "iteration 340000 training loss 0.6848438 lr 0.00009\n",
      "layout:nlp:random 0.8042000076947801\n",
      "layout:nlp:default 0.4252397872553611\n",
      "layout:xla:random 0.499027013681026\n",
      "layout:xla:default 0.2249110297939588\n",
      "epoch 0, it 340000 validation loss -0.488\n",
      "iteration 340100 training loss 0.6349608 lr 0.00008\n",
      "iteration 340200 training loss 0.802141 lr 0.00008\n",
      "iteration 340300 training loss 0.6831002 lr 0.00008\n",
      "iteration 340400 training loss 0.7662553 lr 0.00008\n",
      "iteration 340500 training loss 0.584542 lr 0.00008\n",
      "iteration 340600 training loss 0.67119974 lr 0.00008\n",
      "iteration 340700 training loss 0.82630885 lr 0.00008\n",
      "iteration 340800 training loss 0.6466954 lr 0.00008\n",
      "iteration 340900 training loss 0.6531035 lr 0.00008\n",
      "iteration 341000 training loss 0.7260731 lr 0.00008\n",
      "iteration 341100 training loss 0.6934417 lr 0.00008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 341200 training loss 0.775799 lr 0.00008\n",
      "iteration 341300 training loss 0.7341122 lr 0.00008\n",
      "iteration 341400 training loss 0.52507263 lr 0.00008\n",
      "iteration 341500 training loss 0.56790704 lr 0.00008\n",
      "iteration 341600 training loss 0.7733406 lr 0.00008\n",
      "iteration 341700 training loss 0.7669343 lr 0.00008\n",
      "iteration 341800 training loss 0.4686357 lr 0.00008\n",
      "iteration 341900 training loss 0.570433 lr 0.00008\n",
      "iteration 342000 training loss 0.554243 lr 0.00008\n",
      "iteration 342100 training loss 0.5851549 lr 0.00008\n",
      "iteration 342200 training loss 0.73391575 lr 0.00008\n",
      "iteration 342300 training loss 0.69641995 lr 0.00008\n",
      "iteration 342400 training loss 0.74262494 lr 0.00008\n",
      "iteration 342500 training loss 0.6600195 lr 0.00008\n",
      "iteration 342600 training loss 0.55692756 lr 0.00008\n",
      "iteration 342700 training loss 0.53428507 lr 0.00008\n",
      "iteration 342800 training loss 0.65685755 lr 0.00008\n",
      "iteration 342900 training loss 0.77048653 lr 0.00008\n",
      "iteration 343000 training loss 0.63357806 lr 0.00008\n",
      "iteration 343100 training loss 0.55056876 lr 0.00008\n",
      "iteration 343200 training loss 0.7396754 lr 0.00008\n",
      "iteration 343300 training loss 0.93771213 lr 0.00008\n",
      "iteration 343400 training loss 0.64725846 lr 0.00008\n",
      "iteration 343500 training loss 0.6037132 lr 0.00008\n",
      "iteration 343600 training loss 0.665983 lr 0.00008\n",
      "iteration 343700 training loss 0.6310883 lr 0.00008\n",
      "iteration 343800 training loss 0.6029587 lr 0.00008\n",
      "iteration 343900 training loss 0.6695694 lr 0.00008\n",
      "iteration 344000 training loss 0.743459 lr 0.00008\n",
      "iteration 344100 training loss 0.62757903 lr 0.00008\n",
      "iteration 344200 training loss 0.6036107 lr 0.00008\n",
      "iteration 344300 training loss 0.6761832 lr 0.00008\n",
      "iteration 344400 training loss 0.59126866 lr 0.00008\n",
      "iteration 344500 training loss 0.7793788 lr 0.00008\n",
      "iteration 344600 training loss 0.5965892 lr 0.00008\n",
      "iteration 344700 training loss 0.85347193 lr 0.00008\n",
      "iteration 344800 training loss 0.44377676 lr 0.00008\n",
      "iteration 344900 training loss 0.47548598 lr 0.00008\n",
      "iteration 345000 training loss 0.7773155 lr 0.00008\n",
      "iteration 345100 training loss 0.6762797 lr 0.00008\n",
      "iteration 345200 training loss 0.75698084 lr 0.00008\n",
      "iteration 345300 training loss 0.57119197 lr 0.00008\n",
      "iteration 345400 training loss 0.3968022 lr 0.00008\n",
      "iteration 345500 training loss 0.4853979 lr 0.00008\n",
      "iteration 345600 training loss 0.6888205 lr 0.00008\n",
      "iteration 345700 training loss 0.53447735 lr 0.00008\n",
      "iteration 345800 training loss 0.719784 lr 0.00008\n",
      "iteration 345900 training loss 0.6451186 lr 0.00008\n",
      "iteration 346000 training loss 0.47439346 lr 0.00008\n",
      "iteration 346100 training loss 0.72222143 lr 0.00008\n",
      "iteration 346200 training loss 0.64266956 lr 0.00008\n",
      "iteration 346300 training loss 0.66010845 lr 0.00008\n",
      "iteration 346400 training loss 0.5905716 lr 0.00008\n",
      "iteration 346500 training loss 0.54894555 lr 0.00008\n",
      "iteration 346600 training loss 0.7065827 lr 0.00008\n",
      "iteration 346700 training loss 0.4702735 lr 0.00008\n",
      "iteration 346800 training loss 0.4885142 lr 0.00008\n",
      "iteration 346900 training loss 0.5035976 lr 0.00008\n",
      "iteration 347000 training loss 0.73313963 lr 0.00008\n",
      "iteration 347100 training loss 0.56834453 lr 0.00008\n",
      "iteration 347200 training loss 0.6386274 lr 0.00008\n",
      "iteration 347300 training loss 0.71912324 lr 0.00008\n",
      "iteration 347400 training loss 0.67072016 lr 0.00008\n",
      "iteration 347500 training loss 0.57032716 lr 0.00008\n",
      "iteration 347600 training loss 0.6847753 lr 0.00008\n",
      "iteration 347700 training loss 0.6801073 lr 0.00008\n",
      "iteration 347800 training loss 0.8050395 lr 0.00008\n",
      "iteration 347900 training loss 0.6748466 lr 0.00008\n",
      "iteration 348000 training loss 0.6805833 lr 0.00008\n",
      "iteration 348100 training loss 0.66949624 lr 0.00008\n",
      "iteration 348200 training loss 0.7434675 lr 0.00008\n",
      "iteration 348300 training loss 0.80825686 lr 0.00008\n",
      "iteration 348400 training loss 0.636861 lr 0.00008\n",
      "iteration 348500 training loss 0.708274 lr 0.00008\n",
      "iteration 348600 training loss 0.69421506 lr 0.00008\n",
      "iteration 348700 training loss 0.80601424 lr 0.00008\n",
      "iteration 348800 training loss 0.73006725 lr 0.00008\n",
      "iteration 348900 training loss 0.7978283 lr 0.00008\n",
      "iteration 349000 training loss 0.7224239 lr 0.00008\n",
      "iteration 349100 training loss 0.55704075 lr 0.00008\n",
      "iteration 349200 training loss 0.6457602 lr 0.00008\n",
      "iteration 349300 training loss 0.6665073 lr 0.00008\n",
      "iteration 349400 training loss 0.5285115 lr 0.00008\n",
      "iteration 349500 training loss 0.7444678 lr 0.00008\n",
      "iteration 349600 training loss 0.64681435 lr 0.00008\n",
      "iteration 349700 training loss 0.66146815 lr 0.00008\n",
      "iteration 349800 training loss 0.74178696 lr 0.00008\n",
      "iteration 349900 training loss 0.6466609 lr 0.00008\n",
      "iteration 350000 training loss 0.63534886 lr 0.00008\n",
      "iteration 350100 training loss 0.7699065 lr 0.00008\n",
      "iteration 350200 training loss 0.8788639 lr 0.00008\n",
      "iteration 350300 training loss 0.8183417 lr 0.00008\n",
      "iteration 350400 training loss 0.5626824 lr 0.00008\n",
      "iteration 350500 training loss 0.63841224 lr 0.00008\n",
      "iteration 350600 training loss 0.6800645 lr 0.00008\n",
      "iteration 350700 training loss 0.53507274 lr 0.00008\n",
      "iteration 350800 training loss 0.6383947 lr 0.00008\n",
      "iteration 350900 training loss 0.63621265 lr 0.00008\n",
      "iteration 351000 training loss 0.6167343 lr 0.00008\n",
      "iteration 351100 training loss 0.7843292 lr 0.00008\n",
      "iteration 351200 training loss 0.7816224 lr 0.00008\n",
      "iteration 351300 training loss 0.6200279 lr 0.00008\n",
      "iteration 351400 training loss 0.6626696 lr 0.00008\n",
      "iteration 351500 training loss 0.7469278 lr 0.00008\n",
      "iteration 351600 training loss 0.8348001 lr 0.00008\n",
      "iteration 351700 training loss 0.66687053 lr 0.00008\n",
      "iteration 351800 training loss 0.6365209 lr 0.00008\n",
      "iteration 351900 training loss 0.83050424 lr 0.00008\n",
      "iteration 352000 training loss 0.8110499 lr 0.00008\n",
      "iteration 352100 training loss 0.785163 lr 0.00008\n",
      "iteration 352200 training loss 0.6600822 lr 0.00008\n",
      "iteration 352300 training loss 0.54166484 lr 0.00008\n",
      "iteration 352400 training loss 0.86181307 lr 0.00008\n",
      "iteration 352500 training loss 0.89571065 lr 0.00008\n",
      "iteration 352600 training loss 0.7394479 lr 0.00008\n",
      "iteration 352700 training loss 0.60913384 lr 0.00008\n",
      "iteration 352800 training loss 0.748885 lr 0.00008\n",
      "iteration 352900 training loss 0.79846466 lr 0.00008\n",
      "iteration 353000 training loss 0.773381 lr 0.00008\n",
      "iteration 353100 training loss 0.8048822 lr 0.00008\n",
      "iteration 353200 training loss 0.75246143 lr 0.00008\n",
      "iteration 353300 training loss 0.8328811 lr 0.00008\n",
      "iteration 353400 training loss 0.76821786 lr 0.00008\n",
      "iteration 353500 training loss 0.7475066 lr 0.00008\n",
      "iteration 353600 training loss 0.6351632 lr 0.00008\n",
      "iteration 353700 training loss 0.8388558 lr 0.00008\n",
      "iteration 353800 training loss 0.626812 lr 0.00008\n",
      "iteration 353900 training loss 0.64362544 lr 0.00008\n",
      "iteration 354000 training loss 0.58238536 lr 0.00008\n",
      "iteration 354100 training loss 0.70748436 lr 0.00008\n",
      "iteration 354200 training loss 0.58997035 lr 0.00008\n",
      "iteration 354300 training loss 0.63045394 lr 0.00008\n",
      "iteration 354400 training loss 0.549648 lr 0.00008\n",
      "iteration 354500 training loss 0.5254496 lr 0.00008\n",
      "iteration 354600 training loss 0.5503025 lr 0.00008\n",
      "iteration 354700 training loss 0.6431243 lr 0.00008\n",
      "iteration 354800 training loss 0.62868106 lr 0.00008\n",
      "iteration 354900 training loss 0.79016113 lr 0.00008\n",
      "iteration 355000 training loss 0.5601879 lr 0.00008\n",
      "iteration 355100 training loss 0.6887508 lr 0.00008\n",
      "iteration 355200 training loss 0.66199607 lr 0.00008\n",
      "iteration 355300 training loss 0.5147802 lr 0.00008\n",
      "iteration 355400 training loss 0.71889794 lr 0.00008\n",
      "iteration 355500 training loss 0.5425757 lr 0.00008\n",
      "iteration 355600 training loss 0.5810843 lr 0.00008\n",
      "iteration 355700 training loss 0.637013 lr 0.00008\n",
      "iteration 355800 training loss 0.8034478 lr 0.00008\n",
      "iteration 355900 training loss 0.7535076 lr 0.00008\n",
      "iteration 356000 training loss 0.7012634 lr 0.00008\n",
      "iteration 356100 training loss 0.5798491 lr 0.00008\n",
      "iteration 356200 training loss 0.52192414 lr 0.00008\n",
      "iteration 356300 training loss 0.60668457 lr 0.00008\n",
      "iteration 356400 training loss 0.6958308 lr 0.00008\n",
      "iteration 356500 training loss 0.65754884 lr 0.00008\n",
      "iteration 356600 training loss 0.80333436 lr 0.00008\n",
      "iteration 356700 training loss 0.61656135 lr 0.00008\n",
      "iteration 356800 training loss 0.4702345 lr 0.00008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 356900 training loss 0.78668624 lr 0.00008\n",
      "iteration 357000 training loss 0.6331637 lr 0.00008\n",
      "iteration 357100 training loss 0.6999377 lr 0.00008\n",
      "iteration 357200 training loss 0.65334207 lr 0.00008\n",
      "iteration 357300 training loss 0.701258 lr 0.00008\n",
      "iteration 357400 training loss 0.5232396 lr 0.00008\n",
      "iteration 357500 training loss 0.5619362 lr 0.00008\n",
      "iteration 357600 training loss 0.75030917 lr 0.00008\n",
      "iteration 357700 training loss 0.76857984 lr 0.00008\n",
      "iteration 357800 training loss 0.51280737 lr 0.00008\n",
      "iteration 357900 training loss 0.550922 lr 0.00008\n",
      "iteration 358000 training loss 0.8475083 lr 0.00008\n",
      "iteration 358100 training loss 0.77811706 lr 0.00008\n",
      "iteration 358200 training loss 0.7417475 lr 0.00008\n",
      "iteration 358300 training loss 0.81187177 lr 0.00008\n",
      "iteration 358400 training loss 0.5684629 lr 0.00008\n",
      "iteration 358500 training loss 0.5774438 lr 0.00008\n",
      "iteration 358600 training loss 0.72850436 lr 0.00008\n",
      "iteration 358700 training loss 0.45890617 lr 0.00008\n",
      "iteration 358800 training loss 0.70690066 lr 0.00008\n",
      "iteration 358900 training loss 0.6674454 lr 0.00008\n",
      "iteration 359000 training loss 0.705499 lr 0.00008\n",
      "iteration 359100 training loss 0.69437855 lr 0.00008\n",
      "iteration 359200 training loss 0.7775642 lr 0.00008\n",
      "iteration 359300 training loss 0.4855608 lr 0.00008\n",
      "iteration 359400 training loss 0.745434 lr 0.00008\n",
      "iteration 359500 training loss 0.62468827 lr 0.00008\n",
      "iteration 359600 training loss 0.74510694 lr 0.00008\n",
      "iteration 359700 training loss 0.73305154 lr 0.00008\n",
      "iteration 359800 training loss 0.7249337 lr 0.00008\n",
      "iteration 359900 training loss 0.66008574 lr 0.00008\n",
      "iteration 360000 training loss 0.71625966 lr 0.00008\n",
      "layout:nlp:random 0.8075626789070401\n",
      "layout:nlp:default 0.42080722854879216\n",
      "layout:xla:random 0.5249152887228767\n",
      "layout:xla:default 0.23804810154452224\n",
      "epoch 0, it 360000 validation loss -0.498\n",
      "iteration 360100 training loss 0.5417838 lr 0.00008\n",
      "iteration 360200 training loss 0.6733379 lr 0.00008\n",
      "iteration 360300 training loss 0.54909724 lr 0.00008\n",
      "iteration 360400 training loss 0.7474609 lr 0.00008\n",
      "iteration 360500 training loss 0.673445 lr 0.00008\n",
      "iteration 360600 training loss 0.62165266 lr 0.00008\n",
      "iteration 360700 training loss 0.5910368 lr 0.00008\n",
      "iteration 360800 training loss 0.5645785 lr 0.00008\n",
      "iteration 360900 training loss 0.71728253 lr 0.00008\n",
      "iteration 361000 training loss 0.6251911 lr 0.00008\n",
      "iteration 361100 training loss 0.4545199 lr 0.00008\n",
      "iteration 361200 training loss 0.51413435 lr 0.00008\n",
      "iteration 361300 training loss 0.56371194 lr 0.00008\n",
      "iteration 361400 training loss 0.6541619 lr 0.00008\n",
      "iteration 361500 training loss 0.5885085 lr 0.00008\n",
      "iteration 361600 training loss 0.72643995 lr 0.00008\n",
      "iteration 361700 training loss 0.61071855 lr 0.00008\n",
      "iteration 361800 training loss 0.79164904 lr 0.00008\n",
      "iteration 361900 training loss 0.74367476 lr 0.00008\n",
      "iteration 362000 training loss 0.7053818 lr 0.00008\n",
      "iteration 362100 training loss 0.5483346 lr 0.00008\n",
      "iteration 362200 training loss 0.7114271 lr 0.00008\n",
      "iteration 362300 training loss 0.5732489 lr 0.00008\n",
      "iteration 362400 training loss 0.8857653 lr 0.00008\n",
      "iteration 362500 training loss 0.8232159 lr 0.00008\n",
      "iteration 362600 training loss 0.7702299 lr 0.00008\n",
      "iteration 362700 training loss 0.6646305 lr 0.00008\n",
      "iteration 362800 training loss 0.7170309 lr 0.00008\n",
      "iteration 362900 training loss 0.6568512 lr 0.00008\n",
      "iteration 363000 training loss 0.817475 lr 0.00008\n",
      "iteration 363100 training loss 0.5985103 lr 0.00008\n",
      "iteration 363200 training loss 0.69956434 lr 0.00008\n",
      "iteration 363300 training loss 0.59810764 lr 0.00008\n",
      "iteration 363400 training loss 0.6588288 lr 0.00008\n",
      "iteration 363500 training loss 0.6016673 lr 0.00008\n",
      "iteration 363600 training loss 0.62172 lr 0.00008\n",
      "iteration 363700 training loss 0.5010809 lr 0.00008\n",
      "iteration 363800 training loss 0.587442 lr 0.00008\n",
      "iteration 363900 training loss 0.5967482 lr 0.00008\n",
      "iteration 364000 training loss 0.5505359 lr 0.00008\n",
      "iteration 364100 training loss 0.73388475 lr 0.00008\n",
      "iteration 364200 training loss 0.501809 lr 0.00008\n",
      "iteration 364300 training loss 0.81567514 lr 0.00008\n",
      "iteration 364400 training loss 0.6875777 lr 0.00008\n",
      "iteration 364500 training loss 0.6804495 lr 0.00008\n",
      "iteration 364600 training loss 0.7223722 lr 0.00008\n",
      "iteration 364700 training loss 0.8373274 lr 0.00008\n",
      "iteration 364800 training loss 0.60386854 lr 0.00008\n",
      "iteration 364900 training loss 0.7476774 lr 0.00008\n",
      "iteration 365000 training loss 0.6069534 lr 0.00008\n",
      "iteration 365100 training loss 0.69328976 lr 0.00008\n",
      "iteration 365200 training loss 0.6162609 lr 0.00008\n",
      "iteration 365300 training loss 0.66456676 lr 0.00008\n",
      "iteration 365400 training loss 0.91862607 lr 0.00008\n",
      "iteration 365500 training loss 0.5960641 lr 0.00008\n",
      "iteration 365600 training loss 0.59740317 lr 0.00008\n",
      "iteration 365700 training loss 0.7937335 lr 0.00008\n",
      "iteration 365800 training loss 0.7492105 lr 0.00008\n",
      "iteration 365900 training loss 0.57705337 lr 0.00008\n",
      "iteration 366000 training loss 0.7052395 lr 0.00008\n",
      "iteration 366100 training loss 0.71796834 lr 0.00008\n",
      "iteration 366200 training loss 0.65243036 lr 0.00008\n",
      "iteration 366300 training loss 0.5467861 lr 0.00008\n",
      "iteration 366400 training loss 0.7269495 lr 0.00008\n",
      "iteration 366500 training loss 0.77132523 lr 0.00008\n",
      "iteration 366600 training loss 0.71992904 lr 0.00008\n",
      "iteration 366700 training loss 0.9872251 lr 0.00008\n",
      "iteration 366800 training loss 0.882883 lr 0.00008\n",
      "iteration 366900 training loss 0.7533044 lr 0.00008\n",
      "iteration 367000 training loss 0.68201965 lr 0.00008\n",
      "iteration 367100 training loss 0.7057971 lr 0.00008\n",
      "iteration 367200 training loss 0.73854244 lr 0.00008\n",
      "iteration 367300 training loss 0.7353051 lr 0.00008\n",
      "iteration 367400 training loss 0.83613425 lr 0.00008\n",
      "iteration 367500 training loss 0.75432533 lr 0.00008\n",
      "iteration 367600 training loss 0.6775181 lr 0.00008\n",
      "iteration 367700 training loss 0.643919 lr 0.00008\n",
      "iteration 367800 training loss 0.75036734 lr 0.00008\n",
      "iteration 367900 training loss 0.6867603 lr 0.00008\n",
      "iteration 368000 training loss 0.88049066 lr 0.00008\n",
      "iteration 368100 training loss 0.64362395 lr 0.00008\n",
      "iteration 368200 training loss 0.60996485 lr 0.00008\n",
      "iteration 368300 training loss 0.5166862 lr 0.00008\n",
      "iteration 368400 training loss 0.5794615 lr 0.00008\n",
      "iteration 368500 training loss 0.7397167 lr 0.00008\n",
      "iteration 368600 training loss 0.7162517 lr 0.00008\n",
      "iteration 368700 training loss 0.80200565 lr 0.00008\n",
      "iteration 368800 training loss 0.7333357 lr 0.00008\n",
      "iteration 368900 training loss 0.56611454 lr 0.00008\n",
      "iteration 369000 training loss 0.6619362 lr 0.00008\n",
      "iteration 369100 training loss 0.5434731 lr 0.00008\n",
      "iteration 369200 training loss 0.6643707 lr 0.00008\n",
      "iteration 369300 training loss 0.6127876 lr 0.00008\n",
      "iteration 369400 training loss 0.62869185 lr 0.00008\n",
      "iteration 369500 training loss 0.71936005 lr 0.00008\n",
      "iteration 369600 training loss 0.65816987 lr 0.00008\n",
      "iteration 369700 training loss 0.66916364 lr 0.00008\n",
      "iteration 369800 training loss 0.65862435 lr 0.00008\n",
      "iteration 369900 training loss 0.54540974 lr 0.00008\n",
      "iteration 370000 training loss 0.57000905 lr 0.00008\n",
      "iteration 370100 training loss 0.37001443 lr 0.00008\n",
      "iteration 370200 training loss 0.6977671 lr 0.00008\n",
      "iteration 370300 training loss 0.6413568 lr 0.00008\n",
      "iteration 370400 training loss 0.66403836 lr 0.00008\n",
      "iteration 370500 training loss 0.8206313 lr 0.00008\n",
      "iteration 370600 training loss 0.6600396 lr 0.00008\n",
      "iteration 370700 training loss 0.6179269 lr 0.00008\n",
      "iteration 370800 training loss 0.78866446 lr 0.00008\n",
      "iteration 370900 training loss 0.6123686 lr 0.00008\n",
      "iteration 371000 training loss 0.5735696 lr 0.00008\n",
      "iteration 371100 training loss 0.774355 lr 0.00008\n",
      "iteration 371200 training loss 0.75109094 lr 0.00008\n",
      "iteration 371300 training loss 0.73764426 lr 0.00008\n",
      "iteration 371400 training loss 0.7062698 lr 0.00008\n",
      "iteration 371500 training loss 0.6878483 lr 0.00008\n",
      "iteration 371600 training loss 0.7213789 lr 0.00008\n",
      "iteration 371700 training loss 0.5271669 lr 0.00008\n",
      "iteration 371800 training loss 0.7497264 lr 0.00008\n",
      "iteration 371900 training loss 0.75314194 lr 0.00008\n",
      "iteration 372000 training loss 0.7624242 lr 0.00008\n",
      "iteration 372100 training loss 0.6357832 lr 0.00008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 372200 training loss 0.6943235 lr 0.00008\n",
      "iteration 372300 training loss 0.7347319 lr 0.00008\n",
      "iteration 372400 training loss 0.69314367 lr 0.00008\n",
      "iteration 372500 training loss 0.74308926 lr 0.00008\n",
      "iteration 372600 training loss 0.5920758 lr 0.00008\n",
      "iteration 372700 training loss 0.68528897 lr 0.00008\n",
      "iteration 372800 training loss 0.5252674 lr 0.00008\n",
      "iteration 372900 training loss 0.7159719 lr 0.00008\n",
      "iteration 373000 training loss 0.85126954 lr 0.00008\n",
      "iteration 373100 training loss 0.6297464 lr 0.00008\n",
      "iteration 373200 training loss 0.55196404 lr 0.00008\n",
      "iteration 373300 training loss 0.95741636 lr 0.00008\n",
      "iteration 373400 training loss 0.6535112 lr 0.00008\n",
      "iteration 373500 training loss 0.6571726 lr 0.00008\n",
      "iteration 373600 training loss 0.73644984 lr 0.00008\n",
      "iteration 373700 training loss 0.6739288 lr 0.00008\n",
      "iteration 373800 training loss 0.88716114 lr 0.00008\n",
      "iteration 373900 training loss 0.5617375 lr 0.00008\n",
      "iteration 374000 training loss 0.71503913 lr 0.00008\n",
      "iteration 374100 training loss 0.7509124 lr 0.00008\n",
      "iteration 374200 training loss 0.5858397 lr 0.00008\n",
      "iteration 374300 training loss 0.5484996 lr 0.00008\n",
      "iteration 374400 training loss 0.79759705 lr 0.00008\n",
      "iteration 374500 training loss 0.541115 lr 0.00008\n",
      "iteration 374600 training loss 0.81438744 lr 0.00008\n",
      "iteration 374700 training loss 0.6341012 lr 0.00008\n",
      "iteration 374800 training loss 0.6514505 lr 0.00008\n",
      "iteration 374900 training loss 0.63791996 lr 0.00008\n",
      "iteration 375000 training loss 0.52267337 lr 0.00008\n",
      "iteration 375100 training loss 0.6457899 lr 0.00008\n",
      "iteration 375200 training loss 0.6409699 lr 0.00008\n",
      "iteration 375300 training loss 0.6370214 lr 0.00008\n",
      "iteration 375400 training loss 0.69448715 lr 0.00008\n",
      "iteration 375500 training loss 0.4780261 lr 0.00008\n",
      "iteration 375600 training loss 0.8516562 lr 0.00008\n",
      "iteration 375700 training loss 0.46792248 lr 0.00008\n",
      "iteration 375800 training loss 0.5367021 lr 0.00008\n",
      "iteration 375900 training loss 0.599689 lr 0.00008\n",
      "iteration 376000 training loss 0.62877446 lr 0.00008\n",
      "iteration 376100 training loss 0.55187786 lr 0.00008\n",
      "iteration 376200 training loss 0.769783 lr 0.00008\n",
      "iteration 376300 training loss 0.6519265 lr 0.00008\n",
      "iteration 376400 training loss 0.65925044 lr 0.00008\n",
      "iteration 376500 training loss 0.57476646 lr 0.00008\n",
      "iteration 376600 training loss 0.76383394 lr 0.00008\n",
      "iteration 376700 training loss 0.62385327 lr 0.00008\n",
      "iteration 376800 training loss 0.7324808 lr 0.00008\n",
      "iteration 376900 training loss 0.6569656 lr 0.00008\n",
      "iteration 377000 training loss 0.7354259 lr 0.00008\n",
      "iteration 377100 training loss 0.69244844 lr 0.00008\n",
      "iteration 377200 training loss 0.8334586 lr 0.00008\n",
      "iteration 377300 training loss 0.65875524 lr 0.00008\n",
      "iteration 377400 training loss 0.54327637 lr 0.00008\n",
      "iteration 377500 training loss 0.80546457 lr 0.00008\n",
      "iteration 377600 training loss 0.6020117 lr 0.00008\n",
      "iteration 377700 training loss 0.5550125 lr 0.00008\n",
      "iteration 377800 training loss 0.58176064 lr 0.00008\n",
      "iteration 377900 training loss 0.8516951 lr 0.00008\n",
      "iteration 378000 training loss 0.7079381 lr 0.00008\n",
      "iteration 378100 training loss 0.66145873 lr 0.00008\n",
      "iteration 378200 training loss 0.55459136 lr 0.00008\n",
      "iteration 378300 training loss 0.75220376 lr 0.00008\n",
      "iteration 378400 training loss 0.65167546 lr 0.00008\n",
      "iteration 378500 training loss 0.67578447 lr 0.00008\n",
      "iteration 378600 training loss 0.8277686 lr 0.00008\n",
      "iteration 378700 training loss 0.58285433 lr 0.00008\n",
      "iteration 378800 training loss 0.6558157 lr 0.00008\n",
      "iteration 378900 training loss 0.821765 lr 0.00008\n",
      "iteration 379000 training loss 0.6999119 lr 0.00008\n",
      "iteration 379100 training loss 0.74097115 lr 0.00008\n",
      "iteration 379200 training loss 0.7723852 lr 0.00008\n",
      "iteration 379300 training loss 0.69456285 lr 0.00008\n",
      "iteration 379400 training loss 0.5866304 lr 0.00008\n",
      "iteration 379500 training loss 0.62724733 lr 0.00008\n",
      "iteration 379600 training loss 0.5387991 lr 0.00008\n",
      "iteration 379700 training loss 0.61331415 lr 0.00008\n",
      "iteration 379800 training loss 0.7008868 lr 0.00008\n",
      "iteration 379900 training loss 0.53287166 lr 0.00008\n",
      "iteration 380000 training loss 0.5900694 lr 0.00008\n",
      "layout:nlp:random 0.8245376536198219\n",
      "layout:nlp:default 0.42430808719780816\n",
      "layout:xla:random 0.5072583768680242\n",
      "layout:xla:default 0.23635034923767165\n",
      "epoch 0, it 380000 validation loss -0.498\n",
      "iteration 380100 training loss 0.7839175 lr 0.00007\n",
      "iteration 380200 training loss 0.57569665 lr 0.00007\n",
      "iteration 380300 training loss 0.6132613 lr 0.00007\n",
      "iteration 380400 training loss 0.48498946 lr 0.00007\n",
      "iteration 380500 training loss 0.8265474 lr 0.00007\n",
      "iteration 380600 training loss 0.5788287 lr 0.00007\n",
      "iteration 380700 training loss 0.6148279 lr 0.00007\n",
      "iteration 380800 training loss 0.68041795 lr 0.00007\n",
      "iteration 380900 training loss 0.6472564 lr 0.00007\n",
      "iteration 381000 training loss 0.6878581 lr 0.00007\n",
      "iteration 381100 training loss 0.7211278 lr 0.00007\n",
      "iteration 381200 training loss 0.540163 lr 0.00007\n",
      "iteration 381300 training loss 0.54911244 lr 0.00007\n",
      "iteration 381400 training loss 0.71589863 lr 0.00007\n",
      "iteration 381500 training loss 0.6257571 lr 0.00007\n",
      "iteration 381600 training loss 0.73123765 lr 0.00007\n",
      "iteration 381700 training loss 0.77238816 lr 0.00007\n",
      "iteration 381800 training loss 0.5802207 lr 0.00007\n",
      "iteration 381900 training loss 0.6715007 lr 0.00007\n",
      "iteration 382000 training loss 0.7409464 lr 0.00007\n",
      "iteration 382100 training loss 0.6418341 lr 0.00007\n",
      "iteration 382200 training loss 0.49769235 lr 0.00007\n",
      "iteration 382300 training loss 0.69037807 lr 0.00007\n",
      "iteration 382400 training loss 0.77049994 lr 0.00007\n",
      "iteration 382500 training loss 0.6799975 lr 0.00007\n",
      "iteration 382600 training loss 0.76316226 lr 0.00007\n",
      "iteration 382700 training loss 0.6878911 lr 0.00007\n",
      "iteration 382800 training loss 0.48001814 lr 0.00007\n",
      "iteration 382900 training loss 0.66860795 lr 0.00007\n",
      "iteration 383000 training loss 0.6614512 lr 0.00007\n",
      "iteration 383100 training loss 0.7345889 lr 0.00007\n",
      "iteration 383200 training loss 0.74395883 lr 0.00007\n",
      "iteration 383300 training loss 0.63816917 lr 0.00007\n",
      "iteration 383400 training loss 0.8235733 lr 0.00007\n",
      "iteration 383500 training loss 0.64374256 lr 0.00007\n",
      "iteration 383600 training loss 0.6924648 lr 0.00007\n",
      "iteration 383700 training loss 0.7466083 lr 0.00007\n",
      "iteration 383800 training loss 0.7235979 lr 0.00007\n",
      "iteration 383900 training loss 0.81215715 lr 0.00007\n",
      "iteration 384000 training loss 0.7259833 lr 0.00007\n",
      "iteration 384100 training loss 0.8772156 lr 0.00007\n",
      "iteration 384200 training loss 0.63286376 lr 0.00007\n",
      "iteration 384300 training loss 0.64648145 lr 0.00007\n",
      "iteration 384400 training loss 0.60171366 lr 0.00007\n",
      "iteration 384500 training loss 0.9286061 lr 0.00007\n",
      "iteration 384600 training loss 0.60528916 lr 0.00007\n",
      "iteration 384700 training loss 0.5714373 lr 0.00007\n",
      "iteration 384800 training loss 0.55044764 lr 0.00007\n",
      "iteration 384900 training loss 0.7267154 lr 0.00007\n",
      "iteration 385000 training loss 0.5983047 lr 0.00007\n",
      "iteration 385100 training loss 0.7594855 lr 0.00007\n",
      "iteration 385200 training loss 0.66687953 lr 0.00007\n",
      "iteration 385300 training loss 0.65390986 lr 0.00007\n",
      "iteration 385400 training loss 0.48521444 lr 0.00007\n",
      "iteration 385500 training loss 0.63765067 lr 0.00007\n",
      "iteration 385600 training loss 0.6614618 lr 0.00007\n",
      "iteration 385700 training loss 0.64156216 lr 0.00007\n",
      "iteration 385800 training loss 0.67755157 lr 0.00007\n",
      "iteration 385900 training loss 0.79242647 lr 0.00007\n",
      "iteration 386000 training loss 0.77496845 lr 0.00007\n",
      "iteration 386100 training loss 0.772424 lr 0.00007\n",
      "iteration 386200 training loss 0.8067882 lr 0.00007\n",
      "iteration 386300 training loss 0.6900854 lr 0.00007\n",
      "iteration 386400 training loss 0.7809127 lr 0.00007\n",
      "iteration 386500 training loss 0.6702057 lr 0.00007\n",
      "iteration 386600 training loss 0.6586939 lr 0.00007\n",
      "iteration 386700 training loss 0.7271081 lr 0.00007\n",
      "iteration 386800 training loss 0.7246611 lr 0.00007\n",
      "iteration 386900 training loss 0.59247077 lr 0.00007\n",
      "iteration 387000 training loss 0.7698863 lr 0.00007\n",
      "iteration 387100 training loss 0.63677746 lr 0.00007\n",
      "iteration 387200 training loss 0.7283715 lr 0.00007\n",
      "iteration 387300 training loss 0.6642048 lr 0.00007\n",
      "iteration 387400 training loss 0.68321 lr 0.00007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 387500 training loss 0.76502764 lr 0.00007\n",
      "iteration 387600 training loss 0.5356537 lr 0.00007\n",
      "iteration 387700 training loss 0.5735999 lr 0.00007\n",
      "iteration 387800 training loss 0.62601906 lr 0.00007\n",
      "iteration 387900 training loss 0.5496073 lr 0.00007\n",
      "iteration 388000 training loss 0.68018705 lr 0.00007\n",
      "iteration 388100 training loss 0.60321325 lr 0.00007\n",
      "iteration 388200 training loss 0.5913639 lr 0.00007\n",
      "iteration 388300 training loss 0.68477553 lr 0.00007\n",
      "iteration 388400 training loss 0.673127 lr 0.00007\n",
      "iteration 388500 training loss 0.6840205 lr 0.00007\n",
      "iteration 388600 training loss 0.5136042 lr 0.00007\n",
      "iteration 388700 training loss 0.707784 lr 0.00007\n",
      "iteration 388800 training loss 0.5509922 lr 0.00007\n",
      "iteration 388900 training loss 0.65855044 lr 0.00007\n",
      "iteration 389000 training loss 0.58757716 lr 0.00007\n",
      "iteration 389100 training loss 0.6304129 lr 0.00007\n",
      "iteration 389200 training loss 0.62583894 lr 0.00007\n",
      "iteration 389300 training loss 0.6222066 lr 0.00007\n",
      "iteration 389400 training loss 0.43897462 lr 0.00007\n",
      "iteration 389500 training loss 0.52321786 lr 0.00007\n",
      "iteration 389600 training loss 0.58218634 lr 0.00007\n",
      "iteration 389700 training loss 0.65559244 lr 0.00007\n",
      "iteration 389800 training loss 0.51105493 lr 0.00007\n",
      "iteration 389900 training loss 0.62824875 lr 0.00007\n",
      "iteration 390000 training loss 0.6534268 lr 0.00007\n",
      "iteration 390100 training loss 0.60695714 lr 0.00007\n",
      "iteration 390200 training loss 0.5788104 lr 0.00007\n",
      "iteration 390300 training loss 0.5679964 lr 0.00007\n",
      "iteration 390400 training loss 0.7827813 lr 0.00007\n",
      "iteration 390500 training loss 0.45222893 lr 0.00007\n",
      "iteration 390600 training loss 0.65123314 lr 0.00007\n",
      "iteration 390700 training loss 0.5724673 lr 0.00007\n",
      "iteration 390800 training loss 0.6903791 lr 0.00007\n",
      "iteration 390900 training loss 0.8068604 lr 0.00007\n",
      "iteration 391000 training loss 0.64348334 lr 0.00007\n",
      "iteration 391100 training loss 0.7782222 lr 0.00007\n",
      "iteration 391200 training loss 0.8257707 lr 0.00007\n",
      "iteration 391300 training loss 0.72572607 lr 0.00007\n",
      "iteration 391400 training loss 0.7643468 lr 0.00007\n",
      "iteration 391500 training loss 0.66940993 lr 0.00007\n",
      "iteration 391600 training loss 0.6873497 lr 0.00007\n",
      "iteration 391700 training loss 0.5920232 lr 0.00007\n",
      "iteration 391800 training loss 0.5265391 lr 0.00007\n",
      "iteration 391900 training loss 0.5688373 lr 0.00007\n",
      "iteration 392000 training loss 0.79766715 lr 0.00007\n",
      "iteration 392100 training loss 0.7893162 lr 0.00007\n",
      "iteration 392200 training loss 0.6141135 lr 0.00007\n",
      "iteration 392300 training loss 0.65385604 lr 0.00007\n",
      "iteration 392400 training loss 0.56660116 lr 0.00007\n",
      "iteration 392500 training loss 0.6136259 lr 0.00007\n",
      "iteration 392600 training loss 0.7768742 lr 0.00007\n",
      "iteration 392700 training loss 0.63077414 lr 0.00007\n",
      "iteration 392800 training loss 0.7435417 lr 0.00007\n",
      "iteration 392900 training loss 0.75731546 lr 0.00007\n",
      "iteration 393000 training loss 0.7276906 lr 0.00007\n",
      "iteration 393100 training loss 0.6882957 lr 0.00007\n",
      "iteration 393200 training loss 0.64324486 lr 0.00007\n",
      "iteration 393300 training loss 0.7776052 lr 0.00007\n",
      "iteration 393400 training loss 0.75180745 lr 0.00007\n",
      "iteration 393500 training loss 0.773617 lr 0.00007\n",
      "iteration 393600 training loss 0.63627416 lr 0.00007\n",
      "iteration 393700 training loss 0.7822551 lr 0.00007\n",
      "iteration 393800 training loss 0.76463103 lr 0.00007\n",
      "iteration 393900 training loss 0.69579786 lr 0.00007\n",
      "iteration 394000 training loss 0.9592789 lr 0.00007\n",
      "iteration 394100 training loss 0.7417902 lr 0.00007\n",
      "iteration 394200 training loss 0.61498106 lr 0.00007\n",
      "iteration 394300 training loss 0.70827836 lr 0.00007\n",
      "iteration 394400 training loss 0.6917436 lr 0.00007\n",
      "iteration 394500 training loss 0.7116273 lr 0.00007\n",
      "iteration 394600 training loss 0.67659223 lr 0.00007\n",
      "iteration 394700 training loss 0.76917386 lr 0.00007\n",
      "iteration 394800 training loss 0.6479202 lr 0.00007\n",
      "iteration 394900 training loss 0.5541691 lr 0.00007\n",
      "iteration 395000 training loss 0.5476206 lr 0.00007\n",
      "iteration 395100 training loss 0.61990356 lr 0.00007\n",
      "iteration 395200 training loss 0.7072489 lr 0.00007\n",
      "iteration 395300 training loss 0.7507407 lr 0.00007\n",
      "iteration 395400 training loss 0.832023 lr 0.00007\n",
      "iteration 395500 training loss 0.68617797 lr 0.00007\n",
      "iteration 395600 training loss 0.5392421 lr 0.00007\n",
      "iteration 395700 training loss 0.6672486 lr 0.00007\n",
      "iteration 395800 training loss 0.6231913 lr 0.00007\n",
      "iteration 395900 training loss 0.6855286 lr 0.00007\n",
      "iteration 396000 training loss 0.64126545 lr 0.00007\n",
      "iteration 396100 training loss 0.69118583 lr 0.00007\n",
      "iteration 396200 training loss 0.64314204 lr 0.00007\n",
      "iteration 396300 training loss 0.76832336 lr 0.00007\n",
      "iteration 396400 training loss 0.63610226 lr 0.00007\n",
      "iteration 396500 training loss 0.7908012 lr 0.00007\n",
      "iteration 396600 training loss 0.85179776 lr 0.00007\n",
      "iteration 396700 training loss 0.8497077 lr 0.00007\n",
      "iteration 396800 training loss 0.5443223 lr 0.00007\n",
      "iteration 396900 training loss 0.6600958 lr 0.00007\n",
      "iteration 397000 training loss 0.8243147 lr 0.00007\n",
      "iteration 397100 training loss 0.7013827 lr 0.00007\n",
      "iteration 397200 training loss 0.64231855 lr 0.00007\n",
      "iteration 397300 training loss 0.63078916 lr 0.00007\n",
      "iteration 397400 training loss 0.6318817 lr 0.00007\n",
      "iteration 397500 training loss 0.64455557 lr 0.00007\n",
      "iteration 397600 training loss 0.5766561 lr 0.00007\n",
      "iteration 397700 training loss 0.5156525 lr 0.00007\n",
      "iteration 397800 training loss 0.81698877 lr 0.00007\n",
      "iteration 397900 training loss 0.74063957 lr 0.00007\n",
      "iteration 398000 training loss 0.6386987 lr 0.00007\n",
      "iteration 398100 training loss 0.62528604 lr 0.00007\n",
      "iteration 398200 training loss 0.45264316 lr 0.00007\n",
      "iteration 398300 training loss 0.85201126 lr 0.00007\n",
      "iteration 398400 training loss 0.4677284 lr 0.00007\n",
      "iteration 398500 training loss 0.7325466 lr 0.00007\n",
      "iteration 398600 training loss 0.69273126 lr 0.00007\n",
      "iteration 398700 training loss 0.48443538 lr 0.00007\n",
      "iteration 398800 training loss 0.71715826 lr 0.00007\n",
      "iteration 398900 training loss 0.6464893 lr 0.00007\n",
      "iteration 399000 training loss 0.63705623 lr 0.00007\n",
      "iteration 399100 training loss 0.73310363 lr 0.00007\n",
      "iteration 399200 training loss 0.5764546 lr 0.00007\n",
      "iteration 399300 training loss 0.49769896 lr 0.00007\n",
      "iteration 399400 training loss 0.8400162 lr 0.00007\n",
      "iteration 399500 training loss 0.7006083 lr 0.00007\n",
      "iteration 399600 training loss 0.70785934 lr 0.00007\n",
      "iteration 399700 training loss 0.773971 lr 0.00007\n",
      "iteration 399800 training loss 0.60113716 lr 0.00007\n",
      "iteration 399900 training loss 0.56850165 lr 0.00007\n",
      "iteration 400000 training loss 0.7260495 lr 0.00007\n",
      "layout:nlp:random 0.8084942144306065\n",
      "layout:nlp:default 0.4218673085968687\n",
      "layout:xla:random 0.4454777117326019\n",
      "layout:xla:default 0.21934467176219502\n",
      "epoch 0, it 400000 validation loss -0.474\n",
      "iteration 400100 training loss 0.72148454 lr 0.00006\n",
      "iteration 400200 training loss 0.5865742 lr 0.00006\n",
      "iteration 400300 training loss 0.6000959 lr 0.00006\n",
      "iteration 400400 training loss 0.8054314 lr 0.00006\n",
      "iteration 400500 training loss 0.87234634 lr 0.00006\n",
      "iteration 400600 training loss 0.75229555 lr 0.00006\n",
      "iteration 400700 training loss 0.8154426 lr 0.00006\n",
      "iteration 400800 training loss 0.5969871 lr 0.00006\n",
      "iteration 400900 training loss 0.7187119 lr 0.00006\n",
      "iteration 401000 training loss 0.77255297 lr 0.00006\n",
      "iteration 401100 training loss 0.5837457 lr 0.00006\n",
      "iteration 401200 training loss 0.54280674 lr 0.00006\n",
      "iteration 401300 training loss 0.48476067 lr 0.00006\n",
      "iteration 401400 training loss 0.616902 lr 0.00006\n",
      "iteration 401500 training loss 0.61755586 lr 0.00006\n",
      "iteration 401600 training loss 0.65049386 lr 0.00006\n",
      "iteration 401700 training loss 0.708349 lr 0.00006\n",
      "iteration 401800 training loss 0.7413515 lr 0.00006\n",
      "iteration 401900 training loss 0.69033355 lr 0.00006\n",
      "iteration 402000 training loss 0.7049354 lr 0.00006\n",
      "iteration 402100 training loss 0.58943766 lr 0.00006\n",
      "iteration 402200 training loss 0.7210688 lr 0.00006\n",
      "iteration 402300 training loss 0.5570105 lr 0.00006\n",
      "iteration 402400 training loss 0.74049973 lr 0.00006\n",
      "iteration 402500 training loss 0.8728111 lr 0.00006\n",
      "iteration 402600 training loss 0.7864772 lr 0.00006\n",
      "iteration 402700 training loss 0.6125893 lr 0.00006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 402800 training loss 0.61246294 lr 0.00006\n",
      "iteration 402900 training loss 0.715901 lr 0.00006\n",
      "iteration 403000 training loss 0.791832 lr 0.00006\n",
      "iteration 403100 training loss 0.72934675 lr 0.00006\n",
      "iteration 403200 training loss 0.6463165 lr 0.00006\n",
      "iteration 403300 training loss 0.76568526 lr 0.00006\n",
      "iteration 403400 training loss 0.6254197 lr 0.00006\n",
      "iteration 403500 training loss 0.6745026 lr 0.00006\n",
      "iteration 403600 training loss 0.58731604 lr 0.00006\n",
      "iteration 403700 training loss 0.609097 lr 0.00006\n",
      "iteration 403800 training loss 0.64033115 lr 0.00006\n",
      "iteration 403900 training loss 0.6371961 lr 0.00006\n",
      "iteration 404000 training loss 0.69955593 lr 0.00006\n",
      "iteration 404100 training loss 0.46059757 lr 0.00006\n",
      "iteration 404200 training loss 0.6207914 lr 0.00006\n",
      "iteration 404300 training loss 0.68230116 lr 0.00006\n",
      "iteration 404400 training loss 0.41294447 lr 0.00006\n",
      "iteration 404500 training loss 0.6685824 lr 0.00006\n",
      "iteration 404600 training loss 0.6501728 lr 0.00006\n",
      "iteration 404700 training loss 0.626827 lr 0.00006\n",
      "iteration 404800 training loss 0.53754526 lr 0.00006\n",
      "iteration 404900 training loss 0.6845496 lr 0.00006\n",
      "iteration 405000 training loss 0.6764065 lr 0.00006\n",
      "iteration 405100 training loss 0.6295002 lr 0.00006\n",
      "iteration 405200 training loss 0.8001805 lr 0.00006\n",
      "iteration 405300 training loss 0.61859834 lr 0.00006\n",
      "iteration 405400 training loss 0.7523289 lr 0.00006\n",
      "iteration 405500 training loss 0.597846 lr 0.00006\n",
      "iteration 405600 training loss 0.60227334 lr 0.00006\n",
      "iteration 405700 training loss 0.62878263 lr 0.00006\n",
      "iteration 405800 training loss 0.7217514 lr 0.00006\n",
      "iteration 405900 training loss 0.7039901 lr 0.00006\n",
      "iteration 406000 training loss 0.56725323 lr 0.00006\n",
      "iteration 406100 training loss 0.6789365 lr 0.00006\n",
      "iteration 406200 training loss 0.7053714 lr 0.00006\n",
      "iteration 406300 training loss 0.5528792 lr 0.00006\n",
      "iteration 406400 training loss 0.64693874 lr 0.00006\n",
      "iteration 406500 training loss 0.71408594 lr 0.00006\n",
      "iteration 406600 training loss 0.6993705 lr 0.00006\n",
      "iteration 406700 training loss 0.8056811 lr 0.00006\n",
      "iteration 406800 training loss 0.7740701 lr 0.00006\n",
      "iteration 406900 training loss 0.7720188 lr 0.00006\n",
      "iteration 407000 training loss 0.57062906 lr 0.00006\n",
      "iteration 407100 training loss 0.8059594 lr 0.00006\n",
      "iteration 407200 training loss 0.6300587 lr 0.00006\n",
      "iteration 407300 training loss 0.6869562 lr 0.00006\n",
      "iteration 407400 training loss 0.649804 lr 0.00006\n",
      "iteration 407500 training loss 0.45424038 lr 0.00006\n",
      "iteration 407600 training loss 0.75587636 lr 0.00006\n",
      "iteration 407700 training loss 0.74442726 lr 0.00006\n",
      "iteration 407800 training loss 0.6142461 lr 0.00006\n",
      "iteration 407900 training loss 0.6849099 lr 0.00006\n",
      "iteration 408000 training loss 0.6579954 lr 0.00006\n",
      "iteration 408100 training loss 0.67803425 lr 0.00006\n",
      "iteration 408200 training loss 0.596864 lr 0.00006\n",
      "iteration 408300 training loss 0.7069587 lr 0.00006\n",
      "iteration 408400 training loss 0.837157 lr 0.00006\n",
      "iteration 408500 training loss 0.6857169 lr 0.00006\n",
      "iteration 408600 training loss 0.6440892 lr 0.00006\n",
      "iteration 408700 training loss 0.83146024 lr 0.00006\n",
      "iteration 408800 training loss 0.63465345 lr 0.00006\n",
      "iteration 408900 training loss 0.5205924 lr 0.00006\n",
      "iteration 409000 training loss 0.8735004 lr 0.00006\n",
      "iteration 409100 training loss 0.71682703 lr 0.00006\n",
      "iteration 409200 training loss 0.8859515 lr 0.00006\n",
      "iteration 409300 training loss 0.6405314 lr 0.00006\n",
      "iteration 409400 training loss 0.5848478 lr 0.00006\n",
      "iteration 409500 training loss 0.86709064 lr 0.00006\n",
      "iteration 409600 training loss 0.89744073 lr 0.00006\n",
      "iteration 409700 training loss 0.649072 lr 0.00006\n",
      "iteration 409800 training loss 0.43055367 lr 0.00006\n",
      "iteration 409900 training loss 0.7145618 lr 0.00006\n",
      "iteration 410000 training loss 0.6513724 lr 0.00006\n",
      "iteration 410100 training loss 0.6718471 lr 0.00006\n",
      "iteration 410200 training loss 0.680748 lr 0.00006\n",
      "iteration 410300 training loss 0.68737566 lr 0.00006\n",
      "iteration 410400 training loss 0.78929263 lr 0.00006\n",
      "iteration 410500 training loss 0.6495539 lr 0.00006\n",
      "iteration 410600 training loss 0.6536015 lr 0.00006\n",
      "iteration 410700 training loss 0.6715604 lr 0.00006\n",
      "iteration 410800 training loss 0.77696425 lr 0.00006\n",
      "iteration 410900 training loss 0.6626512 lr 0.00006\n",
      "iteration 411000 training loss 0.55139506 lr 0.00006\n",
      "iteration 411100 training loss 0.70148754 lr 0.00006\n",
      "iteration 411200 training loss 0.59534323 lr 0.00006\n",
      "iteration 411300 training loss 0.5578161 lr 0.00006\n",
      "iteration 411400 training loss 0.507554 lr 0.00006\n",
      "iteration 411500 training loss 0.59577304 lr 0.00006\n",
      "iteration 411600 training loss 0.6404677 lr 0.00006\n",
      "iteration 411700 training loss 0.69963455 lr 0.00006\n",
      "iteration 411800 training loss 0.5264003 lr 0.00006\n",
      "iteration 411900 training loss 0.49549103 lr 0.00006\n",
      "iteration 412000 training loss 0.6306343 lr 0.00006\n",
      "iteration 412100 training loss 0.6434281 lr 0.00006\n",
      "iteration 412200 training loss 0.5141505 lr 0.00006\n",
      "iteration 412300 training loss 0.756934 lr 0.00006\n",
      "iteration 412400 training loss 0.6748898 lr 0.00006\n",
      "iteration 412500 training loss 0.59778607 lr 0.00006\n",
      "iteration 412600 training loss 0.6424075 lr 0.00006\n",
      "iteration 412700 training loss 0.7064032 lr 0.00006\n",
      "iteration 412800 training loss 0.63796335 lr 0.00006\n",
      "iteration 412900 training loss 0.74646145 lr 0.00006\n",
      "iteration 413000 training loss 0.6344806 lr 0.00006\n",
      "iteration 413100 training loss 0.67948014 lr 0.00006\n",
      "iteration 413200 training loss 0.68328804 lr 0.00006\n",
      "iteration 413300 training loss 0.7261531 lr 0.00006\n",
      "iteration 413400 training loss 0.6500986 lr 0.00006\n",
      "iteration 413500 training loss 0.688423 lr 0.00006\n",
      "iteration 413600 training loss 0.6872574 lr 0.00006\n",
      "iteration 413700 training loss 0.5707144 lr 0.00006\n",
      "iteration 413800 training loss 0.58634514 lr 0.00006\n",
      "iteration 413900 training loss 0.5342212 lr 0.00006\n",
      "iteration 414000 training loss 0.6636145 lr 0.00006\n",
      "iteration 414100 training loss 0.624245 lr 0.00006\n",
      "iteration 414200 training loss 0.8570221 lr 0.00006\n",
      "iteration 414300 training loss 0.7984606 lr 0.00006\n",
      "iteration 414400 training loss 0.60391927 lr 0.00006\n",
      "iteration 414500 training loss 0.7262988 lr 0.00006\n",
      "iteration 414600 training loss 0.77338135 lr 0.00006\n",
      "iteration 414700 training loss 0.5243631 lr 0.00006\n",
      "iteration 414800 training loss 0.5703572 lr 0.00006\n",
      "iteration 414900 training loss 0.7001192 lr 0.00006\n",
      "iteration 415000 training loss 0.6638963 lr 0.00006\n",
      "iteration 415100 training loss 0.7333759 lr 0.00006\n",
      "iteration 415200 training loss 0.447762 lr 0.00006\n",
      "iteration 415300 training loss 0.62638676 lr 0.00006\n",
      "iteration 415400 training loss 0.45867327 lr 0.00006\n",
      "iteration 415500 training loss 0.5579111 lr 0.00006\n",
      "iteration 415600 training loss 0.58442 lr 0.00006\n",
      "iteration 415700 training loss 0.5842317 lr 0.00006\n",
      "iteration 415800 training loss 0.66870546 lr 0.00006\n",
      "iteration 415900 training loss 0.65478086 lr 0.00006\n",
      "iteration 416000 training loss 0.7048323 lr 0.00006\n",
      "iteration 416100 training loss 0.897934 lr 0.00006\n",
      "iteration 416200 training loss 0.65715057 lr 0.00006\n",
      "iteration 416300 training loss 0.53488046 lr 0.00006\n",
      "iteration 416400 training loss 0.61111623 lr 0.00006\n",
      "iteration 416500 training loss 0.77470654 lr 0.00006\n",
      "iteration 416600 training loss 0.69705814 lr 0.00006\n",
      "iteration 416700 training loss 0.6131293 lr 0.00006\n",
      "iteration 416800 training loss 0.75725883 lr 0.00006\n",
      "iteration 416900 training loss 0.62550193 lr 0.00006\n",
      "iteration 417000 training loss 0.7960819 lr 0.00006\n",
      "iteration 417100 training loss 0.64012796 lr 0.00006\n",
      "iteration 417200 training loss 0.73848295 lr 0.00006\n",
      "iteration 417300 training loss 0.6554753 lr 0.00006\n",
      "iteration 417400 training loss 0.61616474 lr 0.00006\n",
      "iteration 417500 training loss 0.5730552 lr 0.00006\n",
      "iteration 417600 training loss 0.5541518 lr 0.00006\n",
      "iteration 417700 training loss 0.54810226 lr 0.00006\n",
      "iteration 417800 training loss 0.49355105 lr 0.00006\n",
      "iteration 417900 training loss 0.73338276 lr 0.00006\n",
      "iteration 418000 training loss 0.66622394 lr 0.00006\n",
      "iteration 418100 training loss 0.7429494 lr 0.00006\n",
      "iteration 418200 training loss 0.7367092 lr 0.00006\n",
      "iteration 418300 training loss 0.6642848 lr 0.00006\n",
      "iteration 418400 training loss 0.44664836 lr 0.00006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 418500 training loss 0.5686066 lr 0.00006\n",
      "iteration 418600 training loss 0.5953515 lr 0.00006\n",
      "iteration 418700 training loss 0.49811137 lr 0.00006\n",
      "iteration 418800 training loss 0.6485481 lr 0.00006\n",
      "iteration 418900 training loss 0.52715343 lr 0.00006\n",
      "iteration 419000 training loss 0.6169645 lr 0.00006\n",
      "iteration 419100 training loss 0.6318433 lr 0.00006\n",
      "iteration 419200 training loss 0.5959637 lr 0.00006\n",
      "iteration 419300 training loss 0.53846747 lr 0.00006\n",
      "iteration 419400 training loss 0.49740818 lr 0.00006\n",
      "iteration 419500 training loss 0.75386465 lr 0.00006\n",
      "iteration 419600 training loss 0.7893658 lr 0.00006\n",
      "iteration 419700 training loss 0.76467884 lr 0.00006\n",
      "iteration 419800 training loss 0.701577 lr 0.00006\n",
      "iteration 419900 training loss 0.4914413 lr 0.00006\n",
      "iteration 420000 training loss 0.63665336 lr 0.00006\n",
      "layout:nlp:random 0.8167070190035532\n",
      "layout:nlp:default 0.4208509632679462\n",
      "layout:xla:random 0.4941145233869843\n",
      "layout:xla:default 0.2249606890841733\n",
      "epoch 0, it 420000 validation loss -0.489\n",
      "iteration 420100 training loss 0.5932022 lr 0.00005\n",
      "iteration 420200 training loss 0.61613905 lr 0.00005\n",
      "iteration 420300 training loss 0.644754 lr 0.00005\n",
      "iteration 420400 training loss 0.7317574 lr 0.00005\n",
      "iteration 420500 training loss 0.59384 lr 0.00005\n",
      "iteration 420600 training loss 0.6303666 lr 0.00005\n",
      "iteration 420700 training loss 0.52305174 lr 0.00005\n",
      "iteration 420800 training loss 0.44328552 lr 0.00005\n",
      "iteration 420900 training loss 0.6493245 lr 0.00005\n",
      "iteration 421000 training loss 0.7778445 lr 0.00005\n",
      "iteration 421100 training loss 0.63647956 lr 0.00005\n",
      "iteration 421200 training loss 0.65387917 lr 0.00005\n",
      "iteration 421300 training loss 0.72051096 lr 0.00005\n",
      "iteration 421400 training loss 0.6846188 lr 0.00005\n",
      "iteration 421500 training loss 0.5726258 lr 0.00005\n",
      "iteration 421600 training loss 0.75191575 lr 0.00005\n",
      "iteration 421700 training loss 0.5867059 lr 0.00005\n",
      "iteration 421800 training loss 0.67274976 lr 0.00005\n",
      "iteration 421900 training loss 0.6022218 lr 0.00005\n",
      "iteration 422000 training loss 0.6434792 lr 0.00005\n",
      "iteration 422100 training loss 0.7314141 lr 0.00005\n",
      "iteration 422200 training loss 0.8117199 lr 0.00005\n",
      "iteration 422300 training loss 0.4399766 lr 0.00005\n",
      "iteration 422400 training loss 0.7972748 lr 0.00005\n",
      "iteration 422500 training loss 0.73391896 lr 0.00005\n",
      "iteration 422600 training loss 0.69965124 lr 0.00005\n",
      "iteration 422700 training loss 0.5742719 lr 0.00005\n",
      "iteration 422800 training loss 0.69501525 lr 0.00005\n",
      "iteration 422900 training loss 0.89635307 lr 0.00005\n",
      "iteration 423000 training loss 0.7759808 lr 0.00005\n",
      "iteration 423100 training loss 0.707762 lr 0.00005\n",
      "iteration 423200 training loss 0.75641376 lr 0.00005\n",
      "iteration 423300 training loss 0.8760986 lr 0.00005\n",
      "iteration 423400 training loss 0.7089968 lr 0.00005\n",
      "iteration 423500 training loss 0.72323865 lr 0.00005\n",
      "iteration 423600 training loss 0.6871649 lr 0.00005\n",
      "iteration 423700 training loss 0.6791477 lr 0.00005\n",
      "iteration 423800 training loss 0.8310828 lr 0.00005\n",
      "iteration 423900 training loss 0.6029819 lr 0.00005\n",
      "iteration 424000 training loss 0.65022284 lr 0.00005\n",
      "iteration 424100 training loss 0.8166853 lr 0.00005\n",
      "iteration 424200 training loss 0.6077452 lr 0.00005\n",
      "iteration 424300 training loss 0.79439616 lr 0.00005\n",
      "iteration 424400 training loss 0.648352 lr 0.00005\n",
      "iteration 424500 training loss 0.68303454 lr 0.00005\n",
      "iteration 424600 training loss 0.65875316 lr 0.00005\n",
      "iteration 424700 training loss 0.59074056 lr 0.00005\n",
      "iteration 424800 training loss 0.5917297 lr 0.00005\n",
      "iteration 424900 training loss 0.84220946 lr 0.00005\n",
      "iteration 425000 training loss 0.80669487 lr 0.00005\n",
      "iteration 425100 training loss 0.52267754 lr 0.00005\n",
      "iteration 425200 training loss 0.68822396 lr 0.00005\n",
      "iteration 425300 training loss 0.6695661 lr 0.00005\n",
      "iteration 425400 training loss 0.71183103 lr 0.00005\n",
      "iteration 425500 training loss 0.67761976 lr 0.00005\n",
      "iteration 425600 training loss 0.4855907 lr 0.00005\n",
      "iteration 425700 training loss 0.5285225 lr 0.00005\n",
      "iteration 425800 training loss 0.5590084 lr 0.00005\n",
      "iteration 425900 training loss 0.5446651 lr 0.00005\n",
      "iteration 426000 training loss 0.70986146 lr 0.00005\n",
      "iteration 426100 training loss 0.7454556 lr 0.00005\n",
      "iteration 426200 training loss 0.58641714 lr 0.00005\n",
      "iteration 426300 training loss 0.5835102 lr 0.00005\n",
      "iteration 426400 training loss 0.5029897 lr 0.00005\n",
      "iteration 426500 training loss 0.6212449 lr 0.00005\n",
      "iteration 426600 training loss 0.7024754 lr 0.00005\n",
      "iteration 426700 training loss 0.61378354 lr 0.00005\n",
      "iteration 426800 training loss 0.70887095 lr 0.00005\n",
      "iteration 426900 training loss 0.6801827 lr 0.00005\n",
      "iteration 427000 training loss 0.7140564 lr 0.00005\n",
      "iteration 427100 training loss 0.54125124 lr 0.00005\n",
      "iteration 427200 training loss 0.6652163 lr 0.00005\n",
      "iteration 427300 training loss 0.65787065 lr 0.00005\n",
      "iteration 427400 training loss 0.58866686 lr 0.00005\n",
      "iteration 427500 training loss 0.583948 lr 0.00005\n",
      "iteration 427600 training loss 0.5899479 lr 0.00005\n",
      "iteration 427700 training loss 0.7013911 lr 0.00005\n",
      "iteration 427800 training loss 0.6365098 lr 0.00005\n",
      "iteration 427900 training loss 0.90170914 lr 0.00005\n",
      "iteration 428000 training loss 0.6889503 lr 0.00005\n",
      "iteration 428100 training loss 0.53315866 lr 0.00005\n",
      "iteration 428200 training loss 0.61500686 lr 0.00005\n",
      "iteration 428300 training loss 0.80107677 lr 0.00005\n",
      "iteration 428400 training loss 0.7951925 lr 0.00005\n",
      "iteration 428500 training loss 0.54797375 lr 0.00005\n",
      "iteration 428600 training loss 0.759198 lr 0.00005\n",
      "iteration 428700 training loss 0.5852075 lr 0.00005\n",
      "iteration 428800 training loss 0.6601654 lr 0.00005\n",
      "iteration 428900 training loss 0.74739605 lr 0.00005\n",
      "iteration 429000 training loss 0.76455927 lr 0.00005\n",
      "iteration 429100 training loss 0.6737289 lr 0.00005\n",
      "iteration 429200 training loss 0.5989718 lr 0.00005\n",
      "iteration 429300 training loss 0.6337167 lr 0.00005\n",
      "iteration 429400 training loss 0.7074421 lr 0.00005\n",
      "iteration 429500 training loss 0.5277268 lr 0.00005\n",
      "iteration 429600 training loss 0.7821223 lr 0.00005\n",
      "iteration 429700 training loss 0.6803267 lr 0.00005\n",
      "iteration 429800 training loss 0.6695297 lr 0.00005\n",
      "iteration 429900 training loss 0.8363523 lr 0.00005\n",
      "iteration 430000 training loss 0.7277135 lr 0.00005\n",
      "iteration 430100 training loss 0.6673101 lr 0.00005\n",
      "iteration 430200 training loss 0.75917286 lr 0.00005\n",
      "iteration 430300 training loss 0.79761964 lr 0.00005\n",
      "iteration 430400 training loss 0.75773454 lr 0.00005\n",
      "iteration 430500 training loss 0.83943945 lr 0.00005\n",
      "iteration 430600 training loss 0.50973356 lr 0.00005\n",
      "iteration 430700 training loss 0.7211201 lr 0.00005\n",
      "iteration 430800 training loss 0.6986057 lr 0.00005\n",
      "iteration 430900 training loss 0.46055034 lr 0.00005\n",
      "iteration 431000 training loss 0.82696605 lr 0.00005\n",
      "iteration 431100 training loss 0.5821829 lr 0.00005\n",
      "iteration 431200 training loss 0.54416496 lr 0.00005\n",
      "iteration 431300 training loss 0.8840042 lr 0.00005\n",
      "iteration 431400 training loss 0.6965577 lr 0.00005\n",
      "iteration 431500 training loss 0.49412262 lr 0.00005\n",
      "iteration 431600 training loss 0.6385173 lr 0.00005\n",
      "iteration 431700 training loss 0.70917624 lr 0.00005\n",
      "iteration 431800 training loss 0.67395353 lr 0.00005\n",
      "iteration 431900 training loss 0.70640963 lr 0.00005\n",
      "iteration 432000 training loss 0.7421444 lr 0.00005\n",
      "iteration 432100 training loss 0.58002794 lr 0.00005\n",
      "iteration 432200 training loss 0.77060735 lr 0.00005\n",
      "iteration 432300 training loss 0.50865114 lr 0.00005\n",
      "iteration 432400 training loss 0.503466 lr 0.00005\n",
      "iteration 432500 training loss 0.59625995 lr 0.00005\n",
      "iteration 432600 training loss 0.49025708 lr 0.00005\n",
      "iteration 432700 training loss 0.51322126 lr 0.00005\n",
      "iteration 432800 training loss 0.4763417 lr 0.00005\n",
      "iteration 432900 training loss 0.61206734 lr 0.00005\n",
      "iteration 433000 training loss 0.715219 lr 0.00005\n",
      "iteration 433100 training loss 0.7502354 lr 0.00005\n",
      "iteration 433200 training loss 0.7073938 lr 0.00005\n",
      "iteration 433300 training loss 0.6929623 lr 0.00005\n",
      "iteration 433400 training loss 0.66304535 lr 0.00005\n",
      "iteration 433500 training loss 0.75665563 lr 0.00005\n",
      "iteration 433600 training loss 0.48143044 lr 0.00005\n",
      "iteration 433700 training loss 0.5179532 lr 0.00005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 433800 training loss 0.5673549 lr 0.00005\n",
      "iteration 433900 training loss 0.71306723 lr 0.00005\n",
      "iteration 434000 training loss 0.6247071 lr 0.00005\n",
      "iteration 434100 training loss 0.56945074 lr 0.00005\n",
      "iteration 434200 training loss 0.548786 lr 0.00005\n",
      "iteration 434300 training loss 0.5637879 lr 0.00005\n",
      "iteration 434400 training loss 0.61204296 lr 0.00005\n",
      "iteration 434500 training loss 0.54428416 lr 0.00005\n",
      "iteration 434600 training loss 0.6344067 lr 0.00005\n",
      "iteration 434700 training loss 0.61916333 lr 0.00005\n",
      "iteration 434800 training loss 0.53525764 lr 0.00005\n",
      "iteration 434900 training loss 0.6370007 lr 0.00005\n",
      "iteration 435000 training loss 0.651217 lr 0.00005\n",
      "iteration 435100 training loss 0.7446531 lr 0.00005\n",
      "iteration 435200 training loss 0.60106057 lr 0.00005\n",
      "iteration 435300 training loss 0.46266654 lr 0.00005\n",
      "iteration 435400 training loss 0.74618506 lr 0.00005\n",
      "iteration 435500 training loss 0.74331397 lr 0.00005\n",
      "iteration 435600 training loss 0.6449536 lr 0.00005\n",
      "iteration 435700 training loss 0.5562383 lr 0.00005\n",
      "iteration 435800 training loss 0.7465817 lr 0.00005\n",
      "iteration 435900 training loss 0.5169812 lr 0.00005\n",
      "iteration 436000 training loss 0.6161923 lr 0.00005\n",
      "iteration 436100 training loss 0.57050365 lr 0.00005\n",
      "iteration 436200 training loss 0.5981731 lr 0.00005\n",
      "iteration 436300 training loss 0.6012591 lr 0.00005\n",
      "iteration 436400 training loss 0.77637 lr 0.00005\n",
      "iteration 436500 training loss 0.7898321 lr 0.00005\n",
      "iteration 436600 training loss 0.69137436 lr 0.00005\n",
      "iteration 436700 training loss 0.6906882 lr 0.00005\n",
      "iteration 436800 training loss 0.68014103 lr 0.00005\n",
      "iteration 436900 training loss 0.7511611 lr 0.00005\n",
      "iteration 437000 training loss 0.5976607 lr 0.00005\n",
      "iteration 437100 training loss 0.6614775 lr 0.00005\n",
      "iteration 437200 training loss 0.7068577 lr 0.00005\n",
      "iteration 437300 training loss 0.72357374 lr 0.00005\n",
      "iteration 437400 training loss 0.6415849 lr 0.00005\n",
      "iteration 437500 training loss 0.7472785 lr 0.00005\n",
      "iteration 437600 training loss 0.6775134 lr 0.00005\n",
      "iteration 437700 training loss 0.5297665 lr 0.00005\n",
      "iteration 437800 training loss 0.5825698 lr 0.00005\n",
      "iteration 437900 training loss 0.6757253 lr 0.00005\n",
      "iteration 438000 training loss 0.7278283 lr 0.00005\n",
      "iteration 438100 training loss 0.6966687 lr 0.00005\n",
      "iteration 438200 training loss 0.95219296 lr 0.00005\n",
      "iteration 438300 training loss 0.45810977 lr 0.00005\n",
      "iteration 438400 training loss 0.78998446 lr 0.00005\n",
      "iteration 438500 training loss 0.5385482 lr 0.00005\n",
      "iteration 438600 training loss 0.5898252 lr 0.00005\n",
      "iteration 438700 training loss 0.6240606 lr 0.00005\n",
      "iteration 438800 training loss 0.70578027 lr 0.00005\n",
      "iteration 438900 training loss 0.6076373 lr 0.00005\n",
      "iteration 439000 training loss 0.6151327 lr 0.00005\n",
      "iteration 439100 training loss 0.5430318 lr 0.00005\n",
      "iteration 439200 training loss 0.6453968 lr 0.00005\n",
      "iteration 439300 training loss 0.5712764 lr 0.00005\n",
      "iteration 439400 training loss 0.7364283 lr 0.00005\n",
      "iteration 439500 training loss 0.65000314 lr 0.00005\n",
      "iteration 439600 training loss 0.6421096 lr 0.00005\n",
      "iteration 439700 training loss 0.71629864 lr 0.00005\n",
      "iteration 439800 training loss 0.7987408 lr 0.00005\n",
      "iteration 439900 training loss 0.73994195 lr 0.00005\n",
      "iteration 440000 training loss 0.59853804 lr 0.00005\n",
      "layout:nlp:random 0.820018187463693\n",
      "layout:nlp:default 0.4305570648263476\n",
      "layout:xla:random 0.473838907732333\n",
      "layout:xla:default 0.21599303094016892\n",
      "epoch 0, it 440000 validation loss -0.485\n",
      "iteration 440100 training loss 0.7348523 lr 0.00005\n",
      "iteration 440200 training loss 0.70534325 lr 0.00005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/kaggle_model_runtime/models.py:106\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, training_dataset, validation_dataset)\u001b[0m\n\u001b[1;32m    103\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    104\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 106\u001b[0m should_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m training_dataset:\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.train(dataset.train_data, dataset.valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3418de",
   "metadata": {},
   "source": [
    "## Evaluate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac0e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = mlp.predict_over_dataset(dataset.valid_data, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1d9033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(val_df['target'], bins=50)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(val_df['prediction'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72249379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"b'layout:xla:random:inception_v3_batch_128_train'\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_lin = np.linspace(15, 25, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "random_sample = val_df.sample(10_000)\n",
    "graph_id = np.random.choice(val_df['ID'].unique())\n",
    "random_sample = val_df[val_df['ID'] == graph_id]\n",
    "\n",
    "plt.scatter(\n",
    "    random_sample.target,\n",
    "    np.clip(random_sample.prediction, a_min=-10000.0, a_max=1000.0),\n",
    "    alpha=0.1,\n",
    "    #c=random_sample['ID'].apply(lambda x: x.decode('UTF-8').split(':')[1] == 'xla').values.astype(float)\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(graph_id)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0639b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = val_df.sample(5_000)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.abs(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('abs error')\n",
    "x_lin = np.linspace(0, 0.7, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.square(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('squared error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c359b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "layout:nlp:default:albert_en_xlarge_batch_size_16_test                               33725;28620;25258;25253;28643;33752;48197;4260...\n",
       "layout:nlp:default:bert_en_cased_L-12_H-768_A-12_batch_size_16_test                  33861;29668;15787;59590;22487;22204;15800;1579...\n",
       "layout:nlp:default:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train              11447;11445;20061;11473;11452;11479;11444;7163...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test      91726;87214;71864;73624;73623;87558;13758;7999...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train     47728;48483;32133;32130;37636;32740;32651;3265...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test      49776;84047;82277;29292;91437;51777;99597;2552...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train     36544;23171;19025;36556;16195;497;487;18395;25...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test      23872;56137;64088;23842;23863;21272;64113;1965...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train    597;693;4985;4984;26580;26571;745;669;4971;640...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test     59594;8070;7400;46133;59575;52934;17571;31160;...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train    16532;16510;16544;17454;16509;21407;16508;1745...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train      665;660;84396;17832;8761;8740;8752;8156;83434;...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train      83016;82995;4698;90998;82997;90994;8844;1383;8...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train      43081;18106;17120;63639;58617;15460;15460;6361...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train      39316;13923;13923;80498;14614;14364;14602;1460...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train      52207;52317;52317;52174;52210;52254;52254;5218...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test       8860;9944;9907;9905;42054;83732;31062;19528;69...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test      36816;36816;17673;79112;79112;79112;62565;2558...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train     17155;35029;40400;35045;18727;34993;35018;2139...\n",
       "layout:nlp:default:talking-heads_large_batch_size_16_train                           5876;10484;10450;10450;10450;10450;10469;5947;...\n",
       "layout:nlp:random:albert_en_xlarge_batch_size_16_test                                40835;46767;43889;6548;8428;30844;42489;47109;...\n",
       "layout:nlp:random:bert_en_cased_L-12_H-768_A-12_batch_size_16_test                   10417;39708;97184;15927;61892;84713;23875;2435...\n",
       "layout:nlp:random:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train               1375;18430;10347;9880;17697;23022;6450;24810;2...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test       93069;83452;83452;41782;77958;36326;18891;4923...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train      36105;20216;1636;22895;24005;48414;48414;26759...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test       82222;19216;75171;50501;50501;15982;84520;8839...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train      36719;15252;40813;4810;31936;4827;12718;2558;3...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test       59541;85735;87954;37438;18679;54496;13310;3939...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train     29129;16637;548;14119;14119;17919;18516;17940;...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test      83489;85338;38181;38181;57357;62904;31295;2350...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train     5334;1179;22246;22828;16003;3351;3351;6003;197...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train       7870;39823;80768;71944;45729;66498;66498;43678...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train       50289;59262;11505;43130;47558;47558;78689;8412...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train       2114;41490;50360;35353;37093;60033;29847;16682...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train       34282;51430;13359;66031;48507;69519;1024;29032...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train       32423;45577;46996;42339;15517;32177;57813;5781...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test        21148;15532;54164;35782;1145;11803;34502;68936...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test       66881;96606;56602;61412;61412;46641;62945;4348...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train      17317;17317;20759;31354;12163;24211;1247;11832...\n",
       "layout:nlp:random:talking-heads_large_batch_size_16_train                            0;1647;6880;3057;1093;4695;10156;9610;6880;469...\n",
       "layout:xla:default:bert_pretraining.4x4.fp16                                         19216;11470;9608;10850;18808;14062;10153;10158...\n",
       "layout:xla:default:inception_v3_batch_128_train                                      3480;3482;3489;3491;3490;4728;5815;3499;4644;3...\n",
       "layout:xla:default:mlperf_bert_batch_24_2x2                                          5929;5883;5424;3228;3167;5897;3178;5933;5937;5...\n",
       "layout:xla:default:resnet50.4x4.fp16                                                 3590;3584;740;3561;3577;3576;718;3531;1886;140...\n",
       "layout:xla:default:resnet_v1_50_official_batch_128_bf16                              8239;6898;6892;4229;4231;4239;5156;5155;8210;6...\n",
       "layout:xla:default:tf2_bert_pretrain_dynamic_batch_size                              6601;14117;14102;14102;14102;14102;14132;14132...\n",
       "layout:xla:default:unet_3d.4x4.bf16                                                  393;390;382;395;389;385;374;353;354;364;365;36...\n",
       "layout:xla:random:bert_pretraining.4x4.fp16                                          8963;8963;11203;3960;8573;3407;1190;1190;15603...\n",
       "layout:xla:random:inception_v3_batch_128_train                                       4099;125;2149;2955;2013;1984;187;4328;2216;189...\n",
       "layout:xla:random:mlperf_bert_batch_24_2x2                                           2590;136;5246;1209;2539;4200;2972;947;3686;192...\n",
       "layout:xla:random:resnet50.4x4.fp16                                                  2567;4835;2762;101;2383;3436;1091;5065;1690;34...\n",
       "layout:xla:random:resnet_v1_50_official_batch_128_bf16                               4678;1572;1734;6216;6177;6101;176;2663;1279;28...\n",
       "layout:xla:random:tf2_bert_pretrain_dynamic_batch_size                               13868;4902;4156;16118;7604;7604;12156;12156;13...\n",
       "layout:xla:random:unet_3d.4x4.bf16                                                   380;408;293;252;188;129;318;290;184;44;67;277;...\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_configs(df):\n",
    "    top = df.sort_values('prediction')\n",
    "    top = top['config_index'].values.tolist()\n",
    "    top = [str(i) for i in top]\n",
    "    return ';'.join(top)\n",
    "\n",
    "val_prediction = val_df.groupby('ID').apply(sort_configs)\n",
    "val_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb521f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3])).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layout_score_group(df):\n",
    "    score, _ = kendalltau(df['prediction'], df['target'])\n",
    "    return score\n",
    "\n",
    "val_df['subset'] = val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3]))\n",
    "for subset in val_df['subset'].unique():\n",
    "    mean = np.mean(val_df[val_df['subset'] == subset].groupby('ID').apply(compute_layout_score_group))\n",
    "    print(subset, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16107638",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.368, 0.137, 0.738, 0.346, 0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_score(candidate_order, layout_dict):\n",
    "    runtimes = layout_dict['config_runtime']\n",
    "    best_ranking = np.argsort(runtimes)\n",
    "    assert len(candidate_order) == len(runtimes)\n",
    "    score, _ = kendalltau(candidate_order, best_ranking)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4289a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_order = np.argsort(layout_dict['config_runtime'])\n",
    "plt.scatter(true_order, candidate_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d439d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layout_set = 'valid'\n",
    "true_orders = []\n",
    "layout_ids = []\n",
    "for dirpath, dirnames, filenames in os.walk('predict-ai-model-runtime/npz_all/npz/layout'):\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "    \n",
    "    if dirpath.split('/')[-1] != layout_set:\n",
    "        continue\n",
    "        \n",
    "    layout_id_prefix = ':'.join(dirpath.split('/')[-4:-1])\n",
    "    for filename in os.listdir(dirpath):\n",
    "        print(filename)\n",
    "        layout_id = layout_id_prefix+':'+filename[:-4]\n",
    "        layout_dict = dict(np.load(os.path.join(dirpath, filename)))\n",
    "        runtimes = layout_dict['config_runtime']\n",
    "        best_ranking = np.argsort(runtimes)\n",
    "        best_ranking = ';'.join([str(i) for i in best_ranking])\n",
    "        true_orders.append(best_ranking)\n",
    "        layout_ids.append(layout_id)\n",
    "        \n",
    "true_order_df = pd.DataFrame(\n",
    "    data=np.stack([layout_ids, true_orders], axis=-1),\n",
    "    columns=['ID', 'true_order']\n",
    ")\n",
    "true_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6177a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layout_id = true_order_df.sample()['ID'].values[0]\n",
    "layout_id = 'layout:xla:default:resnet50.4x4.fp16'\n",
    "true_order = [int(i) for i in true_order_df[true_order_df['ID'] == layout_id]['true_order'].values[0].split(';')]\n",
    "candidate_order = [int(i) for i in val_prediction[layout_id].split(';')]\n",
    "\n",
    "plt.scatter(true_order, candidate_order)\n",
    "plt.xlabel('true order')\n",
    "plt.ylabel('candidate order')\n",
    "plt.title(f'{layout_id}, len {len(true_order)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d506a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dict = dict(np.load('predict-ai-model-runtime/npz_all/npz/layout/nlp/default/valid/small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train.npz'))\n",
    "layout_dict['node_config_feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[val_df['ID'] == b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18339",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_layout['score'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434f868",
   "metadata": {},
   "source": [
    "## Inference over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac93738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>config_index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'layout:xla:default:3e7156ac468dfb75cf5c9615e...</td>\n",
       "      <td>1</td>\n",
       "      <td>314.054749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'layout:xla:default:3e7156ac468dfb75cf5c9615e...</td>\n",
       "      <td>20</td>\n",
       "      <td>314.347351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'layout:xla:default:3e7156ac468dfb75cf5c9615e...</td>\n",
       "      <td>4</td>\n",
       "      <td>313.785461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'layout:xla:default:3e7156ac468dfb75cf5c9615e...</td>\n",
       "      <td>0</td>\n",
       "      <td>313.313507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'layout:xla:default:3e7156ac468dfb75cf5c9615e...</td>\n",
       "      <td>23</td>\n",
       "      <td>314.489502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>b'layout:nlp:random:38524e2ff135ded55b5286407e...</td>\n",
       "      <td>938</td>\n",
       "      <td>118.774933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>b'layout:nlp:random:38524e2ff135ded55b5286407e...</td>\n",
       "      <td>976</td>\n",
       "      <td>121.391235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>b'layout:nlp:random:38524e2ff135ded55b5286407e...</td>\n",
       "      <td>984</td>\n",
       "      <td>117.100609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>b'layout:nlp:random:38524e2ff135ded55b5286407e...</td>\n",
       "      <td>979</td>\n",
       "      <td>108.957985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>b'layout:nlp:random:38524e2ff135ded55b5286407e...</td>\n",
       "      <td>998</td>\n",
       "      <td>116.349609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50002 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ID  config_index  \\\n",
       "0      b'layout:xla:default:3e7156ac468dfb75cf5c9615e...             1   \n",
       "1      b'layout:xla:default:3e7156ac468dfb75cf5c9615e...            20   \n",
       "2      b'layout:xla:default:3e7156ac468dfb75cf5c9615e...             4   \n",
       "3      b'layout:xla:default:3e7156ac468dfb75cf5c9615e...             0   \n",
       "4      b'layout:xla:default:3e7156ac468dfb75cf5c9615e...            23   \n",
       "...                                                  ...           ...   \n",
       "49997  b'layout:nlp:random:38524e2ff135ded55b5286407e...           938   \n",
       "49998  b'layout:nlp:random:38524e2ff135ded55b5286407e...           976   \n",
       "49999  b'layout:nlp:random:38524e2ff135ded55b5286407e...           984   \n",
       "50000  b'layout:nlp:random:38524e2ff135ded55b5286407e...           979   \n",
       "50001  b'layout:nlp:random:38524e2ff135ded55b5286407e...           998   \n",
       "\n",
       "       prediction  \n",
       "0      314.054749  \n",
       "1      314.347351  \n",
       "2      313.785461  \n",
       "3      313.313507  \n",
       "4      314.489502  \n",
       "...           ...  \n",
       "49997  118.774933  \n",
       "49998  121.391235  \n",
       "49999  117.100609  \n",
       "50000  108.957985  \n",
       "50001  116.349609  \n",
       "\n",
       "[50002 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = mlp.predict_over_dataset(dataset.test_data, return_labels=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b21ba9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "layout:nlp:default:016ac66a44a906a695afd2228509046a    302;192;714;380;797;513;934;416;893;31;767;49;...\n",
       "layout:nlp:default:171b0513d8874a427ccfa46d136fbadc    154;768;653;951;868;242;290;485;618;459;84;270...\n",
       "layout:nlp:default:23559853d9702baaaacbb0c83fd32266    839;785;993;284;728;852;660;215;330;400;305;80...\n",
       "layout:nlp:default:29886a50d55cfe77a9497bc906c76ce9    498;493;886;96;727;295;204;34;596;286;974;32;1...\n",
       "layout:nlp:default:32531d07a084b319dce484f53a4cf3fc    327;359;313;655;621;226;892;397;512;482;188;84...\n",
       "layout:nlp:default:38524e2ff135ded55b5286407e7af6b7    532;930;783;242;762;966;408;522;754;407;29;902...\n",
       "layout:nlp:default:3a0c5517a87df8d82fd637b83298a3ba    518;839;926;210;156;939;202;492;580;418;180;83...\n",
       "layout:nlp:default:492c7a94d559aa4a88769142d2a68362    25;684;486;24;560;827;426;755;442;752;796;168;...\n",
       "layout:nlp:default:58cc2e418c3a8a19b871e15964b534ad    834;771;161;954;265;885;314;868;44;49;549;131;...\n",
       "layout:nlp:default:60880ed76de53f4d7a1b960b24f20f7d    814;470;718;622;35;574;739;670;671;293;881;387...\n",
       "layout:nlp:default:6c1101f6231f4d1722c3b9f6d1e25026    270;691;582;447;595;911;533;919;905;183;471;13...\n",
       "layout:nlp:default:7105451001e119f65b66570d170b94a8    501;588;851;996;194;114;945;282;556;709;789;27...\n",
       "layout:nlp:default:71b79ca6db513e7979c3702c595150c2    922;928;522;577;897;291;955;894;381;536;899;28...\n",
       "layout:nlp:default:7f6284ebe027b1e9a3850fc703858a59    626;984;723;232;939;405;498;228;434;667;523;49...\n",
       "layout:nlp:default:b2fdde3b72980907578648774101543e    485;499;142;745;962;748;323;533;52;620;288;993...\n",
       "layout:nlp:default:d15316c12eefdef1ba549eb433797f77    997;259;198;193;400;421;280;373;159;953;95;221...\n",
       "layout:nlp:default:f6c146fc5cf10be4f3accbaca9897311    621;972;814;147;884;447;950;179;299;882;478;73...\n",
       "layout:nlp:random:016ac66a44a906a695afd2228509046a     56;533;540;273;781;624;848;813;924;900;644;416...\n",
       "layout:nlp:random:171b0513d8874a427ccfa46d136fbadc     75;902;669;260;140;629;391;940;565;434;44;710;...\n",
       "layout:nlp:random:23559853d9702baaaacbb0c83fd32266     924;703;214;204;344;436;587;126;215;51;377;624...\n",
       "layout:nlp:random:29886a50d55cfe77a9497bc906c76ce9     704;617;689;31;962;219;554;488;392;84;857;4;65...\n",
       "layout:nlp:random:32531d07a084b319dce484f53a4cf3fc     691;26;853;22;907;189;325;464;748;939;977;536;...\n",
       "layout:nlp:random:38524e2ff135ded55b5286407e7af6b7     874;482;162;604;693;979;669;934;411;926;268;92...\n",
       "layout:nlp:random:3a0c5517a87df8d82fd637b83298a3ba     405;615;859;242;654;75;897;765;49;559;729;29;1...\n",
       "layout:nlp:random:492c7a94d559aa4a88769142d2a68362     941;812;687;982;543;802;827;656;720;320;658;33...\n",
       "layout:nlp:random:58cc2e418c3a8a19b871e15964b534ad     489;737;34;667;399;772;239;463;526;669;350;858...\n",
       "layout:nlp:random:60880ed76de53f4d7a1b960b24f20f7d     320;645;356;143;270;457;703;742;836;219;790;62...\n",
       "layout:nlp:random:6c1101f6231f4d1722c3b9f6d1e25026     222;766;82;138;166;142;933;24;6;649;865;444;29...\n",
       "layout:nlp:random:7105451001e119f65b66570d170b94a8     930;340;46;392;966;955;2;617;376;530;540;654;3...\n",
       "layout:nlp:random:71b79ca6db513e7979c3702c595150c2     314;924;310;275;96;527;879;280;803;389;506;418...\n",
       "layout:nlp:random:7f6284ebe027b1e9a3850fc703858a59     499;901;988;12;880;359;41;719;422;179;747;811;...\n",
       "layout:nlp:random:b2fdde3b72980907578648774101543e     339;10;765;700;2;937;256;333;679;564;379;844;8...\n",
       "layout:nlp:random:d15316c12eefdef1ba549eb433797f77     717;912;901;140;979;825;425;131;58;511;925;833...\n",
       "layout:nlp:random:f6c146fc5cf10be4f3accbaca9897311     634;846;726;487;749;158;882;352;667;188;827;33...\n",
       "layout:xla:default:05ae41e26dd3c4c06390371a0423233c    739;438;75;77;836;496;893;194;327;563;831;392;...\n",
       "layout:xla:default:3e7156ac468dfb75cf5c9615e1e5887d    137;621;514;816;536;228;644;509;79;888;706;36;...\n",
       "layout:xla:default:5335ed13823b0a518ee3c79ba4425f34    970;94;528;999;172;936;28;902;74;312;67;864;60...\n",
       "layout:xla:default:937ee0eb0d5d6151b7b8252933b5c1c9    304;921;92;695;49;232;782;653;284;870;73;821;7...\n",
       "layout:xla:default:cd708819d3f5103afd6460b15e74eaf3    430;604;503;4;463;455;920;739;601;401;911;953;...\n",
       "layout:xla:default:db59a991b7c607634f13570d52ce885f    456;133;542;2;276;960;197;861;523;332;798;67;3...\n",
       "layout:xla:default:e8a3a1401b5e79f66d7037e424f3b6df    579;994;252;79;974;976;441;183;908;110;850;412...\n",
       "layout:xla:default:fbaa8bb6a1aed9988281085c91065c05    971;217;432;779;500;343;323;254;320;68;800;607...\n",
       "layout:xla:random:05ae41e26dd3c4c06390371a0423233c     874;17;89;532;498;267;779;259;849;585;709;232;...\n",
       "layout:xla:random:3e7156ac468dfb75cf5c9615e1e5887d     271;856;385;992;705;904;802;860;690;29;640;75;...\n",
       "layout:xla:random:5335ed13823b0a518ee3c79ba4425f34     916;196;95;687;447;30;583;471;725;21;137;496;3...\n",
       "layout:xla:random:937ee0eb0d5d6151b7b8252933b5c1c9     564;577;438;568;901;417;132;935;18;641;368;674...\n",
       "layout:xla:random:cd708819d3f5103afd6460b15e74eaf3     198;243;247;151;519;326;479;372;278;963;708;52...\n",
       "layout:xla:random:db59a991b7c607634f13570d52ce885f     578;322;202;15;189;105;144;371;188;104;501;766...\n",
       "layout:xla:random:e8a3a1401b5e79f66d7037e424f3b6df     801;97;991;280;96;182;41;281;865;109;136;227;7...\n",
       "layout:xla:random:fbaa8bb6a1aed9988281085c91065c05     708;380;214;68;108;93;739;864;780;990;753;190;...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = test_df.groupby('ID').apply(sort_configs)\n",
    "test_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d09fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_prediction, columns=['TopConfigs']).to_csv('layout_none_test_prediction_10_08_18_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(mlp.dense_layer_1.kernel.numpy().flatten()), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f73f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
