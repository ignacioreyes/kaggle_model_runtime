{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db851207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 01:38:53.603130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 01:38:54.283283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dataset import LayoutDataset\n",
    "from models import LayoutMLP\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dda8a3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 01:38:55.084774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.111629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.111888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.114001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.114209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.114395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.629857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.630084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.630271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-06 01:38:55.630421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dataset = LayoutDataset(\n",
    "    batch_size, train_sample_fraction=1.0, \n",
    "    subset=None, build_tfrecords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d32ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = LayoutMLP(batch_size, learning_rate=1e-3, mask_max_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a121fb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 01:38:57.974752: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 723968000 exceeds 10% of free system memory.\n",
      "2023-10-06 01:38:58.679444: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 723968000 exceeds 10% of free system memory.\n",
      "2023-10-06 01:39:02.169760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-06 01:39:02.172856: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9160cad770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-06 01:39:02.172869: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2023-10-06 01:39:02.175813: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-06 01:39:02.317291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-06 01:39:02.402760: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 training loss 1.4531025 lr 0.00100\n",
      "iteration 200 training loss 1.2901208 lr 0.00100\n",
      "iteration 300 training loss 1.1047039 lr 0.00100\n",
      "iteration 400 training loss 1.0841609 lr 0.00100\n",
      "iteration 500 training loss 1.0609791 lr 0.00100\n",
      "iteration 600 training loss 0.99343264 lr 0.00100\n",
      "iteration 700 training loss 1.0447245 lr 0.00100\n",
      "iteration 800 training loss 0.94314337 lr 0.00100\n",
      "iteration 900 training loss 0.947058 lr 0.00100\n",
      "iteration 1000 training loss 0.86408573 lr 0.00100\n",
      "iteration 1100 training loss 0.73883855 lr 0.00100\n",
      "iteration 1200 training loss 0.7116703 lr 0.00100\n",
      "iteration 1300 training loss 0.8798144 lr 0.00100\n",
      "iteration 1400 training loss 0.71304625 lr 0.00100\n",
      "iteration 1500 training loss 0.7703396 lr 0.00100\n",
      "iteration 1600 training loss 1.4395368 lr 0.00100\n",
      "iteration 1700 training loss 1.2733127 lr 0.00100\n",
      "iteration 1800 training loss 1.2436625 lr 0.00100\n",
      "iteration 1900 training loss 1.0397867 lr 0.00100\n",
      "iteration 2000 training loss 1.0790567 lr 0.00100\n",
      "iteration 2100 training loss 1.1943959 lr 0.00100\n",
      "iteration 2200 training loss 0.97691464 lr 0.00100\n",
      "iteration 2300 training loss 1.1377017 lr 0.00100\n",
      "iteration 2400 training loss 1.0068153 lr 0.00100\n",
      "iteration 2500 training loss 1.0970591 lr 0.00100\n",
      "iteration 2600 training loss 0.9466096 lr 0.00100\n",
      "iteration 2700 training loss 0.898574 lr 0.00100\n",
      "iteration 2800 training loss 0.7653345 lr 0.00100\n",
      "iteration 2900 training loss 0.9761528 lr 0.00100\n",
      "iteration 3000 training loss 1.0653535 lr 0.00100\n",
      "iteration 3100 training loss 1.0526398 lr 0.00100\n",
      "iteration 3200 training loss 1.3754503 lr 0.00100\n",
      "iteration 3300 training loss 1.0278685 lr 0.00100\n",
      "iteration 3400 training loss 1.20577 lr 0.00100\n",
      "iteration 3500 training loss 1.0461124 lr 0.00100\n",
      "iteration 3600 training loss 1.0840456 lr 0.00100\n",
      "iteration 3700 training loss 0.89044774 lr 0.00100\n",
      "iteration 3800 training loss 1.1725116 lr 0.00100\n",
      "iteration 3900 training loss 1.1146538 lr 0.00100\n",
      "iteration 4000 training loss 1.0078735 lr 0.00100\n",
      "iteration 4100 training loss 1.1038221 lr 0.00100\n",
      "iteration 4200 training loss 1.3103812 lr 0.00100\n",
      "iteration 4300 training loss 1.2089003 lr 0.00100\n",
      "iteration 4400 training loss 0.9865694 lr 0.00100\n",
      "iteration 4500 training loss 1.1258883 lr 0.00100\n",
      "iteration 4600 training loss 0.88333386 lr 0.00100\n",
      "iteration 4700 training loss 1.2586539 lr 0.00100\n",
      "iteration 4800 training loss 0.72691226 lr 0.00100\n",
      "iteration 4900 training loss 0.6435726 lr 0.00100\n",
      "iteration 5000 training loss 0.83242124 lr 0.00100\n",
      "iteration 5100 training loss 0.83849025 lr 0.00100\n",
      "iteration 5200 training loss 0.54725343 lr 0.00100\n",
      "iteration 5300 training loss 0.68078023 lr 0.00100\n",
      "iteration 5400 training loss 0.75234544 lr 0.00100\n",
      "iteration 5500 training loss 0.56629527 lr 0.00100\n",
      "iteration 5600 training loss 0.6406052 lr 0.00100\n",
      "iteration 5700 training loss 0.46535066 lr 0.00100\n",
      "iteration 5800 training loss 0.7043231 lr 0.00100\n",
      "iteration 5900 training loss 0.7870504 lr 0.00100\n",
      "iteration 6000 training loss 0.8685474 lr 0.00100\n",
      "iteration 6100 training loss 0.7261466 lr 0.00100\n",
      "iteration 6200 training loss 0.48988324 lr 0.00100\n",
      "iteration 6300 training loss 0.7562175 lr 0.00100\n",
      "iteration 6400 training loss 0.56588215 lr 0.00100\n",
      "iteration 6500 training loss 0.6366847 lr 0.00100\n",
      "iteration 6600 training loss 0.5028677 lr 0.00100\n",
      "iteration 6700 training loss 0.585749 lr 0.00100\n",
      "iteration 6800 training loss 0.5493755 lr 0.00100\n",
      "iteration 6900 training loss 0.8168545 lr 0.00100\n",
      "iteration 7000 training loss 0.78362226 lr 0.00100\n",
      "iteration 7100 training loss 0.81247914 lr 0.00100\n",
      "iteration 7200 training loss 0.7275112 lr 0.00100\n",
      "iteration 7300 training loss 0.49168923 lr 0.00100\n",
      "iteration 7400 training loss 0.8001182 lr 0.00100\n",
      "iteration 7500 training loss 0.81573373 lr 0.00100\n",
      "iteration 7600 training loss 0.77584577 lr 0.00100\n",
      "iteration 7700 training loss 0.77066016 lr 0.00100\n",
      "iteration 7800 training loss 0.86104715 lr 0.00100\n",
      "iteration 7900 training loss 1.0852561 lr 0.00100\n",
      "iteration 8000 training loss 1.1006572 lr 0.00100\n",
      "iteration 8100 training loss 0.77362186 lr 0.00100\n",
      "iteration 8200 training loss 0.86657184 lr 0.00100\n",
      "iteration 8300 training loss 0.79468614 lr 0.00100\n",
      "iteration 8400 training loss 0.8661461 lr 0.00100\n",
      "iteration 8500 training loss 0.8277265 lr 0.00100\n",
      "iteration 8600 training loss 0.8789557 lr 0.00100\n",
      "iteration 8700 training loss 0.78155833 lr 0.00100\n",
      "iteration 8800 training loss 0.70114726 lr 0.00100\n",
      "iteration 8900 training loss 0.94122905 lr 0.00100\n",
      "iteration 9000 training loss 0.7642241 lr 0.00100\n",
      "iteration 9100 training loss 0.9457746 lr 0.00100\n",
      "iteration 9200 training loss 0.8686039 lr 0.00100\n",
      "iteration 9300 training loss 0.80931157 lr 0.00100\n",
      "iteration 9400 training loss 1.2181624 lr 0.00100\n",
      "iteration 9500 training loss 1.004073 lr 0.00100\n",
      "iteration 9600 training loss 0.98327017 lr 0.00100\n",
      "iteration 9700 training loss 1.132457 lr 0.00100\n",
      "iteration 9800 training loss 1.1211588 lr 0.00100\n",
      "iteration 9900 training loss 0.9048752 lr 0.00100\n",
      "iteration 10000 training loss 0.9743615 lr 0.00100\n",
      "layout:xla:random 0.18896470887371788\n",
      "layout:nlp:random 0.6117909372333705\n",
      "layout:nlp:default 0.34020007460556323\n",
      "layout:xla:default -0.019278253560244006\n",
      "epoch 0, it 10000 validation loss -0.280\n",
      "iteration 10100 training loss 0.99024814 lr 0.00100\n",
      "iteration 10200 training loss 0.9847151 lr 0.00100\n",
      "iteration 10300 training loss 0.9239308 lr 0.00100\n",
      "iteration 10400 training loss 0.9424817 lr 0.00100\n",
      "iteration 10500 training loss 0.7627624 lr 0.00100\n",
      "iteration 10600 training loss 0.93735915 lr 0.00100\n",
      "iteration 10700 training loss 0.89242005 lr 0.00100\n",
      "iteration 10800 training loss 0.7646475 lr 0.00100\n",
      "iteration 10900 training loss 0.6941228 lr 0.00100\n",
      "iteration 11000 training loss 0.8844068 lr 0.00100\n",
      "iteration 11100 training loss 0.91193295 lr 0.00100\n",
      "iteration 11200 training loss 0.87268835 lr 0.00100\n",
      "iteration 11300 training loss 0.7479001 lr 0.00100\n",
      "iteration 11400 training loss 0.84876716 lr 0.00100\n",
      "iteration 11500 training loss 0.716865 lr 0.00100\n",
      "iteration 11600 training loss 0.9331778 lr 0.00100\n",
      "iteration 11700 training loss 0.6143381 lr 0.00100\n",
      "iteration 11800 training loss 0.8460149 lr 0.00100\n",
      "iteration 11900 training loss 0.71982455 lr 0.00100\n",
      "iteration 12000 training loss 0.7269516 lr 0.00100\n",
      "iteration 12100 training loss 0.6539893 lr 0.00100\n",
      "iteration 12200 training loss 0.57553214 lr 0.00100\n",
      "iteration 12300 training loss 0.9064564 lr 0.00100\n",
      "iteration 12400 training loss 0.8176386 lr 0.00100\n",
      "iteration 12500 training loss 0.53193873 lr 0.00100\n",
      "iteration 12600 training loss 0.7419619 lr 0.00100\n",
      "iteration 12700 training loss 0.85212606 lr 0.00100\n",
      "iteration 12800 training loss 0.91149974 lr 0.00100\n",
      "iteration 12900 training loss 0.9743911 lr 0.00100\n",
      "iteration 13000 training loss 0.77782375 lr 0.00100\n",
      "iteration 13100 training loss 0.72427076 lr 0.00100\n",
      "iteration 13200 training loss 0.7014751 lr 0.00100\n",
      "iteration 13300 training loss 0.53759015 lr 0.00100\n",
      "iteration 13400 training loss 0.8367632 lr 0.00100\n",
      "iteration 13500 training loss 0.9532279 lr 0.00100\n",
      "iteration 13600 training loss 0.8727911 lr 0.00100\n",
      "iteration 13700 training loss 0.8448394 lr 0.00100\n",
      "iteration 13800 training loss 0.48333862 lr 0.00100\n",
      "iteration 13900 training loss 0.6156802 lr 0.00100\n",
      "iteration 14000 training loss 0.65383136 lr 0.00100\n",
      "iteration 14100 training loss 0.9531196 lr 0.00100\n",
      "iteration 14200 training loss 1.0685592 lr 0.00100\n",
      "iteration 14300 training loss 0.82602316 lr 0.00100\n",
      "iteration 14400 training loss 1.023052 lr 0.00100\n",
      "iteration 14500 training loss 0.8861014 lr 0.00100\n",
      "iteration 14600 training loss 0.8712303 lr 0.00100\n",
      "iteration 14700 training loss 0.985426 lr 0.00100\n",
      "iteration 14800 training loss 0.9046236 lr 0.00100\n",
      "iteration 14900 training loss 0.78884524 lr 0.00100\n",
      "iteration 15000 training loss 0.94669616 lr 0.00100\n",
      "iteration 15100 training loss 0.86569315 lr 0.00100\n",
      "iteration 15200 training loss 1.1359578 lr 0.00100\n",
      "iteration 15300 training loss 1.0335388 lr 0.00100\n",
      "iteration 15400 training loss 1.0803486 lr 0.00100\n",
      "iteration 15500 training loss 0.8408967 lr 0.00100\n",
      "iteration 15600 training loss 0.7869156 lr 0.00100\n",
      "iteration 15700 training loss 1.0261334 lr 0.00100\n",
      "iteration 15800 training loss 0.8389548 lr 0.00100\n",
      "iteration 15900 training loss 0.899861 lr 0.00100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16000 training loss 0.76253825 lr 0.00100\n",
      "iteration 16100 training loss 0.9160571 lr 0.00100\n",
      "iteration 16200 training loss 0.80206907 lr 0.00100\n",
      "iteration 16300 training loss 0.54727954 lr 0.00100\n",
      "iteration 16400 training loss 0.968549 lr 0.00100\n",
      "iteration 16500 training loss 0.89503986 lr 0.00100\n",
      "iteration 16600 training loss 0.8112401 lr 0.00100\n",
      "iteration 16700 training loss 0.70975786 lr 0.00100\n",
      "iteration 16800 training loss 0.8818317 lr 0.00100\n",
      "iteration 16900 training loss 0.45959678 lr 0.00100\n",
      "iteration 17000 training loss 0.81727314 lr 0.00100\n",
      "iteration 17100 training loss 0.8368105 lr 0.00100\n",
      "iteration 17200 training loss 0.68429255 lr 0.00100\n",
      "iteration 17300 training loss 0.7439478 lr 0.00100\n",
      "iteration 17400 training loss 0.635169 lr 0.00100\n",
      "iteration 17500 training loss 0.74236447 lr 0.00100\n",
      "iteration 17600 training loss 0.6351934 lr 0.00100\n",
      "iteration 17700 training loss 0.744358 lr 0.00100\n",
      "iteration 17800 training loss 0.67498446 lr 0.00100\n",
      "iteration 17900 training loss 0.6824944 lr 0.00100\n",
      "iteration 18000 training loss 0.749759 lr 0.00100\n",
      "iteration 18100 training loss 0.7558241 lr 0.00100\n",
      "iteration 18200 training loss 0.7449559 lr 0.00100\n",
      "iteration 18300 training loss 0.7692796 lr 0.00100\n",
      "iteration 18400 training loss 0.8030846 lr 0.00100\n",
      "iteration 18500 training loss 0.87975615 lr 0.00100\n",
      "iteration 18600 training loss 0.9739155 lr 0.00100\n",
      "iteration 18700 training loss 0.7904882 lr 0.00100\n",
      "iteration 18800 training loss 1.0268873 lr 0.00100\n",
      "iteration 18900 training loss 0.88787293 lr 0.00100\n",
      "iteration 19000 training loss 0.8574101 lr 0.00100\n",
      "iteration 19100 training loss 0.8890461 lr 0.00100\n",
      "iteration 19200 training loss 0.7752843 lr 0.00100\n",
      "iteration 19300 training loss 1.1953053 lr 0.00100\n",
      "iteration 19400 training loss 0.77438706 lr 0.00100\n",
      "iteration 19500 training loss 1.073106 lr 0.00100\n",
      "iteration 19600 training loss 0.722626 lr 0.00100\n",
      "iteration 19700 training loss 0.82415986 lr 0.00100\n",
      "iteration 19800 training loss 0.8564969 lr 0.00100\n",
      "iteration 19900 training loss 0.82542557 lr 0.00100\n",
      "iteration 20000 training loss 0.92172647 lr 0.00100\n",
      "layout:xla:default 0.07029514052471744\n",
      "layout:nlp:random 0.7133915411334794\n",
      "layout:xla:random 0.22845869403739275\n",
      "layout:nlp:default 0.3632726912187506\n",
      "epoch 0, it 20000 validation loss -0.344\n",
      "iteration 20100 training loss 0.7724283 lr 0.00090\n",
      "iteration 20200 training loss 0.7890408 lr 0.00090\n",
      "iteration 20300 training loss 0.7984201 lr 0.00090\n",
      "iteration 20400 training loss 0.7245217 lr 0.00090\n",
      "iteration 20500 training loss 0.73003715 lr 0.00090\n",
      "iteration 20600 training loss 0.8711062 lr 0.00090\n",
      "iteration 20700 training loss 1.1195649 lr 0.00090\n",
      "iteration 20800 training loss 0.79955417 lr 0.00090\n",
      "iteration 20900 training loss 0.790394 lr 0.00090\n",
      "iteration 21000 training loss 0.8293733 lr 0.00090\n",
      "iteration 21100 training loss 0.81701034 lr 0.00090\n",
      "iteration 21200 training loss 0.89397365 lr 0.00090\n",
      "iteration 21300 training loss 0.61323833 lr 0.00090\n",
      "iteration 21400 training loss 0.5599951 lr 0.00090\n",
      "iteration 21500 training loss 0.6260153 lr 0.00090\n",
      "iteration 21600 training loss 0.71434575 lr 0.00090\n",
      "iteration 21700 training loss 0.61720103 lr 0.00090\n",
      "iteration 21800 training loss 0.67356294 lr 0.00090\n",
      "iteration 21900 training loss 0.7730661 lr 0.00090\n",
      "iteration 22000 training loss 1.1046922 lr 0.00090\n",
      "iteration 22100 training loss 0.7944542 lr 0.00090\n",
      "iteration 22200 training loss 0.90887576 lr 0.00090\n",
      "iteration 22300 training loss 1.0815045 lr 0.00090\n",
      "iteration 22400 training loss 0.81878984 lr 0.00090\n",
      "iteration 22500 training loss 1.1377066 lr 0.00090\n",
      "iteration 22600 training loss 0.80425984 lr 0.00090\n",
      "iteration 22700 training loss 1.0550961 lr 0.00090\n",
      "iteration 22800 training loss 0.7697417 lr 0.00090\n",
      "iteration 22900 training loss 0.6964673 lr 0.00090\n",
      "iteration 23000 training loss 0.8889915 lr 0.00090\n",
      "iteration 23100 training loss 0.7390772 lr 0.00090\n",
      "iteration 23200 training loss 0.7959037 lr 0.00090\n",
      "iteration 23300 training loss 0.63171476 lr 0.00090\n",
      "iteration 23400 training loss 0.8080887 lr 0.00090\n",
      "iteration 23500 training loss 0.9515793 lr 0.00090\n",
      "iteration 23600 training loss 0.8505268 lr 0.00090\n",
      "iteration 23700 training loss 0.8891653 lr 0.00090\n",
      "iteration 23800 training loss 0.7713973 lr 0.00090\n",
      "iteration 23900 training loss 0.8094856 lr 0.00090\n",
      "iteration 24000 training loss 0.5259129 lr 0.00090\n",
      "iteration 24100 training loss 0.85493654 lr 0.00090\n",
      "iteration 24200 training loss 0.43619922 lr 0.00090\n",
      "iteration 24300 training loss 0.69854486 lr 0.00090\n",
      "iteration 24400 training loss 0.7378564 lr 0.00090\n",
      "iteration 24500 training loss 0.6995671 lr 0.00090\n",
      "iteration 24600 training loss 0.55746824 lr 0.00090\n",
      "iteration 24700 training loss 0.61837924 lr 0.00090\n",
      "iteration 24800 training loss 0.48745197 lr 0.00090\n",
      "iteration 24900 training loss 0.5696352 lr 0.00090\n",
      "iteration 25000 training loss 0.8306586 lr 0.00090\n",
      "iteration 25100 training loss 0.75488144 lr 0.00090\n",
      "iteration 25200 training loss 0.8100362 lr 0.00090\n",
      "iteration 25300 training loss 0.87967885 lr 0.00090\n",
      "iteration 25400 training loss 0.71635866 lr 0.00090\n",
      "iteration 25500 training loss 0.6511599 lr 0.00090\n",
      "iteration 25600 training loss 0.7037337 lr 0.00090\n",
      "iteration 25700 training loss 0.988756 lr 0.00090\n",
      "iteration 25800 training loss 0.677993 lr 0.00090\n",
      "iteration 25900 training loss 0.75226295 lr 0.00090\n",
      "iteration 26000 training loss 0.85099554 lr 0.00090\n",
      "iteration 26100 training loss 0.63353133 lr 0.00090\n",
      "iteration 26200 training loss 0.84800905 lr 0.00090\n",
      "iteration 26300 training loss 0.84485614 lr 0.00090\n",
      "iteration 26400 training loss 1.0214577 lr 0.00090\n",
      "iteration 26500 training loss 0.83425194 lr 0.00090\n",
      "iteration 26600 training loss 0.7582162 lr 0.00090\n",
      "iteration 26700 training loss 0.7729611 lr 0.00090\n",
      "iteration 26800 training loss 0.724222 lr 0.00090\n",
      "iteration 26900 training loss 0.7428795 lr 0.00090\n",
      "iteration 27000 training loss 0.68078595 lr 0.00090\n",
      "iteration 27100 training loss 0.74438125 lr 0.00090\n",
      "iteration 27200 training loss 0.7320391 lr 0.00090\n",
      "iteration 27300 training loss 0.60429907 lr 0.00090\n",
      "iteration 27400 training loss 0.77957195 lr 0.00090\n",
      "iteration 27500 training loss 0.6839731 lr 0.00090\n",
      "iteration 27600 training loss 0.83184224 lr 0.00090\n",
      "iteration 27700 training loss 0.7117485 lr 0.00090\n",
      "iteration 27800 training loss 0.86361945 lr 0.00090\n",
      "iteration 27900 training loss 1.0495936 lr 0.00090\n",
      "iteration 28000 training loss 0.8355031 lr 0.00090\n",
      "iteration 28100 training loss 0.9066338 lr 0.00090\n",
      "iteration 28200 training loss 1.084283 lr 0.00090\n",
      "iteration 28300 training loss 0.8550744 lr 0.00090\n",
      "iteration 28400 training loss 0.9646025 lr 0.00090\n",
      "iteration 28500 training loss 0.93494177 lr 0.00090\n",
      "iteration 28600 training loss 1.0005739 lr 0.00090\n",
      "iteration 28700 training loss 0.7990281 lr 0.00090\n",
      "iteration 28800 training loss 0.69291437 lr 0.00090\n",
      "iteration 28900 training loss 0.84381735 lr 0.00090\n",
      "iteration 29000 training loss 1.0272607 lr 0.00090\n",
      "iteration 29100 training loss 0.7981337 lr 0.00090\n",
      "iteration 29200 training loss 0.7544296 lr 0.00090\n",
      "iteration 29300 training loss 0.7690138 lr 0.00090\n",
      "iteration 29400 training loss 0.8654223 lr 0.00090\n",
      "iteration 29500 training loss 0.69198316 lr 0.00090\n",
      "iteration 29600 training loss 0.5580255 lr 0.00090\n",
      "iteration 29700 training loss 1.2021979 lr 0.00090\n",
      "iteration 29800 training loss 0.88809854 lr 0.00090\n",
      "iteration 29900 training loss 0.86285657 lr 0.00090\n",
      "iteration 30000 training loss 0.72147375 lr 0.00090\n",
      "layout:xla:random 0.20762167863969924\n",
      "layout:xla:default 0.01714100909362548\n",
      "layout:nlp:default 0.300966916153505\n",
      "layout:nlp:random 0.6355492690130802\n",
      "epoch 0, it 30000 validation loss -0.290\n",
      "iteration 30100 training loss 0.8392838 lr 0.00090\n",
      "iteration 30200 training loss 0.8292776 lr 0.00090\n",
      "iteration 30300 training loss 0.7805772 lr 0.00090\n",
      "iteration 30400 training loss 0.61598897 lr 0.00090\n",
      "iteration 30500 training loss 0.5870725 lr 0.00090\n",
      "iteration 30600 training loss 0.48047042 lr 0.00090\n",
      "iteration 30700 training loss 0.63497096 lr 0.00090\n",
      "iteration 30800 training loss 0.6463209 lr 0.00090\n",
      "iteration 30900 training loss 0.63622665 lr 0.00090\n",
      "iteration 31000 training loss 0.69033754 lr 0.00090\n",
      "iteration 31100 training loss 0.4655664 lr 0.00090\n",
      "iteration 31200 training loss 0.6227988 lr 0.00090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 31300 training loss 0.84661674 lr 0.00090\n",
      "iteration 31400 training loss 0.70281273 lr 0.00090\n",
      "iteration 31500 training loss 0.7206976 lr 0.00090\n",
      "iteration 31600 training loss 0.52670926 lr 0.00090\n",
      "iteration 31700 training loss 0.8843896 lr 0.00090\n",
      "iteration 31800 training loss 0.771713 lr 0.00090\n",
      "iteration 31900 training loss 0.6830624 lr 0.00090\n",
      "iteration 32000 training loss 0.7075871 lr 0.00090\n",
      "iteration 32100 training loss 0.6297584 lr 0.00090\n",
      "iteration 32200 training loss 1.0514492 lr 0.00090\n",
      "iteration 32300 training loss 1.2023894 lr 0.00090\n",
      "iteration 32400 training loss 1.027092 lr 0.00090\n",
      "iteration 32500 training loss 0.88420457 lr 0.00090\n",
      "iteration 32600 training loss 0.6198519 lr 0.00090\n",
      "iteration 32700 training loss 0.81440014 lr 0.00090\n",
      "iteration 32800 training loss 0.6529772 lr 0.00090\n",
      "iteration 32900 training loss 0.9213074 lr 0.00090\n",
      "iteration 33000 training loss 0.8185416 lr 0.00090\n",
      "iteration 33100 training loss 0.9321726 lr 0.00090\n",
      "iteration 33200 training loss 0.52450323 lr 0.00090\n",
      "iteration 33300 training loss 0.84183276 lr 0.00090\n",
      "iteration 33400 training loss 0.63945055 lr 0.00090\n",
      "iteration 33500 training loss 0.9113644 lr 0.00090\n",
      "iteration 33600 training loss 0.6914227 lr 0.00090\n",
      "iteration 33700 training loss 0.67147684 lr 0.00090\n",
      "iteration 33800 training loss 0.6226105 lr 0.00090\n",
      "iteration 33900 training loss 0.737729 lr 0.00090\n",
      "iteration 34000 training loss 0.7234573 lr 0.00090\n",
      "iteration 34100 training loss 0.5622124 lr 0.00090\n",
      "iteration 34200 training loss 0.74318933 lr 0.00090\n",
      "iteration 34300 training loss 0.43125427 lr 0.00090\n",
      "iteration 34400 training loss 0.5759427 lr 0.00090\n",
      "iteration 34500 training loss 0.5216533 lr 0.00090\n",
      "iteration 34600 training loss 0.48833233 lr 0.00090\n",
      "iteration 34700 training loss 0.6666839 lr 0.00090\n",
      "iteration 34800 training loss 0.554791 lr 0.00090\n",
      "iteration 34900 training loss 0.565936 lr 0.00090\n",
      "iteration 35000 training loss 0.7359983 lr 0.00090\n",
      "iteration 35100 training loss 0.6956762 lr 0.00090\n",
      "iteration 35200 training loss 0.7024084 lr 0.00090\n",
      "iteration 35300 training loss 0.6692773 lr 0.00090\n",
      "iteration 35400 training loss 0.6334485 lr 0.00090\n",
      "iteration 35500 training loss 0.9401922 lr 0.00090\n",
      "iteration 35600 training loss 0.8373196 lr 0.00090\n",
      "iteration 35700 training loss 0.4965852 lr 0.00090\n",
      "iteration 35800 training loss 0.6583938 lr 0.00090\n",
      "iteration 35900 training loss 0.6385786 lr 0.00090\n",
      "iteration 36000 training loss 0.8313114 lr 0.00090\n",
      "iteration 36100 training loss 0.7269414 lr 0.00090\n",
      "iteration 36200 training loss 0.7537384 lr 0.00090\n",
      "iteration 36300 training loss 0.72380364 lr 0.00090\n",
      "iteration 36400 training loss 0.71135515 lr 0.00090\n",
      "iteration 36500 training loss 0.5640824 lr 0.00090\n",
      "iteration 36600 training loss 0.5889334 lr 0.00090\n",
      "iteration 36700 training loss 0.7694477 lr 0.00090\n",
      "iteration 36800 training loss 0.6743384 lr 0.00090\n",
      "iteration 36900 training loss 0.8153507 lr 0.00090\n",
      "iteration 37000 training loss 0.6676134 lr 0.00090\n",
      "iteration 37100 training loss 0.77015996 lr 0.00090\n",
      "iteration 37200 training loss 0.78890467 lr 0.00090\n",
      "iteration 37300 training loss 0.76755285 lr 0.00090\n",
      "iteration 37400 training loss 0.7200717 lr 0.00090\n",
      "iteration 37500 training loss 0.89187384 lr 0.00090\n",
      "iteration 37600 training loss 0.65890676 lr 0.00090\n",
      "iteration 37700 training loss 0.7765181 lr 0.00090\n",
      "iteration 37800 training loss 0.9601252 lr 0.00090\n",
      "iteration 37900 training loss 0.7610318 lr 0.00090\n",
      "iteration 38000 training loss 0.76603895 lr 0.00090\n",
      "iteration 38100 training loss 0.5900148 lr 0.00090\n",
      "iteration 38200 training loss 0.7042669 lr 0.00090\n",
      "iteration 38300 training loss 0.60628825 lr 0.00090\n",
      "iteration 38400 training loss 0.7804136 lr 0.00090\n",
      "iteration 38500 training loss 0.5587027 lr 0.00090\n",
      "iteration 38600 training loss 0.5308774 lr 0.00090\n",
      "iteration 38700 training loss 0.7581634 lr 0.00090\n",
      "iteration 38800 training loss 0.66799045 lr 0.00090\n",
      "iteration 38900 training loss 0.7864714 lr 0.00090\n",
      "iteration 39000 training loss 0.94115925 lr 0.00090\n",
      "iteration 39100 training loss 1.0712392 lr 0.00090\n",
      "iteration 39200 training loss 0.8690746 lr 0.00090\n",
      "iteration 39300 training loss 0.7031664 lr 0.00090\n",
      "iteration 39400 training loss 0.7932699 lr 0.00090\n",
      "iteration 39500 training loss 0.8153138 lr 0.00090\n",
      "iteration 39600 training loss 0.9255418 lr 0.00090\n",
      "iteration 39700 training loss 0.9095214 lr 0.00090\n",
      "iteration 39800 training loss 0.5565269 lr 0.00090\n",
      "iteration 39900 training loss 0.6488812 lr 0.00090\n",
      "iteration 40000 training loss 0.7827198 lr 0.00090\n",
      "layout:xla:random 0.21709208029177782\n",
      "layout:nlp:random 0.6901130347207112\n",
      "layout:xla:default 0.019380850840476343\n",
      "layout:nlp:default 0.3357778007959107\n",
      "epoch 0, it 40000 validation loss -0.316\n",
      "iteration 40100 training loss 0.59081614 lr 0.00081\n",
      "iteration 40200 training loss 0.8716119 lr 0.00081\n",
      "iteration 40300 training loss 0.8040292 lr 0.00081\n",
      "iteration 40400 training loss 0.8067987 lr 0.00081\n",
      "iteration 40500 training loss 0.8763738 lr 0.00081\n",
      "iteration 40600 training loss 0.9161913 lr 0.00081\n",
      "iteration 40700 training loss 0.7405748 lr 0.00081\n",
      "iteration 40800 training loss 0.6530095 lr 0.00081\n",
      "iteration 40900 training loss 0.75753874 lr 0.00081\n",
      "iteration 41000 training loss 0.6248126 lr 0.00081\n",
      "iteration 41100 training loss 0.6073762 lr 0.00081\n",
      "iteration 41200 training loss 0.63948876 lr 0.00081\n",
      "iteration 41300 training loss 0.70653224 lr 0.00081\n",
      "iteration 41400 training loss 0.54398525 lr 0.00081\n",
      "iteration 41500 training loss 0.5785054 lr 0.00081\n",
      "iteration 41600 training loss 0.8435902 lr 0.00081\n",
      "iteration 41700 training loss 0.83346987 lr 0.00081\n",
      "iteration 41800 training loss 0.87075925 lr 0.00081\n",
      "iteration 41900 training loss 0.5832886 lr 0.00081\n",
      "iteration 42000 training loss 0.61195153 lr 0.00081\n",
      "iteration 42100 training loss 0.80884916 lr 0.00081\n",
      "iteration 42200 training loss 0.858328 lr 0.00081\n",
      "iteration 42300 training loss 0.8020305 lr 0.00081\n",
      "iteration 42400 training loss 0.78999484 lr 0.00081\n",
      "iteration 42500 training loss 0.80718493 lr 0.00081\n",
      "iteration 42600 training loss 0.91142774 lr 0.00081\n",
      "iteration 42700 training loss 0.9595147 lr 0.00081\n",
      "iteration 42800 training loss 0.84621805 lr 0.00081\n",
      "iteration 42900 training loss 0.5844462 lr 0.00081\n",
      "iteration 43000 training loss 0.8484482 lr 0.00081\n",
      "iteration 43100 training loss 0.855562 lr 0.00081\n",
      "iteration 43200 training loss 0.6082238 lr 0.00081\n",
      "iteration 43300 training loss 0.6927794 lr 0.00081\n",
      "iteration 43400 training loss 0.709026 lr 0.00081\n",
      "iteration 43500 training loss 0.60750425 lr 0.00081\n",
      "iteration 43600 training loss 0.6670983 lr 0.00081\n",
      "iteration 43700 training loss 0.7457547 lr 0.00081\n",
      "iteration 43800 training loss 0.6479135 lr 0.00081\n",
      "iteration 43900 training loss 0.81808925 lr 0.00081\n",
      "iteration 44000 training loss 0.6809255 lr 0.00081\n",
      "iteration 44100 training loss 0.61971337 lr 0.00081\n",
      "iteration 44200 training loss 0.48590696 lr 0.00081\n",
      "iteration 44300 training loss 0.8110529 lr 0.00081\n",
      "iteration 44400 training loss 0.6839793 lr 0.00081\n",
      "iteration 44500 training loss 0.70415795 lr 0.00081\n",
      "iteration 44600 training loss 0.68915355 lr 0.00081\n",
      "iteration 44700 training loss 0.678437 lr 0.00081\n",
      "iteration 44800 training loss 0.70469266 lr 0.00081\n",
      "iteration 44900 training loss 0.5905347 lr 0.00081\n",
      "iteration 45000 training loss 0.5783805 lr 0.00081\n",
      "iteration 45100 training loss 0.46627167 lr 0.00081\n",
      "iteration 45200 training loss 0.59473515 lr 0.00081\n",
      "iteration 45300 training loss 0.7822871 lr 0.00081\n",
      "iteration 45400 training loss 0.7755389 lr 0.00081\n",
      "iteration 45500 training loss 0.8291565 lr 0.00081\n",
      "iteration 45600 training loss 0.70778966 lr 0.00081\n",
      "iteration 45700 training loss 0.7557731 lr 0.00081\n",
      "iteration 45800 training loss 0.76434666 lr 0.00081\n",
      "iteration 45900 training loss 0.6079368 lr 0.00081\n",
      "iteration 46000 training loss 0.9196762 lr 0.00081\n",
      "iteration 46100 training loss 0.9058397 lr 0.00081\n",
      "iteration 46200 training loss 0.8587707 lr 0.00081\n",
      "iteration 46300 training loss 1.0642196 lr 0.00081\n",
      "iteration 46400 training loss 0.9813779 lr 0.00081\n",
      "iteration 46500 training loss 0.93129545 lr 0.00081\n",
      "iteration 46600 training loss 0.7846785 lr 0.00081\n",
      "iteration 46700 training loss 0.9849807 lr 0.00081\n",
      "iteration 46800 training loss 0.9216938 lr 0.00081\n",
      "iteration 46900 training loss 0.81929547 lr 0.00081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47000 training loss 1.094952 lr 0.00081\n",
      "iteration 47100 training loss 1.2266155 lr 0.00081\n",
      "iteration 47200 training loss 0.8300328 lr 0.00081\n",
      "iteration 47300 training loss 0.94694746 lr 0.00081\n",
      "iteration 47400 training loss 1.0665736 lr 0.00081\n",
      "iteration 47500 training loss 0.89929384 lr 0.00081\n",
      "iteration 47600 training loss 0.87886643 lr 0.00081\n",
      "iteration 47700 training loss 0.8057654 lr 0.00081\n",
      "iteration 47800 training loss 1.0039078 lr 0.00081\n",
      "iteration 47900 training loss 0.91427314 lr 0.00081\n",
      "iteration 48000 training loss 0.89590144 lr 0.00081\n",
      "iteration 48100 training loss 0.95408475 lr 0.00081\n",
      "iteration 48200 training loss 0.884502 lr 0.00081\n",
      "iteration 48300 training loss 1.01861 lr 0.00081\n",
      "iteration 48400 training loss 0.9434843 lr 0.00081\n",
      "iteration 48500 training loss 0.97950983 lr 0.00081\n",
      "iteration 48600 training loss 0.969084 lr 0.00081\n",
      "iteration 48700 training loss 1.1838859 lr 0.00081\n",
      "iteration 48800 training loss 0.75536674 lr 0.00081\n",
      "iteration 48900 training loss 0.9677309 lr 0.00081\n",
      "iteration 49000 training loss 0.7341061 lr 0.00081\n",
      "iteration 49100 training loss 0.72600657 lr 0.00081\n",
      "iteration 49200 training loss 1.158726 lr 0.00081\n",
      "iteration 49300 training loss 0.82657343 lr 0.00081\n",
      "iteration 49400 training loss 0.82486784 lr 0.00081\n",
      "iteration 49500 training loss 0.7106637 lr 0.00081\n",
      "iteration 49600 training loss 0.8534247 lr 0.00081\n",
      "iteration 49700 training loss 0.7821564 lr 0.00081\n",
      "iteration 49800 training loss 0.7625136 lr 0.00081\n",
      "iteration 49900 training loss 0.5895751 lr 0.00081\n",
      "iteration 50000 training loss 0.60282224 lr 0.00081\n",
      "layout:xla:random 0.2534013085864118\n",
      "layout:nlp:random 0.7412237210939088\n",
      "layout:xla:default 0.05222197494346297\n",
      "layout:nlp:default 0.3498264119395403\n",
      "epoch 0, it 50000 validation loss -0.349\n",
      "iteration 50100 training loss 0.5804403 lr 0.00081\n",
      "iteration 50200 training loss 0.47407296 lr 0.00081\n",
      "iteration 50300 training loss 0.5942118 lr 0.00081\n",
      "iteration 50400 training loss 0.57099265 lr 0.00081\n",
      "iteration 50500 training loss 0.5013845 lr 0.00081\n",
      "iteration 50600 training loss 0.6997037 lr 0.00081\n",
      "iteration 50700 training loss 0.83344305 lr 0.00081\n",
      "iteration 50800 training loss 0.7106541 lr 0.00081\n",
      "iteration 50900 training loss 0.99378204 lr 0.00081\n",
      "iteration 51000 training loss 0.736476 lr 0.00081\n",
      "iteration 51100 training loss 0.99969846 lr 0.00081\n",
      "iteration 51200 training loss 0.7998641 lr 0.00081\n",
      "iteration 51300 training loss 0.8423384 lr 0.00081\n",
      "iteration 51400 training loss 0.9123353 lr 0.00081\n",
      "iteration 51500 training loss 0.60483736 lr 0.00081\n",
      "iteration 51600 training loss 0.8818957 lr 0.00081\n",
      "iteration 51700 training loss 0.84445554 lr 0.00081\n",
      "iteration 51800 training loss 0.76598567 lr 0.00081\n",
      "iteration 51900 training loss 0.9099805 lr 0.00081\n",
      "iteration 52000 training loss 1.0634669 lr 0.00081\n",
      "iteration 52100 training loss 1.1714127 lr 0.00081\n",
      "iteration 52200 training loss 1.0153977 lr 0.00081\n",
      "iteration 52300 training loss 0.83593196 lr 0.00081\n",
      "iteration 52400 training loss 0.87867284 lr 0.00081\n",
      "iteration 52500 training loss 0.88248706 lr 0.00081\n",
      "iteration 52600 training loss 0.82550627 lr 0.00081\n",
      "iteration 52700 training loss 0.99885565 lr 0.00081\n",
      "iteration 52800 training loss 0.7870372 lr 0.00081\n",
      "iteration 52900 training loss 0.9296764 lr 0.00081\n",
      "iteration 53000 training loss 0.8514796 lr 0.00081\n",
      "iteration 53100 training loss 0.92900115 lr 0.00081\n",
      "iteration 53200 training loss 0.77457017 lr 0.00081\n",
      "iteration 53300 training loss 0.63614005 lr 0.00081\n",
      "iteration 53400 training loss 0.73174566 lr 0.00081\n",
      "iteration 53500 training loss 1.0243249 lr 0.00081\n",
      "iteration 53600 training loss 0.61304766 lr 0.00081\n",
      "iteration 53700 training loss 0.763717 lr 0.00081\n",
      "iteration 53800 training loss 0.52662456 lr 0.00081\n",
      "iteration 53900 training loss 0.9588014 lr 0.00081\n",
      "iteration 54000 training loss 0.70015633 lr 0.00081\n",
      "iteration 54100 training loss 0.926845 lr 0.00081\n",
      "iteration 54200 training loss 1.0275201 lr 0.00081\n",
      "iteration 54300 training loss 0.8266735 lr 0.00081\n",
      "iteration 54400 training loss 0.72497886 lr 0.00081\n",
      "iteration 54500 training loss 0.8546129 lr 0.00081\n",
      "iteration 54600 training loss 0.86245394 lr 0.00081\n",
      "iteration 54700 training loss 0.6359413 lr 0.00081\n",
      "iteration 54800 training loss 0.84278363 lr 0.00081\n",
      "iteration 54900 training loss 0.5888278 lr 0.00081\n",
      "iteration 55000 training loss 0.7735163 lr 0.00081\n",
      "iteration 55100 training loss 0.7397795 lr 0.00081\n",
      "iteration 55200 training loss 0.76607484 lr 0.00081\n",
      "iteration 55300 training loss 0.5901245 lr 0.00081\n",
      "iteration 55400 training loss 0.77372396 lr 0.00081\n",
      "iteration 55500 training loss 0.89422673 lr 0.00081\n",
      "iteration 55600 training loss 0.6457293 lr 0.00081\n",
      "iteration 55700 training loss 0.41534874 lr 0.00081\n",
      "iteration 55800 training loss 0.5517966 lr 0.00081\n",
      "iteration 55900 training loss 0.4885773 lr 0.00081\n",
      "iteration 56000 training loss 0.48828056 lr 0.00081\n",
      "iteration 56100 training loss 0.6770964 lr 0.00081\n",
      "iteration 56200 training loss 0.5127279 lr 0.00081\n",
      "iteration 56300 training loss 0.5548055 lr 0.00081\n",
      "iteration 56400 training loss 0.53182715 lr 0.00081\n",
      "iteration 56500 training loss 0.42864692 lr 0.00081\n",
      "iteration 56600 training loss 0.41085014 lr 0.00081\n",
      "iteration 56700 training loss 0.44355497 lr 0.00081\n",
      "iteration 56800 training loss 0.6301465 lr 0.00081\n",
      "iteration 56900 training loss 0.38443908 lr 0.00081\n",
      "iteration 57000 training loss 0.39788145 lr 0.00081\n",
      "iteration 57100 training loss 0.55993336 lr 0.00081\n",
      "iteration 57200 training loss 0.5400964 lr 0.00081\n",
      "iteration 57300 training loss 0.5804129 lr 0.00081\n",
      "iteration 57400 training loss 0.82435375 lr 0.00081\n",
      "iteration 57500 training loss 0.5837848 lr 0.00081\n",
      "iteration 57600 training loss 0.7305016 lr 0.00081\n",
      "iteration 57700 training loss 0.81236124 lr 0.00081\n",
      "iteration 57800 training loss 0.5498889 lr 0.00081\n",
      "iteration 57900 training loss 0.6114735 lr 0.00081\n",
      "iteration 58000 training loss 0.6331942 lr 0.00081\n",
      "iteration 58100 training loss 0.8486527 lr 0.00081\n",
      "iteration 58200 training loss 0.9391867 lr 0.00081\n",
      "iteration 58300 training loss 0.64005387 lr 0.00081\n",
      "iteration 58400 training loss 0.5548465 lr 0.00081\n",
      "iteration 58500 training loss 0.5914527 lr 0.00081\n",
      "iteration 58600 training loss 0.59196156 lr 0.00081\n",
      "iteration 58700 training loss 0.81170076 lr 0.00081\n",
      "iteration 58800 training loss 1.045185 lr 0.00081\n",
      "iteration 58900 training loss 0.7029822 lr 0.00081\n",
      "iteration 59000 training loss 0.5885351 lr 0.00081\n",
      "iteration 59100 training loss 0.492005 lr 0.00081\n",
      "iteration 59200 training loss 0.55873007 lr 0.00081\n",
      "iteration 59300 training loss 0.479819 lr 0.00081\n",
      "iteration 59400 training loss 0.5567499 lr 0.00081\n",
      "iteration 59500 training loss 0.4147257 lr 0.00081\n",
      "iteration 59600 training loss 0.44951066 lr 0.00081\n",
      "iteration 59700 training loss 0.4351865 lr 0.00081\n",
      "iteration 59800 training loss 0.713423 lr 0.00081\n",
      "iteration 59900 training loss 0.7646157 lr 0.00081\n",
      "iteration 60000 training loss 0.93136317 lr 0.00081\n",
      "layout:xla:random 0.2324975700024395\n",
      "layout:nlp:random 0.7664791746034146\n",
      "layout:xla:default 0.008121654514068839\n",
      "layout:nlp:default 0.35861984682097836\n",
      "epoch 0, it 60000 validation loss -0.341\n",
      "iteration 60100 training loss 0.79303443 lr 0.00073\n",
      "iteration 60200 training loss 0.59941894 lr 0.00073\n",
      "iteration 60300 training loss 1.0930586 lr 0.00073\n",
      "iteration 60400 training loss 0.93074 lr 0.00073\n",
      "iteration 60500 training loss 0.83552474 lr 0.00073\n",
      "iteration 60600 training loss 0.99429154 lr 0.00073\n",
      "iteration 60700 training loss 0.8272168 lr 0.00073\n",
      "iteration 60800 training loss 0.8177475 lr 0.00073\n",
      "iteration 60900 training loss 0.81156 lr 0.00073\n",
      "iteration 61000 training loss 0.7053494 lr 0.00073\n",
      "iteration 61100 training loss 0.7602876 lr 0.00073\n",
      "iteration 61200 training loss 0.9120728 lr 0.00073\n",
      "iteration 61300 training loss 0.95936984 lr 0.00073\n",
      "iteration 61400 training loss 0.54800093 lr 0.00073\n",
      "iteration 61500 training loss 0.71179783 lr 0.00073\n",
      "iteration 61600 training loss 0.6634777 lr 0.00073\n",
      "iteration 61700 training loss 0.57515824 lr 0.00073\n",
      "iteration 61800 training loss 0.68193734 lr 0.00073\n",
      "iteration 61900 training loss 0.874311 lr 0.00073\n",
      "iteration 62000 training loss 0.66629463 lr 0.00073\n",
      "iteration 62100 training loss 0.85920537 lr 0.00073\n",
      "iteration 62200 training loss 1.0879542 lr 0.00073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 62300 training loss 0.82204074 lr 0.00073\n",
      "iteration 62400 training loss 0.8564319 lr 0.00073\n",
      "iteration 62500 training loss 0.90461403 lr 0.00073\n",
      "iteration 62600 training loss 1.1405674 lr 0.00073\n",
      "iteration 62700 training loss 0.98849845 lr 0.00073\n",
      "iteration 62800 training loss 1.0165759 lr 0.00073\n",
      "iteration 62900 training loss 1.0011253 lr 0.00073\n",
      "iteration 63000 training loss 0.8017304 lr 0.00073\n",
      "iteration 63100 training loss 0.67693317 lr 0.00073\n",
      "iteration 63200 training loss 0.6907765 lr 0.00073\n",
      "iteration 63300 training loss 0.81068003 lr 0.00073\n",
      "iteration 63400 training loss 0.8619576 lr 0.00073\n",
      "iteration 63500 training loss 0.92789423 lr 0.00073\n",
      "iteration 63600 training loss 0.93150705 lr 0.00073\n",
      "iteration 63700 training loss 1.0298846 lr 0.00073\n",
      "iteration 63800 training loss 1.0432054 lr 0.00073\n",
      "iteration 63900 training loss 0.8542322 lr 0.00073\n",
      "iteration 64000 training loss 0.8545199 lr 0.00073\n",
      "iteration 64100 training loss 0.9405291 lr 0.00073\n",
      "iteration 64200 training loss 0.68989855 lr 0.00073\n",
      "iteration 64300 training loss 1.1083977 lr 0.00073\n",
      "iteration 64400 training loss 0.8339338 lr 0.00073\n",
      "iteration 64500 training loss 0.8811648 lr 0.00073\n",
      "iteration 64600 training loss 0.7393863 lr 0.00073\n",
      "iteration 64700 training loss 1.0936769 lr 0.00073\n",
      "iteration 64800 training loss 0.995259 lr 0.00073\n",
      "iteration 64900 training loss 0.98735005 lr 0.00073\n",
      "iteration 65000 training loss 0.8022703 lr 0.00073\n",
      "iteration 65100 training loss 0.83819616 lr 0.00073\n",
      "iteration 65200 training loss 0.7157144 lr 0.00073\n",
      "iteration 65300 training loss 0.6481444 lr 0.00073\n",
      "iteration 65400 training loss 0.5508239 lr 0.00073\n",
      "iteration 65500 training loss 0.66596204 lr 0.00073\n",
      "iteration 65600 training loss 0.89073914 lr 0.00073\n",
      "iteration 65700 training loss 0.5824694 lr 0.00073\n",
      "iteration 65800 training loss 0.6085523 lr 0.00073\n",
      "iteration 65900 training loss 0.8558122 lr 0.00073\n",
      "iteration 66000 training loss 0.8414397 lr 0.00073\n",
      "iteration 66100 training loss 0.86266583 lr 0.00073\n",
      "iteration 66200 training loss 0.70372343 lr 0.00073\n",
      "iteration 66300 training loss 0.92019844 lr 0.00073\n",
      "iteration 66400 training loss 0.73070157 lr 0.00073\n",
      "iteration 66500 training loss 1.1063031 lr 0.00073\n",
      "iteration 66600 training loss 1.1553373 lr 0.00073\n",
      "iteration 66700 training loss 1.152311 lr 0.00073\n",
      "iteration 66800 training loss 1.1365023 lr 0.00073\n",
      "iteration 66900 training loss 0.76612574 lr 0.00073\n",
      "iteration 67000 training loss 0.6720489 lr 0.00073\n",
      "iteration 67100 training loss 0.6788594 lr 0.00073\n",
      "iteration 67200 training loss 0.8830064 lr 0.00073\n",
      "iteration 67300 training loss 0.88349915 lr 0.00073\n",
      "iteration 67400 training loss 1.0010425 lr 0.00073\n",
      "iteration 67500 training loss 0.88338786 lr 0.00073\n",
      "iteration 67600 training loss 0.7957647 lr 0.00073\n",
      "iteration 67700 training loss 0.7779984 lr 0.00073\n",
      "iteration 67800 training loss 0.66357446 lr 0.00073\n",
      "iteration 67900 training loss 0.7833269 lr 0.00073\n",
      "iteration 68000 training loss 0.6625183 lr 0.00073\n",
      "iteration 68100 training loss 0.34681308 lr 0.00073\n",
      "iteration 68200 training loss 0.5032499 lr 0.00073\n",
      "iteration 68300 training loss 0.5358658 lr 0.00073\n",
      "iteration 68400 training loss 0.60545844 lr 0.00073\n",
      "iteration 68500 training loss 0.65428793 lr 0.00073\n",
      "iteration 68600 training loss 0.7637592 lr 0.00073\n",
      "iteration 68700 training loss 0.7260086 lr 0.00073\n",
      "iteration 68800 training loss 0.73465085 lr 0.00073\n",
      "iteration 68900 training loss 0.5902325 lr 0.00073\n",
      "iteration 69000 training loss 0.715824 lr 0.00073\n",
      "iteration 69100 training loss 0.83481693 lr 0.00073\n",
      "iteration 69200 training loss 0.50939447 lr 0.00073\n",
      "iteration 69300 training loss 0.5974455 lr 0.00073\n",
      "iteration 69400 training loss 0.4820721 lr 0.00073\n",
      "iteration 69500 training loss 0.50969195 lr 0.00073\n",
      "iteration 69600 training loss 0.726999 lr 0.00073\n",
      "iteration 69700 training loss 0.51938635 lr 0.00073\n",
      "iteration 69800 training loss 0.6063542 lr 0.00073\n",
      "iteration 69900 training loss 0.7132832 lr 0.00073\n",
      "iteration 70000 training loss 0.58264613 lr 0.00073\n",
      "layout:xla:random 0.2593973962665792\n",
      "layout:xla:default 0.02856222110626241\n",
      "layout:nlp:default 0.37441111242335506\n",
      "layout:nlp:random 0.7382675168491136\n",
      "epoch 0, it 70000 validation loss -0.350\n",
      "iteration 70100 training loss 0.52190274 lr 0.00073\n",
      "iteration 70200 training loss 0.80162644 lr 0.00073\n",
      "iteration 70300 training loss 0.5986346 lr 0.00073\n",
      "iteration 70400 training loss 0.7075298 lr 0.00073\n",
      "iteration 70500 training loss 0.7776385 lr 0.00073\n",
      "iteration 70600 training loss 0.8493295 lr 0.00073\n",
      "iteration 70700 training loss 0.44624814 lr 0.00073\n",
      "iteration 70800 training loss 0.5328227 lr 0.00073\n",
      "iteration 70900 training loss 0.93452877 lr 0.00073\n",
      "iteration 71000 training loss 0.7420145 lr 0.00073\n",
      "iteration 71100 training loss 0.47355837 lr 0.00073\n",
      "iteration 71200 training loss 0.42998707 lr 0.00073\n",
      "iteration 71300 training loss 0.38603273 lr 0.00073\n",
      "iteration 71400 training loss 0.50357175 lr 0.00073\n",
      "iteration 71500 training loss 0.42983174 lr 0.00073\n",
      "iteration 71600 training loss 0.5187476 lr 0.00073\n",
      "iteration 71700 training loss 0.49676666 lr 0.00073\n",
      "iteration 71800 training loss 0.43219674 lr 0.00073\n",
      "iteration 71900 training loss 0.277476 lr 0.00073\n",
      "iteration 72000 training loss 0.28344303 lr 0.00073\n",
      "iteration 72100 training loss 0.34010443 lr 0.00073\n",
      "iteration 72200 training loss 0.4671736 lr 0.00073\n",
      "iteration 72300 training loss 0.5814357 lr 0.00073\n",
      "iteration 72400 training loss 0.68825364 lr 0.00073\n",
      "iteration 72500 training loss 0.9526797 lr 0.00073\n",
      "iteration 72600 training loss 0.59027433 lr 0.00073\n",
      "iteration 72700 training loss 0.79504573 lr 0.00073\n",
      "iteration 72800 training loss 0.92336166 lr 0.00073\n",
      "iteration 72900 training loss 0.71546966 lr 0.00073\n",
      "iteration 73000 training loss 0.87499684 lr 0.00073\n",
      "iteration 73100 training loss 0.748607 lr 0.00073\n",
      "iteration 73200 training loss 0.63660926 lr 0.00073\n",
      "iteration 73300 training loss 1.186264 lr 0.00073\n",
      "iteration 73400 training loss 0.7304065 lr 0.00073\n",
      "iteration 73500 training loss 0.87589884 lr 0.00073\n",
      "iteration 73600 training loss 0.8038128 lr 0.00073\n",
      "iteration 73700 training loss 0.93629754 lr 0.00073\n",
      "iteration 73800 training loss 0.72278273 lr 0.00073\n",
      "iteration 73900 training loss 0.8903051 lr 0.00073\n",
      "iteration 74000 training loss 0.8628364 lr 0.00073\n",
      "iteration 74100 training loss 0.6391018 lr 0.00073\n",
      "iteration 74200 training loss 0.9111244 lr 0.00073\n",
      "iteration 74300 training loss 0.65885735 lr 0.00073\n",
      "iteration 74400 training loss 0.74778736 lr 0.00073\n",
      "iteration 74500 training loss 1.1458913 lr 0.00073\n",
      "iteration 74600 training loss 0.7459151 lr 0.00073\n",
      "iteration 74700 training loss 0.83612084 lr 0.00073\n",
      "iteration 74800 training loss 0.9545864 lr 0.00073\n",
      "iteration 74900 training loss 1.416828 lr 0.00073\n",
      "iteration 75000 training loss 1.0321122 lr 0.00073\n",
      "iteration 75100 training loss 1.0259254 lr 0.00073\n",
      "iteration 75200 training loss 0.8395532 lr 0.00073\n",
      "iteration 75300 training loss 0.7724594 lr 0.00073\n",
      "iteration 75400 training loss 1.0190127 lr 0.00073\n",
      "iteration 75500 training loss 0.9637892 lr 0.00073\n",
      "iteration 75600 training loss 0.7799383 lr 0.00073\n",
      "iteration 75700 training loss 0.7177214 lr 0.00073\n",
      "iteration 75800 training loss 0.6454091 lr 0.00073\n",
      "iteration 75900 training loss 0.4970389 lr 0.00073\n",
      "iteration 76000 training loss 0.5859396 lr 0.00073\n",
      "iteration 76100 training loss 0.5765965 lr 0.00073\n",
      "iteration 76200 training loss 0.4092062 lr 0.00073\n",
      "iteration 76300 training loss 0.78096056 lr 0.00073\n",
      "iteration 76400 training loss 0.64478767 lr 0.00073\n",
      "iteration 76500 training loss 0.5944318 lr 0.00073\n",
      "iteration 76600 training loss 0.6637693 lr 0.00073\n",
      "iteration 76700 training loss 0.65147555 lr 0.00073\n",
      "iteration 76800 training loss 0.62926924 lr 0.00073\n",
      "iteration 76900 training loss 0.80889326 lr 0.00073\n",
      "iteration 77000 training loss 0.8003603 lr 0.00073\n",
      "iteration 77100 training loss 0.555411 lr 0.00073\n",
      "iteration 77200 training loss 0.7362027 lr 0.00073\n",
      "iteration 77300 training loss 0.5821159 lr 0.00073\n",
      "iteration 77400 training loss 0.61622566 lr 0.00073\n",
      "iteration 77500 training loss 0.7961739 lr 0.00073\n",
      "iteration 77600 training loss 0.83075917 lr 0.00073\n",
      "iteration 77700 training loss 0.8924682 lr 0.00073\n",
      "iteration 77800 training loss 0.88424426 lr 0.00073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 77900 training loss 0.53278023 lr 0.00073\n",
      "iteration 78000 training loss 0.45152625 lr 0.00073\n",
      "iteration 78100 training loss 0.3371044 lr 0.00073\n",
      "iteration 78200 training loss 0.4587495 lr 0.00073\n",
      "iteration 78300 training loss 0.37139678 lr 0.00073\n",
      "iteration 78400 training loss 0.5614989 lr 0.00073\n",
      "iteration 78500 training loss 0.45045587 lr 0.00073\n",
      "iteration 78600 training loss 0.423156 lr 0.00073\n",
      "iteration 78700 training loss 0.5771558 lr 0.00073\n",
      "iteration 78800 training loss 0.6472498 lr 0.00073\n",
      "iteration 78900 training loss 0.40214553 lr 0.00073\n",
      "iteration 79000 training loss 0.610184 lr 0.00073\n",
      "iteration 79100 training loss 0.55012196 lr 0.00073\n",
      "iteration 79200 training loss 0.43420884 lr 0.00073\n",
      "iteration 79300 training loss 0.527599 lr 0.00073\n",
      "iteration 79400 training loss 0.5073434 lr 0.00073\n",
      "iteration 79500 training loss 0.39473754 lr 0.00073\n",
      "iteration 79600 training loss 0.5899249 lr 0.00073\n",
      "iteration 79700 training loss 0.6205097 lr 0.00073\n",
      "iteration 79800 training loss 0.54263276 lr 0.00073\n",
      "iteration 79900 training loss 0.581929 lr 0.00073\n",
      "iteration 80000 training loss 0.58410203 lr 0.00073\n",
      "layout:nlp:default 0.34100292647306085\n",
      "layout:xla:default -0.0019325829615391063\n",
      "layout:xla:random 0.16287837146123416\n",
      "layout:nlp:random 0.7401700390205064\n",
      "epoch 1, it 80000 validation loss -0.311\n",
      "iteration 80100 training loss 1.9219537 lr 0.00066\n",
      "iteration 80200 training loss 0.91291183 lr 0.00066\n",
      "iteration 80300 training loss 1.0220535 lr 0.00066\n",
      "iteration 80400 training loss 0.9517888 lr 0.00066\n",
      "iteration 80500 training loss 0.7620222 lr 0.00066\n",
      "iteration 80600 training loss 0.9804569 lr 0.00066\n",
      "iteration 80700 training loss 0.9320254 lr 0.00066\n",
      "iteration 80800 training loss 0.86424214 lr 0.00066\n",
      "iteration 80900 training loss 0.8457251 lr 0.00066\n",
      "iteration 81000 training loss 0.87471586 lr 0.00066\n",
      "iteration 81100 training loss 0.7675424 lr 0.00066\n",
      "iteration 81200 training loss 0.6845988 lr 0.00066\n",
      "iteration 81300 training loss 0.73718053 lr 0.00066\n",
      "iteration 81400 training loss 0.75664234 lr 0.00066\n",
      "iteration 81500 training loss 0.8092683 lr 0.00066\n",
      "iteration 81600 training loss 0.8413558 lr 0.00066\n",
      "iteration 81700 training loss 1.0661111 lr 0.00066\n",
      "iteration 81800 training loss 1.0377054 lr 0.00066\n",
      "iteration 81900 training loss 1.0704466 lr 0.00066\n",
      "iteration 82000 training loss 0.9440389 lr 0.00066\n",
      "iteration 82100 training loss 0.86822474 lr 0.00066\n",
      "iteration 82200 training loss 0.85257435 lr 0.00066\n",
      "iteration 82300 training loss 0.74619985 lr 0.00066\n",
      "iteration 82400 training loss 0.84675235 lr 0.00066\n",
      "iteration 82500 training loss 0.6692156 lr 0.00066\n",
      "iteration 82600 training loss 0.7559216 lr 0.00066\n",
      "iteration 82700 training loss 0.93449426 lr 0.00066\n",
      "iteration 82800 training loss 0.75497276 lr 0.00066\n",
      "iteration 82900 training loss 0.78688383 lr 0.00066\n",
      "iteration 83000 training loss 0.6457691 lr 0.00066\n",
      "iteration 83100 training loss 1.0987607 lr 0.00066\n",
      "iteration 83200 training loss 0.6308979 lr 0.00066\n",
      "iteration 83300 training loss 0.91735744 lr 0.00066\n",
      "iteration 83400 training loss 1.0604393 lr 0.00066\n",
      "iteration 83500 training loss 0.76355475 lr 0.00066\n",
      "iteration 83600 training loss 0.9545551 lr 0.00066\n",
      "iteration 83700 training loss 0.7917985 lr 0.00066\n",
      "iteration 83800 training loss 0.6402929 lr 0.00066\n",
      "iteration 83900 training loss 0.5131936 lr 0.00066\n",
      "iteration 84000 training loss 0.86002815 lr 0.00066\n",
      "iteration 84100 training loss 0.6378546 lr 0.00066\n",
      "iteration 84200 training loss 0.80035704 lr 0.00066\n",
      "iteration 84300 training loss 0.794239 lr 0.00066\n",
      "iteration 84400 training loss 0.505409 lr 0.00066\n",
      "iteration 84500 training loss 0.6647419 lr 0.00066\n",
      "iteration 84600 training loss 0.6413359 lr 0.00066\n",
      "iteration 84700 training loss 0.5889282 lr 0.00066\n",
      "iteration 84800 training loss 0.5224382 lr 0.00066\n",
      "iteration 84900 training loss 0.7505927 lr 0.00066\n",
      "iteration 85000 training loss 0.9058315 lr 0.00066\n",
      "iteration 85100 training loss 0.29515088 lr 0.00066\n",
      "iteration 85200 training loss 0.50686187 lr 0.00066\n",
      "iteration 85300 training loss 0.41978365 lr 0.00066\n",
      "iteration 85400 training loss 0.7274476 lr 0.00066\n",
      "iteration 85500 training loss 0.65498877 lr 0.00066\n",
      "iteration 85600 training loss 0.67087114 lr 0.00066\n",
      "iteration 85700 training loss 0.8411903 lr 0.00066\n",
      "iteration 85800 training loss 0.75728375 lr 0.00066\n",
      "iteration 85900 training loss 0.60911316 lr 0.00066\n",
      "iteration 86000 training loss 0.5618011 lr 0.00066\n",
      "iteration 86100 training loss 0.53090674 lr 0.00066\n",
      "iteration 86200 training loss 0.6148877 lr 0.00066\n",
      "iteration 86300 training loss 0.6817642 lr 0.00066\n",
      "iteration 86400 training loss 0.94226813 lr 0.00066\n",
      "iteration 86500 training loss 0.9102376 lr 0.00066\n",
      "iteration 86600 training loss 1.1863323 lr 0.00066\n",
      "iteration 86700 training loss 1.003782 lr 0.00066\n",
      "iteration 86800 training loss 0.7842582 lr 0.00066\n",
      "iteration 86900 training loss 0.7473051 lr 0.00066\n",
      "iteration 87000 training loss 1.0746304 lr 0.00066\n",
      "iteration 87100 training loss 0.9834997 lr 0.00066\n",
      "iteration 87200 training loss 0.70850384 lr 0.00066\n",
      "iteration 87300 training loss 0.89835507 lr 0.00066\n",
      "iteration 87400 training loss 0.6569149 lr 0.00066\n",
      "iteration 87500 training loss 0.8766342 lr 0.00066\n",
      "iteration 87600 training loss 0.6810872 lr 0.00066\n",
      "iteration 87700 training loss 0.9346341 lr 0.00066\n",
      "iteration 87800 training loss 0.7992654 lr 0.00066\n",
      "iteration 87900 training loss 0.7609176 lr 0.00066\n",
      "iteration 88000 training loss 0.8541136 lr 0.00066\n",
      "iteration 88100 training loss 0.83665454 lr 0.00066\n",
      "iteration 88200 training loss 0.8444947 lr 0.00066\n",
      "iteration 88300 training loss 1.0726072 lr 0.00066\n",
      "iteration 88400 training loss 0.89537084 lr 0.00066\n",
      "iteration 88500 training loss 1.0245038 lr 0.00066\n",
      "iteration 88600 training loss 1.0079122 lr 0.00066\n",
      "iteration 88700 training loss 0.9302101 lr 0.00066\n",
      "iteration 88800 training loss 1.0767357 lr 0.00066\n",
      "iteration 88900 training loss 0.9954342 lr 0.00066\n",
      "iteration 89000 training loss 0.67846334 lr 0.00066\n",
      "iteration 89100 training loss 0.80137306 lr 0.00066\n",
      "iteration 89200 training loss 0.7950703 lr 0.00066\n",
      "iteration 89300 training loss 0.7611687 lr 0.00066\n",
      "iteration 89400 training loss 1.0180606 lr 0.00066\n",
      "iteration 89500 training loss 0.61933887 lr 0.00066\n",
      "iteration 89600 training loss 0.74154073 lr 0.00066\n",
      "iteration 89700 training loss 0.8687097 lr 0.00066\n",
      "iteration 89800 training loss 0.5414646 lr 0.00066\n",
      "iteration 89900 training loss 0.63228726 lr 0.00066\n",
      "iteration 90000 training loss 0.74995905 lr 0.00066\n",
      "layout:xla:default 0.04743019364645512\n",
      "layout:nlp:default 0.36261263844928215\n",
      "layout:xla:random 0.17834291425609577\n",
      "layout:nlp:random 0.6954367943232276\n",
      "epoch 1, it 90000 validation loss -0.321\n",
      "iteration 90100 training loss 0.7192133 lr 0.00066\n",
      "iteration 90200 training loss 0.550301 lr 0.00066\n",
      "iteration 90300 training loss 0.6561429 lr 0.00066\n",
      "iteration 90400 training loss 0.6979433 lr 0.00066\n",
      "iteration 90500 training loss 0.78935295 lr 0.00066\n",
      "iteration 90600 training loss 0.69982135 lr 0.00066\n",
      "iteration 90700 training loss 0.69779533 lr 0.00066\n",
      "iteration 90800 training loss 0.9968875 lr 0.00066\n",
      "iteration 90900 training loss 0.8208536 lr 0.00066\n",
      "iteration 91000 training loss 0.9929661 lr 0.00066\n",
      "iteration 91100 training loss 1.0060481 lr 0.00066\n",
      "iteration 91200 training loss 0.706014 lr 0.00066\n",
      "iteration 91300 training loss 0.65846235 lr 0.00066\n",
      "iteration 91400 training loss 0.6373063 lr 0.00066\n",
      "iteration 91500 training loss 0.7391945 lr 0.00066\n",
      "iteration 91600 training loss 0.65928334 lr 0.00066\n",
      "iteration 91700 training loss 0.7217106 lr 0.00066\n",
      "iteration 91800 training loss 0.55284727 lr 0.00066\n",
      "iteration 91900 training loss 0.70795643 lr 0.00066\n",
      "iteration 92000 training loss 0.8225379 lr 0.00066\n",
      "iteration 92100 training loss 0.6180376 lr 0.00066\n",
      "iteration 92200 training loss 0.8867 lr 0.00066\n",
      "iteration 92300 training loss 0.8159341 lr 0.00066\n",
      "iteration 92400 training loss 0.84957635 lr 0.00066\n",
      "iteration 92500 training loss 0.6451992 lr 0.00066\n",
      "iteration 92600 training loss 0.7122547 lr 0.00066\n",
      "iteration 92700 training loss 0.7017005 lr 0.00066\n",
      "iteration 92800 training loss 0.79410243 lr 0.00066\n",
      "iteration 92900 training loss 0.8244469 lr 0.00066\n",
      "iteration 93000 training loss 0.78476715 lr 0.00066\n",
      "iteration 93100 training loss 1.0733458 lr 0.00066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 93200 training loss 0.8670535 lr 0.00066\n",
      "iteration 93300 training loss 0.9840505 lr 0.00066\n",
      "iteration 93400 training loss 0.71545774 lr 0.00066\n",
      "iteration 93500 training loss 0.97205484 lr 0.00066\n",
      "iteration 93600 training loss 0.8764315 lr 0.00066\n",
      "iteration 93700 training loss 0.9554379 lr 0.00066\n",
      "iteration 93800 training loss 0.8221818 lr 0.00066\n",
      "iteration 93900 training loss 0.7901983 lr 0.00066\n",
      "iteration 94000 training loss 0.7421448 lr 0.00066\n",
      "iteration 94100 training loss 0.9004836 lr 0.00066\n",
      "iteration 94200 training loss 1.046283 lr 0.00066\n",
      "iteration 94300 training loss 0.9003002 lr 0.00066\n",
      "iteration 94400 training loss 0.8719772 lr 0.00066\n",
      "iteration 94500 training loss 0.8663967 lr 0.00066\n",
      "iteration 94600 training loss 0.8025242 lr 0.00066\n",
      "iteration 94700 training loss 0.7294644 lr 0.00066\n",
      "iteration 94800 training loss 0.64002556 lr 0.00066\n",
      "iteration 94900 training loss 0.93513966 lr 0.00066\n",
      "iteration 95000 training loss 0.703811 lr 0.00066\n",
      "iteration 95100 training loss 0.54342645 lr 0.00066\n",
      "iteration 95200 training loss 0.8136518 lr 0.00066\n",
      "iteration 95300 training loss 0.45468348 lr 0.00066\n",
      "iteration 95400 training loss 0.63178885 lr 0.00066\n",
      "iteration 95500 training loss 0.73609287 lr 0.00066\n",
      "iteration 95600 training loss 0.4868022 lr 0.00066\n",
      "iteration 95700 training loss 0.6874742 lr 0.00066\n",
      "iteration 95800 training loss 0.6210273 lr 0.00066\n",
      "iteration 95900 training loss 0.5654536 lr 0.00066\n",
      "iteration 96000 training loss 0.7242211 lr 0.00066\n",
      "iteration 96100 training loss 0.5223014 lr 0.00066\n",
      "iteration 96200 training loss 0.4695091 lr 0.00066\n",
      "iteration 96300 training loss 0.6584906 lr 0.00066\n",
      "iteration 96400 training loss 0.5988915 lr 0.00066\n",
      "iteration 96500 training loss 0.5840735 lr 0.00066\n",
      "iteration 96600 training loss 0.7707746 lr 0.00066\n",
      "iteration 96700 training loss 0.7336275 lr 0.00066\n",
      "iteration 96800 training loss 0.8572899 lr 0.00066\n",
      "iteration 96900 training loss 0.71864074 lr 0.00066\n",
      "iteration 97000 training loss 1.0102967 lr 0.00066\n",
      "iteration 97100 training loss 0.89297104 lr 0.00066\n",
      "iteration 97200 training loss 0.7103603 lr 0.00066\n",
      "iteration 97300 training loss 1.1696036 lr 0.00066\n",
      "iteration 97400 training loss 0.688326 lr 0.00066\n",
      "iteration 97500 training loss 0.7749488 lr 0.00066\n",
      "iteration 97600 training loss 1.0049093 lr 0.00066\n",
      "iteration 97700 training loss 0.89589953 lr 0.00066\n",
      "iteration 97800 training loss 0.74088293 lr 0.00066\n",
      "iteration 97900 training loss 0.7850347 lr 0.00066\n",
      "iteration 98000 training loss 0.7208475 lr 0.00066\n",
      "iteration 98100 training loss 0.77704304 lr 0.00066\n",
      "iteration 98200 training loss 0.9444843 lr 0.00066\n",
      "iteration 98300 training loss 0.80383134 lr 0.00066\n",
      "iteration 98400 training loss 0.82930356 lr 0.00066\n",
      "iteration 98500 training loss 0.78287387 lr 0.00066\n",
      "iteration 98600 training loss 0.76018447 lr 0.00066\n",
      "iteration 98700 training loss 0.77285826 lr 0.00066\n",
      "iteration 98800 training loss 0.70316833 lr 0.00066\n",
      "iteration 98900 training loss 0.7621272 lr 0.00066\n",
      "iteration 99000 training loss 0.7878153 lr 0.00066\n",
      "iteration 99100 training loss 0.7628649 lr 0.00066\n",
      "iteration 99200 training loss 0.85868204 lr 0.00066\n",
      "iteration 99300 training loss 0.83366716 lr 0.00066\n",
      "iteration 99400 training loss 0.811812 lr 0.00066\n",
      "iteration 99500 training loss 0.8793606 lr 0.00066\n",
      "iteration 99600 training loss 0.77174205 lr 0.00066\n",
      "iteration 99700 training loss 0.81764656 lr 0.00066\n",
      "iteration 99800 training loss 0.58457065 lr 0.00066\n",
      "iteration 99900 training loss 0.5956702 lr 0.00066\n",
      "iteration 100000 training loss 0.7101598 lr 0.00066\n",
      "layout:xla:random 0.23490988211384392\n",
      "layout:nlp:random 0.7553306161142841\n",
      "layout:nlp:default 0.35888901393825584\n",
      "layout:xla:default 0.012276554542572636\n",
      "epoch 1, it 100000 validation loss -0.340\n",
      "iteration 100100 training loss 0.7485791 lr 0.00059\n",
      "iteration 100200 training loss 0.6072449 lr 0.00059\n",
      "iteration 100300 training loss 0.6295962 lr 0.00059\n",
      "iteration 100400 training loss 0.84728223 lr 0.00059\n",
      "iteration 100500 training loss 1.3037827 lr 0.00059\n",
      "iteration 100600 training loss 0.7731878 lr 0.00059\n",
      "iteration 100700 training loss 0.9920298 lr 0.00059\n",
      "iteration 100800 training loss 0.99242455 lr 0.00059\n",
      "iteration 100900 training loss 0.718581 lr 0.00059\n",
      "iteration 101000 training loss 1.0376118 lr 0.00059\n",
      "iteration 101100 training loss 1.0816513 lr 0.00059\n",
      "iteration 101200 training loss 0.74003845 lr 0.00059\n",
      "iteration 101300 training loss 0.7394715 lr 0.00059\n",
      "iteration 101400 training loss 1.0563449 lr 0.00059\n",
      "iteration 101500 training loss 0.9576139 lr 0.00059\n",
      "iteration 101600 training loss 0.7843311 lr 0.00059\n",
      "iteration 101700 training loss 0.67809194 lr 0.00059\n",
      "iteration 101800 training loss 0.62090343 lr 0.00059\n",
      "iteration 101900 training loss 0.54968596 lr 0.00059\n",
      "iteration 102000 training loss 0.9203218 lr 0.00059\n",
      "iteration 102100 training loss 0.99769264 lr 0.00059\n",
      "iteration 102200 training loss 1.0244735 lr 0.00059\n",
      "iteration 102300 training loss 0.75242525 lr 0.00059\n",
      "iteration 102400 training loss 0.9678447 lr 0.00059\n",
      "iteration 102500 training loss 0.78386414 lr 0.00059\n",
      "iteration 102600 training loss 0.7757234 lr 0.00059\n",
      "iteration 102700 training loss 0.869193 lr 0.00059\n",
      "iteration 102800 training loss 0.77190477 lr 0.00059\n",
      "iteration 102900 training loss 0.63962495 lr 0.00059\n",
      "iteration 103000 training loss 0.58812696 lr 0.00059\n",
      "iteration 103100 training loss 0.5870495 lr 0.00059\n",
      "iteration 103200 training loss 0.72558576 lr 0.00059\n",
      "iteration 103300 training loss 0.5375768 lr 0.00059\n",
      "iteration 103400 training loss 0.4130742 lr 0.00059\n",
      "iteration 103500 training loss 0.88389033 lr 0.00059\n",
      "iteration 103600 training loss 0.8509125 lr 0.00059\n",
      "iteration 103700 training loss 0.76331323 lr 0.00059\n",
      "iteration 103800 training loss 0.7206887 lr 0.00059\n",
      "iteration 103900 training loss 0.8348156 lr 0.00059\n",
      "iteration 104000 training loss 0.8209535 lr 0.00059\n",
      "iteration 104100 training loss 0.86278975 lr 0.00059\n",
      "iteration 104200 training loss 0.903038 lr 0.00059\n",
      "iteration 104300 training loss 0.8689543 lr 0.00059\n",
      "iteration 104400 training loss 0.5054313 lr 0.00059\n",
      "iteration 104500 training loss 0.86286044 lr 0.00059\n",
      "iteration 104600 training loss 0.6777679 lr 0.00059\n",
      "iteration 104700 training loss 0.74599344 lr 0.00059\n",
      "iteration 104800 training loss 0.7800893 lr 0.00059\n",
      "iteration 104900 training loss 0.8797379 lr 0.00059\n",
      "iteration 105000 training loss 0.76606196 lr 0.00059\n",
      "iteration 105100 training loss 0.72677326 lr 0.00059\n",
      "iteration 105200 training loss 0.62233543 lr 0.00059\n",
      "iteration 105300 training loss 0.567773 lr 0.00059\n",
      "iteration 105400 training loss 0.5761723 lr 0.00059\n",
      "iteration 105500 training loss 0.72788703 lr 0.00059\n",
      "iteration 105600 training loss 0.68360454 lr 0.00059\n",
      "iteration 105700 training loss 0.55670595 lr 0.00059\n",
      "iteration 105800 training loss 0.8398346 lr 0.00059\n",
      "iteration 105900 training loss 0.8670636 lr 0.00059\n",
      "iteration 106000 training loss 0.60727257 lr 0.00059\n",
      "iteration 106100 training loss 0.95676833 lr 0.00059\n",
      "iteration 106200 training loss 0.93097687 lr 0.00059\n",
      "iteration 106300 training loss 0.7829341 lr 0.00059\n",
      "iteration 106400 training loss 0.8491038 lr 0.00059\n",
      "iteration 106500 training loss 0.8855463 lr 0.00059\n",
      "iteration 106600 training loss 0.62546754 lr 0.00059\n",
      "iteration 106700 training loss 0.9385704 lr 0.00059\n",
      "iteration 106800 training loss 0.66601384 lr 0.00059\n",
      "iteration 106900 training loss 0.7354356 lr 0.00059\n",
      "iteration 107000 training loss 0.7904283 lr 0.00059\n",
      "iteration 107100 training loss 1.1598265 lr 0.00059\n",
      "iteration 107200 training loss 0.6627737 lr 0.00059\n",
      "iteration 107300 training loss 0.9998712 lr 0.00059\n",
      "iteration 107400 training loss 0.69760334 lr 0.00059\n",
      "iteration 107500 training loss 0.79398364 lr 0.00059\n",
      "iteration 107600 training loss 0.7818331 lr 0.00059\n",
      "iteration 107700 training loss 0.82267404 lr 0.00059\n",
      "iteration 107800 training loss 0.6679595 lr 0.00059\n",
      "iteration 107900 training loss 0.7022243 lr 0.00059\n",
      "iteration 108000 training loss 0.7973463 lr 0.00059\n",
      "iteration 108100 training loss 0.8571444 lr 0.00059\n",
      "iteration 108200 training loss 0.67236274 lr 0.00059\n",
      "iteration 108300 training loss 0.7625904 lr 0.00059\n",
      "iteration 108400 training loss 0.9532104 lr 0.00059\n",
      "iteration 108500 training loss 0.7000306 lr 0.00059\n",
      "iteration 108600 training loss 1.0889012 lr 0.00059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 108700 training loss 0.8479334 lr 0.00059\n",
      "iteration 108800 training loss 0.77531755 lr 0.00059\n",
      "iteration 108900 training loss 1.0325327 lr 0.00059\n",
      "iteration 109000 training loss 1.016278 lr 0.00059\n",
      "iteration 109100 training loss 0.9353499 lr 0.00059\n",
      "iteration 109200 training loss 0.77913237 lr 0.00059\n",
      "iteration 109300 training loss 0.76576287 lr 0.00059\n",
      "iteration 109400 training loss 0.64205384 lr 0.00059\n",
      "iteration 109500 training loss 0.7180959 lr 0.00059\n",
      "iteration 109600 training loss 0.70897573 lr 0.00059\n",
      "iteration 109700 training loss 0.80113125 lr 0.00059\n",
      "iteration 109800 training loss 0.5328448 lr 0.00059\n",
      "iteration 109900 training loss 0.7784304 lr 0.00059\n",
      "iteration 110000 training loss 0.7579482 lr 0.00059\n",
      "layout:xla:default 0.03089168342911437\n",
      "layout:xla:random 0.23633482709859685\n",
      "layout:nlp:default 0.34217531894247033\n",
      "layout:nlp:random 0.7450391255817594\n",
      "epoch 1, it 110000 validation loss -0.339\n",
      "iteration 110100 training loss 0.78136533 lr 0.00059\n",
      "iteration 110200 training loss 0.5716138 lr 0.00059\n",
      "iteration 110300 training loss 0.6534636 lr 0.00059\n",
      "iteration 110400 training loss 0.7116254 lr 0.00059\n",
      "iteration 110500 training loss 0.8797229 lr 0.00059\n",
      "iteration 110600 training loss 0.8310197 lr 0.00059\n",
      "iteration 110700 training loss 0.77405864 lr 0.00059\n",
      "iteration 110800 training loss 1.0477191 lr 0.00059\n",
      "iteration 110900 training loss 0.99460655 lr 0.00059\n",
      "iteration 111000 training loss 0.6263034 lr 0.00059\n",
      "iteration 111100 training loss 0.66374683 lr 0.00059\n",
      "iteration 111200 training loss 0.8937441 lr 0.00059\n",
      "iteration 111300 training loss 0.89769053 lr 0.00059\n",
      "iteration 111400 training loss 0.6186776 lr 0.00059\n",
      "iteration 111500 training loss 0.65476817 lr 0.00059\n",
      "iteration 111600 training loss 0.73577714 lr 0.00059\n",
      "iteration 111700 training loss 0.66052073 lr 0.00059\n",
      "iteration 111800 training loss 0.6350548 lr 0.00059\n",
      "iteration 111900 training loss 0.8424737 lr 0.00059\n",
      "iteration 112000 training loss 0.6486129 lr 0.00059\n",
      "iteration 112100 training loss 0.6070204 lr 0.00059\n",
      "iteration 112200 training loss 0.69541836 lr 0.00059\n",
      "iteration 112300 training loss 0.88395774 lr 0.00059\n",
      "iteration 112400 training loss 0.57740825 lr 0.00059\n",
      "iteration 112500 training loss 0.7237876 lr 0.00059\n",
      "iteration 112600 training loss 0.55698144 lr 0.00059\n",
      "iteration 112700 training loss 0.66507816 lr 0.00059\n",
      "iteration 112800 training loss 0.5504702 lr 0.00059\n",
      "iteration 112900 training loss 0.46225917 lr 0.00059\n",
      "iteration 113000 training loss 0.63979363 lr 0.00059\n",
      "iteration 113100 training loss 0.6093313 lr 0.00059\n",
      "iteration 113200 training loss 0.5680304 lr 0.00059\n",
      "iteration 113300 training loss 0.5029202 lr 0.00059\n",
      "iteration 113400 training loss 0.6920059 lr 0.00059\n",
      "iteration 113500 training loss 0.53237844 lr 0.00059\n",
      "iteration 113600 training loss 0.83581066 lr 0.00059\n",
      "iteration 113700 training loss 0.69274217 lr 0.00059\n",
      "iteration 113800 training loss 0.637143 lr 0.00059\n",
      "iteration 113900 training loss 0.63654315 lr 0.00059\n",
      "iteration 114000 training loss 0.69048995 lr 0.00059\n",
      "iteration 114100 training loss 0.61528766 lr 0.00059\n",
      "iteration 114200 training loss 0.46739262 lr 0.00059\n",
      "iteration 114300 training loss 0.56713253 lr 0.00059\n",
      "iteration 114400 training loss 0.70539165 lr 0.00059\n",
      "iteration 114500 training loss 0.8304847 lr 0.00059\n",
      "iteration 114600 training loss 0.75368077 lr 0.00059\n",
      "iteration 114700 training loss 0.608033 lr 0.00059\n",
      "iteration 114800 training loss 0.6614933 lr 0.00059\n",
      "iteration 114900 training loss 0.6842032 lr 0.00059\n",
      "iteration 115000 training loss 0.73202 lr 0.00059\n",
      "iteration 115100 training loss 0.7679906 lr 0.00059\n",
      "iteration 115200 training loss 0.777675 lr 0.00059\n",
      "iteration 115300 training loss 0.67266065 lr 0.00059\n",
      "iteration 115400 training loss 0.90476626 lr 0.00059\n",
      "iteration 115500 training loss 0.7164598 lr 0.00059\n",
      "iteration 115600 training loss 0.5958878 lr 0.00059\n",
      "iteration 115700 training loss 0.7490432 lr 0.00059\n",
      "iteration 115800 training loss 0.771045 lr 0.00059\n",
      "iteration 115900 training loss 0.70453304 lr 0.00059\n",
      "iteration 116000 training loss 0.87344825 lr 0.00059\n",
      "iteration 116100 training loss 0.7563342 lr 0.00059\n",
      "iteration 116200 training loss 0.92712545 lr 0.00059\n",
      "iteration 116300 training loss 0.5357779 lr 0.00059\n",
      "iteration 116400 training loss 0.70185924 lr 0.00059\n",
      "iteration 116500 training loss 0.57956237 lr 0.00059\n",
      "iteration 116600 training loss 0.7723286 lr 0.00059\n",
      "iteration 116700 training loss 0.75193876 lr 0.00059\n",
      "iteration 116800 training loss 0.9039957 lr 0.00059\n",
      "iteration 116900 training loss 0.69138694 lr 0.00059\n",
      "iteration 117000 training loss 0.6797422 lr 0.00059\n",
      "iteration 117100 training loss 0.6109913 lr 0.00059\n",
      "iteration 117200 training loss 0.4964819 lr 0.00059\n",
      "iteration 117300 training loss 0.6491289 lr 0.00059\n",
      "iteration 117400 training loss 0.8673506 lr 0.00059\n",
      "iteration 117500 training loss 0.78934205 lr 0.00059\n",
      "iteration 117600 training loss 0.87552005 lr 0.00059\n",
      "iteration 117700 training loss 0.592459 lr 0.00059\n",
      "iteration 117800 training loss 0.9662954 lr 0.00059\n",
      "iteration 117900 training loss 0.68665254 lr 0.00059\n",
      "iteration 118000 training loss 0.74682623 lr 0.00059\n",
      "iteration 118100 training loss 0.7944239 lr 0.00059\n",
      "iteration 118200 training loss 0.9496778 lr 0.00059\n",
      "iteration 118300 training loss 0.8548416 lr 0.00059\n",
      "iteration 118400 training loss 0.9229328 lr 0.00059\n",
      "iteration 118500 training loss 0.78217673 lr 0.00059\n",
      "iteration 118600 training loss 0.68949294 lr 0.00059\n",
      "iteration 118700 training loss 0.64744866 lr 0.00059\n",
      "iteration 118800 training loss 0.7881911 lr 0.00059\n",
      "iteration 118900 training loss 0.700555 lr 0.00059\n",
      "iteration 119000 training loss 0.8077852 lr 0.00059\n",
      "iteration 119100 training loss 0.919211 lr 0.00059\n",
      "iteration 119200 training loss 1.0563251 lr 0.00059\n",
      "iteration 119300 training loss 1.019445 lr 0.00059\n",
      "iteration 119400 training loss 1.1127081 lr 0.00059\n",
      "iteration 119500 training loss 0.7333073 lr 0.00059\n",
      "iteration 119600 training loss 0.787148 lr 0.00059\n",
      "iteration 119700 training loss 0.83040226 lr 0.00059\n",
      "iteration 119800 training loss 0.5456158 lr 0.00059\n",
      "iteration 119900 training loss 0.68879247 lr 0.00059\n",
      "iteration 120000 training loss 0.6261342 lr 0.00059\n",
      "layout:xla:random 0.28485813377019037\n",
      "layout:nlp:default 0.37383785217080157\n",
      "layout:nlp:random 0.7236375540519557\n",
      "layout:xla:default 0.037197191260921296\n",
      "epoch 1, it 120000 validation loss -0.355\n",
      "iteration 120100 training loss 0.63064706 lr 0.00053\n",
      "iteration 120200 training loss 0.9169028 lr 0.00053\n",
      "iteration 120300 training loss 0.5679742 lr 0.00053\n",
      "iteration 120400 training loss 0.7777983 lr 0.00053\n",
      "iteration 120500 training loss 0.71026534 lr 0.00053\n",
      "iteration 120600 training loss 0.6836457 lr 0.00053\n",
      "iteration 120700 training loss 0.85866106 lr 0.00053\n",
      "iteration 120800 training loss 0.7753245 lr 0.00053\n",
      "iteration 120900 training loss 0.9028688 lr 0.00053\n",
      "iteration 121000 training loss 0.7809463 lr 0.00053\n",
      "iteration 121100 training loss 0.86281735 lr 0.00053\n",
      "iteration 121200 training loss 0.86068827 lr 0.00053\n",
      "iteration 121300 training loss 0.77685565 lr 0.00053\n",
      "iteration 121400 training loss 0.918108 lr 0.00053\n",
      "iteration 121500 training loss 0.83216995 lr 0.00053\n",
      "iteration 121600 training loss 0.6392126 lr 0.00053\n",
      "iteration 121700 training loss 0.5014351 lr 0.00053\n",
      "iteration 121800 training loss 0.41391334 lr 0.00053\n",
      "iteration 121900 training loss 0.4072587 lr 0.00053\n",
      "iteration 122000 training loss 0.7254635 lr 0.00053\n",
      "iteration 122100 training loss 0.65775925 lr 0.00053\n",
      "iteration 122200 training loss 0.57233703 lr 0.00053\n",
      "iteration 122300 training loss 0.7168692 lr 0.00053\n",
      "iteration 122400 training loss 0.38033742 lr 0.00053\n",
      "iteration 122500 training loss 0.55547863 lr 0.00053\n",
      "iteration 122600 training loss 0.5916066 lr 0.00053\n",
      "iteration 122700 training loss 0.4019287 lr 0.00053\n",
      "iteration 122800 training loss 0.8131546 lr 0.00053\n",
      "iteration 122900 training loss 0.5843446 lr 0.00053\n",
      "iteration 123000 training loss 0.6841151 lr 0.00053\n",
      "iteration 123100 training loss 0.7266486 lr 0.00053\n",
      "iteration 123200 training loss 0.76431704 lr 0.00053\n",
      "iteration 123300 training loss 0.73182887 lr 0.00053\n",
      "iteration 123400 training loss 0.5495792 lr 0.00053\n",
      "iteration 123500 training loss 0.5332555 lr 0.00053\n",
      "iteration 123600 training loss 0.70027053 lr 0.00053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 123700 training loss 0.66147715 lr 0.00053\n",
      "iteration 123800 training loss 0.9103126 lr 0.00053\n",
      "iteration 123900 training loss 0.99165505 lr 0.00053\n",
      "iteration 124000 training loss 0.59368885 lr 0.00053\n",
      "iteration 124100 training loss 0.81632036 lr 0.00053\n",
      "iteration 124200 training loss 0.6827892 lr 0.00053\n",
      "iteration 124300 training loss 0.8399274 lr 0.00053\n",
      "iteration 124400 training loss 0.7425821 lr 0.00053\n",
      "iteration 124500 training loss 0.7808541 lr 0.00053\n",
      "iteration 124600 training loss 0.8789911 lr 0.00053\n",
      "iteration 124700 training loss 0.93650734 lr 0.00053\n",
      "iteration 124800 training loss 0.8937623 lr 0.00053\n",
      "iteration 124900 training loss 0.8692729 lr 0.00053\n",
      "iteration 125000 training loss 0.9686113 lr 0.00053\n",
      "iteration 125100 training loss 0.926422 lr 0.00053\n",
      "iteration 125200 training loss 1.0298604 lr 0.00053\n",
      "iteration 125300 training loss 0.87343544 lr 0.00053\n",
      "iteration 125400 training loss 1.1362157 lr 0.00053\n",
      "iteration 125500 training loss 0.7998683 lr 0.00053\n",
      "iteration 125600 training loss 0.75472677 lr 0.00053\n",
      "iteration 125700 training loss 0.78180414 lr 0.00053\n",
      "iteration 125800 training loss 0.8011231 lr 0.00053\n",
      "iteration 125900 training loss 0.7105781 lr 0.00053\n",
      "iteration 126000 training loss 0.66933364 lr 0.00053\n",
      "iteration 126100 training loss 0.8520616 lr 0.00053\n",
      "iteration 126200 training loss 1.0566839 lr 0.00053\n",
      "iteration 126300 training loss 0.8527864 lr 0.00053\n",
      "iteration 126400 training loss 0.67273235 lr 0.00053\n",
      "iteration 126500 training loss 0.79906183 lr 0.00053\n",
      "iteration 126600 training loss 0.7016042 lr 0.00053\n",
      "iteration 126700 training loss 0.67256474 lr 0.00053\n",
      "iteration 126800 training loss 0.8743784 lr 0.00053\n",
      "iteration 126900 training loss 0.95336384 lr 0.00053\n",
      "iteration 127000 training loss 0.83036244 lr 0.00053\n",
      "iteration 127100 training loss 0.8929406 lr 0.00053\n",
      "iteration 127200 training loss 1.0814047 lr 0.00053\n",
      "iteration 127300 training loss 0.8260158 lr 0.00053\n",
      "iteration 127400 training loss 0.9942055 lr 0.00053\n",
      "iteration 127500 training loss 0.847349 lr 0.00053\n",
      "iteration 127600 training loss 0.98352414 lr 0.00053\n",
      "iteration 127700 training loss 0.9521237 lr 0.00053\n",
      "iteration 127800 training loss 0.76855195 lr 0.00053\n",
      "iteration 127900 training loss 0.76035047 lr 0.00053\n",
      "iteration 128000 training loss 0.9782917 lr 0.00053\n",
      "iteration 128100 training loss 0.7650585 lr 0.00053\n",
      "iteration 128200 training loss 0.64679015 lr 0.00053\n",
      "iteration 128300 training loss 0.7430432 lr 0.00053\n",
      "iteration 128400 training loss 0.6265678 lr 0.00053\n",
      "iteration 128500 training loss 0.6810979 lr 0.00053\n",
      "iteration 128600 training loss 0.37486184 lr 0.00053\n",
      "iteration 128700 training loss 0.7223288 lr 0.00053\n",
      "iteration 128800 training loss 0.5831121 lr 0.00053\n",
      "iteration 128900 training loss 0.6483942 lr 0.00053\n",
      "iteration 129000 training loss 0.41735926 lr 0.00053\n",
      "iteration 129100 training loss 0.6503248 lr 0.00053\n",
      "iteration 129200 training loss 0.6539684 lr 0.00053\n",
      "iteration 129300 training loss 0.7322466 lr 0.00053\n",
      "iteration 129400 training loss 0.72792 lr 0.00053\n",
      "iteration 129500 training loss 0.66978085 lr 0.00053\n",
      "iteration 129600 training loss 0.8965072 lr 0.00053\n",
      "iteration 129700 training loss 0.6679071 lr 0.00053\n",
      "iteration 129800 training loss 0.9077887 lr 0.00053\n",
      "iteration 129900 training loss 0.7462017 lr 0.00053\n",
      "iteration 130000 training loss 0.9196118 lr 0.00053\n",
      "layout:nlp:default 0.33843884588997153\n",
      "layout:nlp:random 0.6973600040622122\n",
      "layout:xla:random 0.23222368491810766\n",
      "layout:xla:default 0.030031448530650533\n",
      "epoch 1, it 130000 validation loss -0.325\n",
      "iteration 130100 training loss 1.0791941 lr 0.00053\n",
      "iteration 130200 training loss 1.1196525 lr 0.00053\n",
      "iteration 130300 training loss 1.1588246 lr 0.00053\n",
      "iteration 130400 training loss 1.0748633 lr 0.00053\n",
      "iteration 130500 training loss 1.0571122 lr 0.00053\n",
      "iteration 130600 training loss 0.97115904 lr 0.00053\n",
      "iteration 130700 training loss 1.1714216 lr 0.00053\n",
      "iteration 130800 training loss 0.958109 lr 0.00053\n",
      "iteration 130900 training loss 0.7992081 lr 0.00053\n",
      "iteration 131000 training loss 1.0325387 lr 0.00053\n",
      "iteration 131100 training loss 0.7724124 lr 0.00053\n",
      "iteration 131200 training loss 1.0289617 lr 0.00053\n",
      "iteration 131300 training loss 0.6322218 lr 0.00053\n",
      "iteration 131400 training loss 0.71002257 lr 0.00053\n",
      "iteration 131500 training loss 0.62919587 lr 0.00053\n",
      "iteration 131600 training loss 0.73727775 lr 0.00053\n",
      "iteration 131700 training loss 0.8239498 lr 0.00053\n",
      "iteration 131800 training loss 0.8587961 lr 0.00053\n",
      "iteration 131900 training loss 0.7290351 lr 0.00053\n",
      "iteration 132000 training loss 0.6571291 lr 0.00053\n",
      "iteration 132100 training loss 0.8114963 lr 0.00053\n",
      "iteration 132200 training loss 0.6818414 lr 0.00053\n",
      "iteration 132300 training loss 0.95431983 lr 0.00053\n",
      "iteration 132400 training loss 0.92889684 lr 0.00053\n",
      "iteration 132500 training loss 0.77783096 lr 0.00053\n",
      "iteration 132600 training loss 0.7087242 lr 0.00053\n",
      "iteration 132700 training loss 0.756347 lr 0.00053\n",
      "iteration 132800 training loss 0.66901755 lr 0.00053\n",
      "iteration 132900 training loss 0.8846258 lr 0.00053\n",
      "iteration 133000 training loss 0.8658183 lr 0.00053\n",
      "iteration 133100 training loss 0.8011463 lr 0.00053\n",
      "iteration 133200 training loss 0.8184566 lr 0.00053\n",
      "iteration 133300 training loss 0.6851949 lr 0.00053\n",
      "iteration 133400 training loss 0.9939884 lr 0.00053\n",
      "iteration 133500 training loss 0.7628118 lr 0.00053\n",
      "iteration 133600 training loss 0.8048125 lr 0.00053\n",
      "iteration 133700 training loss 0.69796306 lr 0.00053\n",
      "iteration 133800 training loss 0.928769 lr 0.00053\n",
      "iteration 133900 training loss 0.61063737 lr 0.00053\n",
      "iteration 134000 training loss 0.5326581 lr 0.00053\n",
      "iteration 134100 training loss 0.6396445 lr 0.00053\n",
      "iteration 134200 training loss 0.57159317 lr 0.00053\n",
      "iteration 134300 training loss 0.6202136 lr 0.00053\n",
      "iteration 134400 training loss 0.41940898 lr 0.00053\n",
      "iteration 134500 training loss 0.6244632 lr 0.00053\n",
      "iteration 134600 training loss 0.47480902 lr 0.00053\n",
      "iteration 134700 training loss 0.7411994 lr 0.00053\n",
      "iteration 134800 training loss 0.59134865 lr 0.00053\n",
      "iteration 134900 training loss 0.4596236 lr 0.00053\n",
      "iteration 135000 training loss 0.38793042 lr 0.00053\n",
      "iteration 135100 training loss 0.4567645 lr 0.00053\n",
      "iteration 135200 training loss 0.3091353 lr 0.00053\n",
      "iteration 135300 training loss 0.33935577 lr 0.00053\n",
      "iteration 135400 training loss 0.73055035 lr 0.00053\n",
      "iteration 135500 training loss 0.6468116 lr 0.00053\n",
      "iteration 135600 training loss 0.7494152 lr 0.00053\n",
      "iteration 135700 training loss 0.5314703 lr 0.00053\n",
      "iteration 135800 training loss 0.7243965 lr 0.00053\n",
      "iteration 135900 training loss 0.6351134 lr 0.00053\n",
      "iteration 136000 training loss 0.6601066 lr 0.00053\n",
      "iteration 136100 training loss 0.7580607 lr 0.00053\n",
      "iteration 136200 training loss 0.70646095 lr 0.00053\n",
      "iteration 136300 training loss 1.009501 lr 0.00053\n",
      "iteration 136400 training loss 0.7999924 lr 0.00053\n",
      "iteration 136500 training loss 0.5783786 lr 0.00053\n",
      "iteration 136600 training loss 0.61629885 lr 0.00053\n",
      "iteration 136700 training loss 0.53747493 lr 0.00053\n",
      "iteration 136800 training loss 0.5323287 lr 0.00053\n",
      "iteration 136900 training loss 0.69187045 lr 0.00053\n",
      "iteration 137000 training loss 0.5492313 lr 0.00053\n",
      "iteration 137100 training loss 0.6151225 lr 0.00053\n",
      "iteration 137200 training loss 0.6341649 lr 0.00053\n",
      "iteration 137300 training loss 0.7922027 lr 0.00053\n",
      "iteration 137400 training loss 0.65539885 lr 0.00053\n",
      "iteration 137500 training loss 0.6476732 lr 0.00053\n",
      "iteration 137600 training loss 0.8972671 lr 0.00053\n",
      "iteration 137700 training loss 0.5015544 lr 0.00053\n",
      "iteration 137800 training loss 0.51189035 lr 0.00053\n",
      "iteration 137900 training loss 0.5399922 lr 0.00053\n",
      "iteration 138000 training loss 0.3541077 lr 0.00053\n",
      "iteration 138100 training loss 0.568783 lr 0.00053\n",
      "iteration 138200 training loss 0.4977819 lr 0.00053\n",
      "iteration 138300 training loss 0.71134347 lr 0.00053\n",
      "iteration 138400 training loss 0.6412418 lr 0.00053\n",
      "iteration 138500 training loss 0.4721576 lr 0.00053\n",
      "iteration 138600 training loss 0.47429278 lr 0.00053\n",
      "iteration 138700 training loss 0.4858744 lr 0.00053\n",
      "iteration 138800 training loss 0.66692835 lr 0.00053\n",
      "iteration 138900 training loss 0.7956799 lr 0.00053\n",
      "iteration 139000 training loss 0.6939622 lr 0.00053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 139100 training loss 0.620883 lr 0.00053\n",
      "iteration 139200 training loss 0.85622185 lr 0.00053\n",
      "iteration 139300 training loss 1.0134847 lr 0.00053\n",
      "iteration 139400 training loss 0.8456107 lr 0.00053\n",
      "iteration 139500 training loss 0.7531934 lr 0.00053\n",
      "iteration 139600 training loss 0.8822736 lr 0.00053\n",
      "iteration 139700 training loss 0.6510089 lr 0.00053\n",
      "iteration 139800 training loss 0.7146712 lr 0.00053\n",
      "iteration 139900 training loss 1.073248 lr 0.00053\n",
      "iteration 140000 training loss 0.69910073 lr 0.00053\n",
      "layout:xla:default 0.006997481744431945\n",
      "layout:xla:random 0.20612176563213933\n",
      "layout:nlp:default 0.33642181392873927\n",
      "layout:nlp:random 0.6841123358148133\n",
      "epoch 1, it 140000 validation loss -0.308\n",
      "iteration 140100 training loss 0.7905865 lr 0.00048\n",
      "iteration 140200 training loss 0.6457756 lr 0.00048\n",
      "iteration 140300 training loss 0.5932107 lr 0.00048\n",
      "iteration 140400 training loss 0.5129554 lr 0.00048\n",
      "iteration 140500 training loss 0.6124385 lr 0.00048\n",
      "iteration 140600 training loss 0.7049218 lr 0.00048\n",
      "iteration 140700 training loss 0.86098397 lr 0.00048\n",
      "iteration 140800 training loss 0.7092693 lr 0.00048\n",
      "iteration 140900 training loss 0.7510937 lr 0.00048\n",
      "iteration 141000 training loss 0.90244126 lr 0.00048\n",
      "iteration 141100 training loss 0.90220124 lr 0.00048\n",
      "iteration 141200 training loss 0.7010021 lr 0.00048\n",
      "iteration 141300 training loss 0.5874139 lr 0.00048\n",
      "iteration 141400 training loss 0.7541813 lr 0.00048\n",
      "iteration 141500 training loss 0.91166997 lr 0.00048\n",
      "iteration 141600 training loss 1.1214541 lr 0.00048\n",
      "iteration 141700 training loss 0.83390677 lr 0.00048\n",
      "iteration 141800 training loss 0.8484998 lr 0.00048\n",
      "iteration 141900 training loss 0.80640846 lr 0.00048\n",
      "iteration 142000 training loss 0.8539962 lr 0.00048\n",
      "iteration 142100 training loss 0.8179002 lr 0.00048\n",
      "iteration 142200 training loss 0.85222614 lr 0.00048\n",
      "iteration 142300 training loss 0.93196976 lr 0.00048\n",
      "iteration 142400 training loss 0.81649977 lr 0.00048\n",
      "iteration 142500 training loss 0.7904034 lr 0.00048\n",
      "iteration 142600 training loss 0.89862233 lr 0.00048\n",
      "iteration 142700 training loss 0.90294534 lr 0.00048\n",
      "iteration 142800 training loss 1.0985084 lr 0.00048\n",
      "iteration 142900 training loss 0.8828195 lr 0.00048\n",
      "iteration 143000 training loss 0.72725236 lr 0.00048\n",
      "iteration 143100 training loss 0.81747025 lr 0.00048\n",
      "iteration 143200 training loss 1.1241319 lr 0.00048\n",
      "iteration 143300 training loss 0.86159015 lr 0.00048\n",
      "iteration 143400 training loss 0.92201793 lr 0.00048\n",
      "iteration 143500 training loss 0.8843555 lr 0.00048\n",
      "iteration 143600 training loss 0.8990819 lr 0.00048\n",
      "iteration 143700 training loss 0.828057 lr 0.00048\n",
      "iteration 143800 training loss 0.71766037 lr 0.00048\n",
      "iteration 143900 training loss 0.66482335 lr 0.00048\n",
      "iteration 144000 training loss 0.862371 lr 0.00048\n",
      "iteration 144100 training loss 0.5652424 lr 0.00048\n",
      "iteration 144200 training loss 0.8182994 lr 0.00048\n",
      "iteration 144300 training loss 0.6968653 lr 0.00048\n",
      "iteration 144400 training loss 0.7145681 lr 0.00048\n",
      "iteration 144500 training loss 0.517856 lr 0.00048\n",
      "iteration 144600 training loss 0.763619 lr 0.00048\n",
      "iteration 144700 training loss 0.76742303 lr 0.00048\n",
      "iteration 144800 training loss 0.6335274 lr 0.00048\n",
      "iteration 144900 training loss 0.9207319 lr 0.00048\n",
      "iteration 145000 training loss 0.57417274 lr 0.00048\n",
      "iteration 145100 training loss 1.0413859 lr 0.00048\n",
      "iteration 145200 training loss 1.0113282 lr 0.00048\n",
      "iteration 145300 training loss 0.75523144 lr 0.00048\n",
      "iteration 145400 training loss 1.0687783 lr 0.00048\n",
      "iteration 145500 training loss 0.92212945 lr 0.00048\n",
      "iteration 145600 training loss 0.6324865 lr 0.00048\n",
      "iteration 145700 training loss 0.73875326 lr 0.00048\n",
      "iteration 145800 training loss 0.6785562 lr 0.00048\n",
      "iteration 145900 training loss 0.8916855 lr 0.00048\n",
      "iteration 146000 training loss 0.7555727 lr 0.00048\n",
      "iteration 146100 training loss 0.6578644 lr 0.00048\n",
      "iteration 146200 training loss 0.7682159 lr 0.00048\n",
      "iteration 146300 training loss 0.5844309 lr 0.00048\n",
      "iteration 146400 training loss 0.6827418 lr 0.00048\n",
      "iteration 146500 training loss 0.7449621 lr 0.00048\n",
      "iteration 146600 training loss 0.73636293 lr 0.00048\n",
      "iteration 146700 training loss 0.50161517 lr 0.00048\n",
      "iteration 146800 training loss 0.7111286 lr 0.00048\n",
      "iteration 146900 training loss 0.59033656 lr 0.00048\n",
      "iteration 147000 training loss 0.5737024 lr 0.00048\n",
      "iteration 147100 training loss 0.7260695 lr 0.00048\n",
      "iteration 147200 training loss 0.95117867 lr 0.00048\n",
      "iteration 147300 training loss 0.536504 lr 0.00048\n",
      "iteration 147400 training loss 0.7313515 lr 0.00048\n",
      "iteration 147500 training loss 0.5554287 lr 0.00048\n",
      "iteration 147600 training loss 0.77529687 lr 0.00048\n",
      "iteration 147700 training loss 0.64052147 lr 0.00048\n",
      "iteration 147800 training loss 0.5437971 lr 0.00048\n",
      "iteration 147900 training loss 0.52560127 lr 0.00048\n",
      "iteration 148000 training loss 0.6703771 lr 0.00048\n",
      "iteration 148100 training loss 0.44252068 lr 0.00048\n",
      "iteration 148200 training loss 0.53583264 lr 0.00048\n",
      "iteration 148300 training loss 0.7285291 lr 0.00048\n",
      "iteration 148400 training loss 0.5240443 lr 0.00048\n",
      "iteration 148500 training loss 0.80917054 lr 0.00048\n",
      "iteration 148600 training loss 0.7413657 lr 0.00048\n",
      "iteration 148700 training loss 0.52182084 lr 0.00048\n",
      "iteration 148800 training loss 0.70430344 lr 0.00048\n",
      "iteration 148900 training loss 0.65545946 lr 0.00048\n",
      "iteration 149000 training loss 0.9607539 lr 0.00048\n",
      "iteration 149100 training loss 0.67664075 lr 0.00048\n",
      "iteration 149200 training loss 0.5920907 lr 0.00048\n",
      "iteration 149300 training loss 0.677568 lr 0.00048\n",
      "iteration 149400 training loss 0.522165 lr 0.00048\n",
      "iteration 149500 training loss 0.62480736 lr 0.00048\n",
      "iteration 149600 training loss 0.37258247 lr 0.00048\n",
      "iteration 149700 training loss 0.59376895 lr 0.00048\n",
      "iteration 149800 training loss 0.53639734 lr 0.00048\n",
      "iteration 149900 training loss 0.46493152 lr 0.00048\n",
      "iteration 150000 training loss 0.48583156 lr 0.00048\n",
      "layout:xla:default -0.0029788801687355504\n",
      "layout:nlp:default 0.3637309742768553\n",
      "layout:xla:random 0.22462383604852976\n",
      "layout:nlp:random 0.7400332796937765\n",
      "epoch 1, it 150000 validation loss -0.331\n",
      "iteration 150100 training loss 0.4608292 lr 0.00048\n",
      "iteration 150200 training loss 0.3833651 lr 0.00048\n",
      "iteration 150300 training loss 0.5667459 lr 0.00048\n",
      "iteration 150400 training loss 0.56834614 lr 0.00048\n",
      "iteration 150500 training loss 0.40638232 lr 0.00048\n",
      "iteration 150600 training loss 0.671698 lr 0.00048\n",
      "iteration 150700 training loss 0.37848732 lr 0.00048\n",
      "iteration 150800 training loss 0.33577472 lr 0.00048\n",
      "iteration 150900 training loss 0.4398894 lr 0.00048\n",
      "iteration 151000 training loss 0.7644154 lr 0.00048\n",
      "iteration 151100 training loss 0.7932 lr 0.00048\n",
      "iteration 151200 training loss 0.5847123 lr 0.00048\n",
      "iteration 151300 training loss 0.58259475 lr 0.00048\n",
      "iteration 151400 training loss 0.830052 lr 0.00048\n",
      "iteration 151500 training loss 0.9476486 lr 0.00048\n",
      "iteration 151600 training loss 0.88780004 lr 0.00048\n",
      "iteration 151700 training loss 0.69142777 lr 0.00048\n",
      "iteration 151800 training loss 0.8519286 lr 0.00048\n",
      "iteration 151900 training loss 0.7923579 lr 0.00048\n",
      "iteration 152000 training loss 0.77430546 lr 0.00048\n",
      "iteration 152100 training loss 1.1192966 lr 0.00048\n",
      "iteration 152200 training loss 0.9359999 lr 0.00048\n",
      "iteration 152300 training loss 0.9293138 lr 0.00048\n",
      "iteration 152400 training loss 0.9832132 lr 0.00048\n",
      "iteration 152500 training loss 1.0177066 lr 0.00048\n",
      "iteration 152600 training loss 1.1541542 lr 0.00048\n",
      "iteration 152700 training loss 0.82719785 lr 0.00048\n",
      "iteration 152800 training loss 0.987907 lr 0.00048\n",
      "iteration 152900 training loss 0.68121916 lr 0.00048\n",
      "iteration 153000 training loss 1.0237423 lr 0.00048\n",
      "iteration 153100 training loss 0.9591217 lr 0.00048\n",
      "iteration 153200 training loss 0.88456607 lr 0.00048\n",
      "iteration 153300 training loss 0.81471795 lr 0.00048\n",
      "iteration 153400 training loss 0.6094269 lr 0.00048\n",
      "iteration 153500 training loss 1.0847888 lr 0.00048\n",
      "iteration 153600 training loss 0.7878021 lr 0.00048\n",
      "iteration 153700 training loss 0.9057574 lr 0.00048\n",
      "iteration 153800 training loss 0.8121866 lr 0.00048\n",
      "iteration 153900 training loss 0.7206445 lr 0.00048\n",
      "iteration 154000 training loss 0.7281968 lr 0.00048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 154100 training loss 0.5291928 lr 0.00048\n",
      "iteration 154200 training loss 0.72572964 lr 0.00048\n",
      "iteration 154300 training loss 0.48500642 lr 0.00048\n",
      "iteration 154400 training loss 0.5737839 lr 0.00048\n",
      "iteration 154500 training loss 0.7747383 lr 0.00048\n",
      "iteration 154600 training loss 0.5359932 lr 0.00048\n",
      "iteration 154700 training loss 0.49652243 lr 0.00048\n",
      "iteration 154800 training loss 0.5414078 lr 0.00048\n",
      "iteration 154900 training loss 0.854621 lr 0.00048\n",
      "iteration 155000 training loss 0.5923351 lr 0.00048\n",
      "iteration 155100 training loss 0.59676886 lr 0.00048\n",
      "iteration 155200 training loss 0.6250789 lr 0.00048\n",
      "iteration 155300 training loss 0.57902896 lr 0.00048\n",
      "iteration 155400 training loss 0.6893695 lr 0.00048\n",
      "iteration 155500 training loss 0.72483027 lr 0.00048\n",
      "iteration 155600 training loss 0.62941086 lr 0.00048\n",
      "iteration 155700 training loss 0.48913136 lr 0.00048\n",
      "iteration 155800 training loss 0.81276155 lr 0.00048\n",
      "iteration 155900 training loss 0.6956477 lr 0.00048\n",
      "iteration 156000 training loss 0.5005669 lr 0.00048\n",
      "iteration 156100 training loss 0.6160787 lr 0.00048\n",
      "iteration 156200 training loss 0.7148093 lr 0.00048\n",
      "iteration 156300 training loss 0.46609563 lr 0.00048\n",
      "iteration 156400 training loss 0.52149296 lr 0.00048\n",
      "iteration 156500 training loss 0.437696 lr 0.00048\n",
      "iteration 156600 training loss 0.40176365 lr 0.00048\n",
      "iteration 156700 training loss 0.32854795 lr 0.00048\n",
      "iteration 156800 training loss 0.36360723 lr 0.00048\n",
      "iteration 156900 training loss 0.29944345 lr 0.00048\n",
      "iteration 157000 training loss 0.29325923 lr 0.00048\n",
      "iteration 157100 training loss 0.7388614 lr 0.00048\n",
      "iteration 157200 training loss 0.44389474 lr 0.00048\n",
      "iteration 157300 training loss 0.5485 lr 0.00048\n",
      "iteration 157400 training loss 0.700793 lr 0.00048\n",
      "iteration 157500 training loss 0.57616115 lr 0.00048\n",
      "iteration 157600 training loss 0.40337363 lr 0.00048\n",
      "iteration 157700 training loss 0.47803435 lr 0.00048\n",
      "iteration 157800 training loss 0.58638644 lr 0.00048\n",
      "iteration 157900 training loss 0.5993998 lr 0.00048\n",
      "iteration 158000 training loss 0.6819365 lr 0.00048\n",
      "iteration 158100 training loss 0.40631264 lr 0.00048\n",
      "iteration 158200 training loss 0.4822924 lr 0.00048\n",
      "iteration 158300 training loss 0.72114176 lr 0.00048\n",
      "iteration 158400 training loss 0.6499605 lr 0.00048\n",
      "iteration 158500 training loss 0.48451236 lr 0.00048\n",
      "iteration 158600 training loss 0.48385984 lr 0.00048\n",
      "iteration 158700 training loss 1.1067165 lr 0.00048\n",
      "iteration 158800 training loss 1.1848813 lr 0.00048\n",
      "iteration 158900 training loss 0.8311967 lr 0.00048\n",
      "iteration 159000 training loss 0.9980752 lr 0.00048\n",
      "iteration 159100 training loss 0.5882389 lr 0.00048\n",
      "iteration 159200 training loss 0.78529704 lr 0.00048\n",
      "iteration 159300 training loss 0.84428144 lr 0.00048\n",
      "iteration 159400 training loss 0.67787963 lr 0.00048\n",
      "iteration 159500 training loss 0.8648321 lr 0.00048\n",
      "iteration 159600 training loss 1.0125544 lr 0.00048\n",
      "iteration 159700 training loss 0.9637199 lr 0.00048\n",
      "iteration 159800 training loss 0.9605674 lr 0.00048\n",
      "iteration 159900 training loss 0.9110884 lr 0.00048\n",
      "iteration 160000 training loss 0.83159626 lr 0.00048\n",
      "layout:nlp:default 0.37469563326072336\n",
      "layout:xla:random 0.2536596767720839\n",
      "layout:xla:default 0.02807361499574674\n",
      "layout:nlp:random 0.729813748815028\n",
      "epoch 2, it 160000 validation loss -0.347\n",
      "iteration 160100 training loss 0.93451893 lr 0.00043\n",
      "iteration 160200 training loss 1.0625749 lr 0.00043\n",
      "iteration 160300 training loss 1.1275742 lr 0.00043\n",
      "iteration 160400 training loss 1.0903023 lr 0.00043\n",
      "iteration 160500 training loss 1.1631871 lr 0.00043\n",
      "iteration 160600 training loss 0.8154364 lr 0.00043\n",
      "iteration 160700 training loss 0.7365906 lr 0.00043\n",
      "iteration 160800 training loss 0.8723126 lr 0.00043\n",
      "iteration 160900 training loss 0.8290203 lr 0.00043\n",
      "iteration 161000 training loss 0.59015745 lr 0.00043\n",
      "iteration 161100 training loss 0.9121983 lr 0.00043\n",
      "iteration 161200 training loss 0.9705095 lr 0.00043\n",
      "iteration 161300 training loss 1.0869 lr 0.00043\n",
      "iteration 161400 training loss 0.6820246 lr 0.00043\n",
      "iteration 161500 training loss 0.86614436 lr 0.00043\n",
      "iteration 161600 training loss 0.94374084 lr 0.00043\n",
      "iteration 161700 training loss 0.6779638 lr 0.00043\n",
      "iteration 161800 training loss 0.7420977 lr 0.00043\n",
      "iteration 161900 training loss 0.6909222 lr 0.00043\n",
      "iteration 162000 training loss 0.86025184 lr 0.00043\n",
      "iteration 162100 training loss 0.887825 lr 0.00043\n",
      "iteration 162200 training loss 0.64368504 lr 0.00043\n",
      "iteration 162300 training loss 0.5342466 lr 0.00043\n",
      "iteration 162400 training loss 0.7954896 lr 0.00043\n",
      "iteration 162500 training loss 0.8928595 lr 0.00043\n",
      "iteration 162600 training loss 0.67659163 lr 0.00043\n",
      "iteration 162700 training loss 0.47458935 lr 0.00043\n",
      "iteration 162800 training loss 0.8638708 lr 0.00043\n",
      "iteration 162900 training loss 0.6131611 lr 0.00043\n",
      "iteration 163000 training loss 0.6789697 lr 0.00043\n",
      "iteration 163100 training loss 0.52028775 lr 0.00043\n",
      "iteration 163200 training loss 0.49394435 lr 0.00043\n",
      "iteration 163300 training loss 0.58348846 lr 0.00043\n",
      "iteration 163400 training loss 0.8551125 lr 0.00043\n",
      "iteration 163500 training loss 1.0773524 lr 0.00043\n",
      "iteration 163600 training loss 0.90925074 lr 0.00043\n",
      "iteration 163700 training loss 0.5721893 lr 0.00043\n",
      "iteration 163800 training loss 0.643167 lr 0.00043\n",
      "iteration 163900 training loss 0.6465736 lr 0.00043\n",
      "iteration 164000 training loss 0.82940036 lr 0.00043\n",
      "iteration 164100 training loss 0.755023 lr 0.00043\n",
      "iteration 164200 training loss 0.64348406 lr 0.00043\n",
      "iteration 164300 training loss 0.59211564 lr 0.00043\n",
      "iteration 164400 training loss 0.87647283 lr 0.00043\n",
      "iteration 164500 training loss 0.7908827 lr 0.00043\n",
      "iteration 164600 training loss 0.52528113 lr 0.00043\n",
      "iteration 164700 training loss 0.6915918 lr 0.00043\n",
      "iteration 164800 training loss 0.84382814 lr 0.00043\n",
      "iteration 164900 training loss 1.2532026 lr 0.00043\n",
      "iteration 165000 training loss 0.8290095 lr 0.00043\n",
      "iteration 165100 training loss 0.8907854 lr 0.00043\n",
      "iteration 165200 training loss 0.67813945 lr 0.00043\n",
      "iteration 165300 training loss 0.8050446 lr 0.00043\n",
      "iteration 165400 training loss 0.68616426 lr 0.00043\n",
      "iteration 165500 training loss 0.973431 lr 0.00043\n",
      "iteration 165600 training loss 0.7152955 lr 0.00043\n",
      "iteration 165700 training loss 0.7048645 lr 0.00043\n",
      "iteration 165800 training loss 0.75905126 lr 0.00043\n",
      "iteration 165900 training loss 0.69833946 lr 0.00043\n",
      "iteration 166000 training loss 1.0939091 lr 0.00043\n",
      "iteration 166100 training loss 0.74388015 lr 0.00043\n",
      "iteration 166200 training loss 0.82320756 lr 0.00043\n",
      "iteration 166300 training loss 0.74733627 lr 0.00043\n",
      "iteration 166400 training loss 0.8077056 lr 0.00043\n",
      "iteration 166500 training loss 1.0828135 lr 0.00043\n",
      "iteration 166600 training loss 0.7363008 lr 0.00043\n",
      "iteration 166700 training loss 0.8816769 lr 0.00043\n",
      "iteration 166800 training loss 0.9745851 lr 0.00043\n",
      "iteration 166900 training loss 0.96091074 lr 0.00043\n",
      "iteration 167000 training loss 0.8448268 lr 0.00043\n",
      "iteration 167100 training loss 0.92416984 lr 0.00043\n",
      "iteration 167200 training loss 0.9962528 lr 0.00043\n",
      "iteration 167300 training loss 0.90557045 lr 0.00043\n",
      "iteration 167400 training loss 0.6976576 lr 0.00043\n",
      "iteration 167500 training loss 0.8930627 lr 0.00043\n",
      "iteration 167600 training loss 0.7012662 lr 0.00043\n",
      "iteration 167700 training loss 0.6926756 lr 0.00043\n",
      "iteration 167800 training loss 0.98667485 lr 0.00043\n",
      "iteration 167900 training loss 0.595369 lr 0.00043\n",
      "iteration 168000 training loss 0.7570574 lr 0.00043\n",
      "iteration 168100 training loss 1.0437502 lr 0.00043\n",
      "iteration 168200 training loss 1.0058136 lr 0.00043\n",
      "iteration 168300 training loss 0.63580525 lr 0.00043\n",
      "iteration 168400 training loss 0.69023883 lr 0.00043\n",
      "iteration 168500 training loss 0.78920543 lr 0.00043\n",
      "iteration 168600 training loss 0.6537253 lr 0.00043\n",
      "iteration 168700 training loss 0.9225948 lr 0.00043\n",
      "iteration 168800 training loss 0.89578706 lr 0.00043\n",
      "iteration 168900 training loss 0.45906332 lr 0.00043\n",
      "iteration 169000 training loss 0.70254254 lr 0.00043\n",
      "iteration 169100 training loss 0.6734845 lr 0.00043\n",
      "iteration 169200 training loss 0.5192896 lr 0.00043\n",
      "iteration 169300 training loss 0.7201526 lr 0.00043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 169400 training loss 0.4533777 lr 0.00043\n",
      "iteration 169500 training loss 0.600724 lr 0.00043\n",
      "iteration 169600 training loss 0.89100873 lr 0.00043\n",
      "iteration 169700 training loss 0.4923465 lr 0.00043\n",
      "iteration 169800 training loss 0.58770263 lr 0.00043\n",
      "iteration 169900 training loss 0.5011888 lr 0.00043\n",
      "iteration 170000 training loss 0.38898703 lr 0.00043\n",
      "layout:xla:default 0.06426013729603466\n",
      "layout:xla:random 0.27860431414160913\n",
      "layout:nlp:random 0.7657287717958485\n",
      "layout:nlp:default 0.3760004204929531\n",
      "epoch 2, it 170000 validation loss -0.371\n",
      "iteration 170100 training loss 0.5164474 lr 0.00043\n",
      "iteration 170200 training loss 0.660647 lr 0.00043\n",
      "iteration 170300 training loss 0.46367708 lr 0.00043\n",
      "iteration 170400 training loss 0.68407863 lr 0.00043\n",
      "iteration 170500 training loss 0.68254167 lr 0.00043\n",
      "iteration 170600 training loss 0.6685724 lr 0.00043\n",
      "iteration 170700 training loss 0.7356078 lr 0.00043\n",
      "iteration 170800 training loss 0.9913512 lr 0.00043\n",
      "iteration 170900 training loss 0.7298737 lr 0.00043\n",
      "iteration 171000 training loss 0.60024935 lr 0.00043\n",
      "iteration 171100 training loss 0.7998027 lr 0.00043\n",
      "iteration 171200 training loss 0.6464242 lr 0.00043\n",
      "iteration 171300 training loss 0.64329976 lr 0.00043\n",
      "iteration 171400 training loss 0.7584182 lr 0.00043\n",
      "iteration 171500 training loss 0.8771682 lr 0.00043\n",
      "iteration 171600 training loss 0.8575578 lr 0.00043\n",
      "iteration 171700 training loss 0.80229086 lr 0.00043\n",
      "iteration 171800 training loss 0.76531845 lr 0.00043\n",
      "iteration 171900 training loss 0.7961629 lr 0.00043\n",
      "iteration 172000 training loss 0.7116021 lr 0.00043\n",
      "iteration 172100 training loss 0.82791287 lr 0.00043\n",
      "iteration 172200 training loss 0.5318677 lr 0.00043\n",
      "iteration 172300 training loss 0.5262463 lr 0.00043\n",
      "iteration 172400 training loss 0.6878514 lr 0.00043\n",
      "iteration 172500 training loss 0.6300769 lr 0.00043\n",
      "iteration 172600 training loss 0.8913174 lr 0.00043\n",
      "iteration 172700 training loss 0.7315236 lr 0.00043\n",
      "iteration 172800 training loss 0.94416255 lr 0.00043\n",
      "iteration 172900 training loss 0.8260905 lr 0.00043\n",
      "iteration 173000 training loss 0.81640816 lr 0.00043\n",
      "iteration 173100 training loss 0.8595735 lr 0.00043\n",
      "iteration 173200 training loss 0.7826694 lr 0.00043\n",
      "iteration 173300 training loss 0.97586393 lr 0.00043\n",
      "iteration 173400 training loss 0.75967044 lr 0.00043\n",
      "iteration 173500 training loss 0.6390115 lr 0.00043\n",
      "iteration 173600 training loss 0.6044912 lr 0.00043\n",
      "iteration 173700 training loss 0.5230609 lr 0.00043\n",
      "iteration 173800 training loss 0.5724064 lr 0.00043\n",
      "iteration 173900 training loss 0.69225794 lr 0.00043\n",
      "iteration 174000 training loss 0.5955123 lr 0.00043\n",
      "iteration 174100 training loss 0.655221 lr 0.00043\n",
      "iteration 174200 training loss 0.8593489 lr 0.00043\n",
      "iteration 174300 training loss 0.6367917 lr 0.00043\n",
      "iteration 174400 training loss 0.6984511 lr 0.00043\n",
      "iteration 174500 training loss 0.51217884 lr 0.00043\n",
      "iteration 174600 training loss 0.5132541 lr 0.00043\n",
      "iteration 174700 training loss 0.5825411 lr 0.00043\n",
      "iteration 174800 training loss 0.57500535 lr 0.00043\n",
      "iteration 174900 training loss 0.60940367 lr 0.00043\n",
      "iteration 175000 training loss 0.8826596 lr 0.00043\n",
      "iteration 175100 training loss 0.6470248 lr 0.00043\n",
      "iteration 175200 training loss 0.74725914 lr 0.00043\n",
      "iteration 175300 training loss 0.80085087 lr 0.00043\n",
      "iteration 175400 training loss 0.64080274 lr 0.00043\n",
      "iteration 175500 training loss 0.61192876 lr 0.00043\n",
      "iteration 175600 training loss 0.7682558 lr 0.00043\n",
      "iteration 175700 training loss 0.72227156 lr 0.00043\n",
      "iteration 175800 training loss 0.49588758 lr 0.00043\n",
      "iteration 175900 training loss 0.89154536 lr 0.00043\n",
      "iteration 176000 training loss 0.86734617 lr 0.00043\n",
      "iteration 176100 training loss 0.887416 lr 0.00043\n",
      "iteration 176200 training loss 0.9396961 lr 0.00043\n",
      "iteration 176300 training loss 0.83516866 lr 0.00043\n",
      "iteration 176400 training loss 0.65797365 lr 0.00043\n",
      "iteration 176500 training loss 0.6643296 lr 0.00043\n",
      "iteration 176600 training loss 0.71901935 lr 0.00043\n",
      "iteration 176700 training loss 0.9568173 lr 0.00043\n",
      "iteration 176800 training loss 0.68944186 lr 0.00043\n",
      "iteration 176900 training loss 1.0579733 lr 0.00043\n",
      "iteration 177000 training loss 0.64831805 lr 0.00043\n",
      "iteration 177100 training loss 0.98713803 lr 0.00043\n",
      "iteration 177200 training loss 1.0757792 lr 0.00043\n",
      "iteration 177300 training loss 0.73165256 lr 0.00043\n",
      "iteration 177400 training loss 0.718308 lr 0.00043\n",
      "iteration 177500 training loss 0.98670375 lr 0.00043\n",
      "iteration 177600 training loss 0.778181 lr 0.00043\n",
      "iteration 177700 training loss 0.8645757 lr 0.00043\n",
      "iteration 177800 training loss 0.63365966 lr 0.00043\n",
      "iteration 177900 training loss 0.8239228 lr 0.00043\n",
      "iteration 178000 training loss 0.83503497 lr 0.00043\n",
      "iteration 178100 training loss 0.8144663 lr 0.00043\n",
      "iteration 178200 training loss 0.9582162 lr 0.00043\n",
      "iteration 178300 training loss 0.59212655 lr 0.00043\n",
      "iteration 178400 training loss 0.73607385 lr 0.00043\n",
      "iteration 178500 training loss 0.7337418 lr 0.00043\n",
      "iteration 178600 training loss 0.6591244 lr 0.00043\n",
      "iteration 178700 training loss 0.7012062 lr 0.00043\n",
      "iteration 178800 training loss 0.6639206 lr 0.00043\n",
      "iteration 178900 training loss 0.5923209 lr 0.00043\n",
      "iteration 179000 training loss 1.6146641 lr 0.00043\n",
      "iteration 179100 training loss 0.8471091 lr 0.00043\n",
      "iteration 179200 training loss 0.8276137 lr 0.00043\n",
      "iteration 179300 training loss 0.8651594 lr 0.00043\n",
      "iteration 179400 training loss 0.9384867 lr 0.00043\n",
      "iteration 179500 training loss 0.71102214 lr 0.00043\n",
      "iteration 179600 training loss 0.84706897 lr 0.00043\n",
      "iteration 179700 training loss 0.9922143 lr 0.00043\n",
      "iteration 179800 training loss 0.8753852 lr 0.00043\n",
      "iteration 179900 training loss 0.7332652 lr 0.00043\n",
      "iteration 180000 training loss 0.79649705 lr 0.00043\n",
      "layout:xla:random 0.34552400199003797\n",
      "layout:nlp:default 0.38944499425323287\n",
      "layout:xla:default 0.06269968997306938\n",
      "layout:nlp:random 0.7386756535671607\n",
      "epoch 2, it 180000 validation loss -0.384\n",
      "iteration 180100 training loss 0.68345875 lr 0.00039\n",
      "iteration 180200 training loss 0.7848721 lr 0.00039\n",
      "iteration 180300 training loss 0.61493933 lr 0.00039\n",
      "iteration 180400 training loss 0.89172614 lr 0.00039\n",
      "iteration 180500 training loss 0.7713165 lr 0.00039\n",
      "iteration 180600 training loss 0.65870875 lr 0.00039\n",
      "iteration 180700 training loss 0.6661001 lr 0.00039\n",
      "iteration 180800 training loss 0.8680226 lr 0.00039\n",
      "iteration 180900 training loss 0.70446503 lr 0.00039\n",
      "iteration 181000 training loss 0.6010192 lr 0.00039\n",
      "iteration 181100 training loss 0.7110318 lr 0.00039\n",
      "iteration 181200 training loss 0.535436 lr 0.00039\n",
      "iteration 181300 training loss 0.5865336 lr 0.00039\n",
      "iteration 181400 training loss 0.8962643 lr 0.00039\n",
      "iteration 181500 training loss 0.6532992 lr 0.00039\n",
      "iteration 181600 training loss 0.6487947 lr 0.00039\n",
      "iteration 181700 training loss 0.68355584 lr 0.00039\n",
      "iteration 181800 training loss 0.8258349 lr 0.00039\n",
      "iteration 181900 training loss 0.49020207 lr 0.00039\n",
      "iteration 182000 training loss 0.4651485 lr 0.00039\n",
      "iteration 182100 training loss 0.8243468 lr 0.00039\n",
      "iteration 182200 training loss 0.7560743 lr 0.00039\n",
      "iteration 182300 training loss 0.92034763 lr 0.00039\n",
      "iteration 182400 training loss 0.74129176 lr 0.00039\n",
      "iteration 182500 training loss 0.90430915 lr 0.00039\n",
      "iteration 182600 training loss 0.87873894 lr 0.00039\n",
      "iteration 182700 training loss 0.736792 lr 0.00039\n",
      "iteration 182800 training loss 0.6208475 lr 0.00039\n",
      "iteration 182900 training loss 0.7349027 lr 0.00039\n",
      "iteration 183000 training loss 0.7292554 lr 0.00039\n",
      "iteration 183100 training loss 0.71362156 lr 0.00039\n",
      "iteration 183200 training loss 0.6883797 lr 0.00039\n",
      "iteration 183300 training loss 0.6655434 lr 0.00039\n",
      "iteration 183400 training loss 0.8992785 lr 0.00039\n",
      "iteration 183500 training loss 0.93251497 lr 0.00039\n",
      "iteration 183600 training loss 0.7355646 lr 0.00039\n",
      "iteration 183700 training loss 0.68326527 lr 0.00039\n",
      "iteration 183800 training loss 0.72909445 lr 0.00039\n",
      "iteration 183900 training loss 0.5317538 lr 0.00039\n",
      "iteration 184000 training loss 0.6302605 lr 0.00039\n",
      "iteration 184100 training loss 0.7018576 lr 0.00039\n",
      "iteration 184200 training loss 0.5667205 lr 0.00039\n",
      "iteration 184300 training loss 0.8132621 lr 0.00039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 184400 training loss 0.73896027 lr 0.00039\n",
      "iteration 184500 training loss 0.57107234 lr 0.00039\n",
      "iteration 184600 training loss 1.0400059 lr 0.00039\n",
      "iteration 184700 training loss 0.9576543 lr 0.00039\n",
      "iteration 184800 training loss 0.8893749 lr 0.00039\n",
      "iteration 184900 training loss 0.84963614 lr 0.00039\n",
      "iteration 185000 training loss 0.7391815 lr 0.00039\n",
      "iteration 185100 training loss 0.67058223 lr 0.00039\n",
      "iteration 185200 training loss 0.9754896 lr 0.00039\n",
      "iteration 185300 training loss 0.9998378 lr 0.00039\n",
      "iteration 185400 training loss 0.789979 lr 0.00039\n",
      "iteration 185500 training loss 0.9267201 lr 0.00039\n",
      "iteration 185600 training loss 0.8809005 lr 0.00039\n",
      "iteration 185700 training loss 0.6545888 lr 0.00039\n",
      "iteration 185800 training loss 0.6348638 lr 0.00039\n",
      "iteration 185900 training loss 0.6278574 lr 0.00039\n",
      "iteration 186000 training loss 0.5341334 lr 0.00039\n",
      "iteration 186100 training loss 0.85874736 lr 0.00039\n",
      "iteration 186200 training loss 0.5296581 lr 0.00039\n",
      "iteration 186300 training loss 0.8762273 lr 0.00039\n",
      "iteration 186400 training loss 0.75780284 lr 0.00039\n",
      "iteration 186500 training loss 0.7568131 lr 0.00039\n",
      "iteration 186600 training loss 0.6787418 lr 0.00039\n",
      "iteration 186700 training loss 0.7508343 lr 0.00039\n",
      "iteration 186800 training loss 0.7427575 lr 0.00039\n",
      "iteration 186900 training loss 0.88128906 lr 0.00039\n",
      "iteration 187000 training loss 0.7930537 lr 0.00039\n",
      "iteration 187100 training loss 0.8096692 lr 0.00039\n",
      "iteration 187200 training loss 0.8255154 lr 0.00039\n",
      "iteration 187300 training loss 0.99070305 lr 0.00039\n",
      "iteration 187400 training loss 0.62625086 lr 0.00039\n",
      "iteration 187500 training loss 0.7423191 lr 0.00039\n",
      "iteration 187600 training loss 0.84222853 lr 0.00039\n",
      "iteration 187700 training loss 0.8677384 lr 0.00039\n",
      "iteration 187800 training loss 0.49243075 lr 0.00039\n",
      "iteration 187900 training loss 0.58191746 lr 0.00039\n",
      "iteration 188000 training loss 0.85704696 lr 0.00039\n",
      "iteration 188100 training loss 0.65731627 lr 0.00039\n",
      "iteration 188200 training loss 0.6954408 lr 0.00039\n",
      "iteration 188300 training loss 0.79143065 lr 0.00039\n",
      "iteration 188400 training loss 1.0931851 lr 0.00039\n",
      "iteration 188500 training loss 0.5949966 lr 0.00039\n",
      "iteration 188600 training loss 0.8273828 lr 0.00039\n",
      "iteration 188700 training loss 0.58532125 lr 0.00039\n",
      "iteration 188800 training loss 0.43715766 lr 0.00039\n",
      "iteration 188900 training loss 0.5963759 lr 0.00039\n",
      "iteration 189000 training loss 0.5822774 lr 0.00039\n",
      "iteration 189100 training loss 0.415599 lr 0.00039\n",
      "iteration 189200 training loss 0.8156178 lr 0.00039\n",
      "iteration 189300 training loss 0.8504769 lr 0.00039\n",
      "iteration 189400 training loss 0.6853192 lr 0.00039\n",
      "iteration 189500 training loss 0.7547815 lr 0.00039\n",
      "iteration 189600 training loss 0.7133339 lr 0.00039\n",
      "iteration 189700 training loss 0.7727446 lr 0.00039\n",
      "iteration 189800 training loss 0.5835424 lr 0.00039\n",
      "iteration 189900 training loss 0.9730653 lr 0.00039\n",
      "iteration 190000 training loss 0.85975605 lr 0.00039\n",
      "layout:xla:random 0.27828868834803056\n",
      "layout:nlp:default 0.3539175053855107\n",
      "layout:xla:default 0.07512444408645913\n",
      "layout:nlp:random 0.745908848690077\n",
      "epoch 2, it 190000 validation loss -0.363\n",
      "iteration 190100 training loss 0.7109073 lr 0.00039\n",
      "iteration 190200 training loss 0.42386904 lr 0.00039\n",
      "iteration 190300 training loss 0.6385065 lr 0.00039\n",
      "iteration 190400 training loss 0.7901266 lr 0.00039\n",
      "iteration 190500 training loss 0.47608408 lr 0.00039\n",
      "iteration 190600 training loss 0.6174983 lr 0.00039\n",
      "iteration 190700 training loss 0.70390075 lr 0.00039\n",
      "iteration 190800 training loss 0.58942133 lr 0.00039\n",
      "iteration 190900 training loss 0.8288909 lr 0.00039\n",
      "iteration 191000 training loss 0.85892683 lr 0.00039\n",
      "iteration 191100 training loss 0.77586204 lr 0.00039\n",
      "iteration 191200 training loss 0.6966291 lr 0.00039\n",
      "iteration 191300 training loss 0.58919334 lr 0.00039\n",
      "iteration 191400 training loss 0.34396437 lr 0.00039\n",
      "iteration 191500 training loss 0.6498387 lr 0.00039\n",
      "iteration 191600 training loss 0.66137636 lr 0.00039\n",
      "iteration 191700 training loss 0.6278957 lr 0.00039\n",
      "iteration 191800 training loss 0.6124033 lr 0.00039\n",
      "iteration 191900 training loss 0.6725082 lr 0.00039\n",
      "iteration 192000 training loss 0.6043987 lr 0.00039\n",
      "iteration 192100 training loss 0.7089964 lr 0.00039\n",
      "iteration 192200 training loss 0.80754936 lr 0.00039\n",
      "iteration 192300 training loss 0.529566 lr 0.00039\n",
      "iteration 192400 training loss 0.6176318 lr 0.00039\n",
      "iteration 192500 training loss 0.639094 lr 0.00039\n",
      "iteration 192600 training loss 0.684038 lr 0.00039\n",
      "iteration 192700 training loss 0.70181334 lr 0.00039\n",
      "iteration 192800 training loss 0.50244606 lr 0.00039\n",
      "iteration 192900 training loss 0.5188947 lr 0.00039\n",
      "iteration 193000 training loss 0.87092924 lr 0.00039\n",
      "iteration 193100 training loss 0.68363094 lr 0.00039\n",
      "iteration 193200 training loss 0.75796825 lr 0.00039\n",
      "iteration 193300 training loss 0.5121555 lr 0.00039\n",
      "iteration 193400 training loss 0.46481794 lr 0.00039\n",
      "iteration 193500 training loss 0.53229487 lr 0.00039\n",
      "iteration 193600 training loss 0.7512638 lr 0.00039\n",
      "iteration 193700 training loss 0.7281975 lr 0.00039\n",
      "iteration 193800 training loss 0.7159469 lr 0.00039\n",
      "iteration 193900 training loss 0.8452262 lr 0.00039\n",
      "iteration 194000 training loss 0.7600292 lr 0.00039\n",
      "iteration 194100 training loss 0.9676715 lr 0.00039\n",
      "iteration 194200 training loss 0.8308825 lr 0.00039\n",
      "iteration 194300 training loss 1.1133384 lr 0.00039\n",
      "iteration 194400 training loss 0.9153868 lr 0.00039\n",
      "iteration 194500 training loss 0.6660148 lr 0.00039\n",
      "iteration 194600 training loss 1.0692772 lr 0.00039\n",
      "iteration 194700 training loss 0.80685186 lr 0.00039\n",
      "iteration 194800 training loss 0.46652877 lr 0.00039\n",
      "iteration 194900 training loss 0.64254475 lr 0.00039\n",
      "iteration 195000 training loss 0.7885238 lr 0.00039\n",
      "iteration 195100 training loss 0.60172534 lr 0.00039\n",
      "iteration 195200 training loss 0.7580019 lr 0.00039\n",
      "iteration 195300 training loss 0.6435404 lr 0.00039\n",
      "iteration 195400 training loss 0.5974858 lr 0.00039\n",
      "iteration 195500 training loss 0.76475376 lr 0.00039\n",
      "iteration 195600 training loss 0.46912697 lr 0.00039\n",
      "iteration 195700 training loss 0.6261298 lr 0.00039\n",
      "iteration 195800 training loss 0.60974497 lr 0.00039\n",
      "iteration 195900 training loss 0.8635808 lr 0.00039\n",
      "iteration 196000 training loss 0.60033697 lr 0.00039\n",
      "iteration 196100 training loss 0.7205024 lr 0.00039\n",
      "iteration 196200 training loss 0.69401944 lr 0.00039\n",
      "iteration 196300 training loss 0.70588636 lr 0.00039\n",
      "iteration 196400 training loss 0.7537972 lr 0.00039\n",
      "iteration 196500 training loss 0.6800159 lr 0.00039\n",
      "iteration 196600 training loss 0.8342107 lr 0.00039\n",
      "iteration 196700 training loss 0.8525649 lr 0.00039\n",
      "iteration 196800 training loss 0.9377261 lr 0.00039\n",
      "iteration 196900 training loss 1.0057257 lr 0.00039\n",
      "iteration 197000 training loss 0.68691736 lr 0.00039\n",
      "iteration 197100 training loss 0.8494193 lr 0.00039\n",
      "iteration 197200 training loss 0.74439204 lr 0.00039\n",
      "iteration 197300 training loss 0.9721847 lr 0.00039\n",
      "iteration 197400 training loss 0.7897066 lr 0.00039\n",
      "iteration 197500 training loss 0.6706341 lr 0.00039\n",
      "iteration 197600 training loss 0.86355263 lr 0.00039\n",
      "iteration 197700 training loss 0.8450076 lr 0.00039\n",
      "iteration 197800 training loss 0.8544058 lr 0.00039\n",
      "iteration 197900 training loss 0.7508017 lr 0.00039\n",
      "iteration 198000 training loss 0.70785546 lr 0.00039\n",
      "iteration 198100 training loss 0.79289955 lr 0.00039\n",
      "iteration 198200 training loss 0.5879243 lr 0.00039\n",
      "iteration 198300 training loss 0.8160236 lr 0.00039\n",
      "iteration 198400 training loss 0.8394127 lr 0.00039\n",
      "iteration 198500 training loss 0.80217284 lr 0.00039\n",
      "iteration 198600 training loss 0.72514683 lr 0.00039\n",
      "iteration 198700 training loss 0.89124656 lr 0.00039\n",
      "iteration 198800 training loss 0.5499437 lr 0.00039\n",
      "iteration 198900 training loss 0.86295897 lr 0.00039\n",
      "iteration 199000 training loss 0.81857246 lr 0.00039\n",
      "iteration 199100 training loss 0.74079734 lr 0.00039\n",
      "iteration 199200 training loss 0.7673367 lr 0.00039\n",
      "iteration 199300 training loss 0.7630113 lr 0.00039\n",
      "iteration 199400 training loss 1.0315778 lr 0.00039\n",
      "iteration 199500 training loss 0.93583775 lr 0.00039\n",
      "iteration 199600 training loss 1.0087028 lr 0.00039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 199700 training loss 0.88108057 lr 0.00039\n",
      "iteration 199800 training loss 0.80049956 lr 0.00039\n",
      "iteration 199900 training loss 0.7854478 lr 0.00039\n",
      "iteration 200000 training loss 0.85973316 lr 0.00039\n",
      "layout:xla:random 0.33105144717448776\n",
      "layout:xla:default 0.06216968083782748\n",
      "layout:nlp:random 0.7534337837133155\n",
      "layout:nlp:default 0.3515553002349759\n",
      "epoch 2, it 200000 validation loss -0.375\n",
      "iteration 200100 training loss 0.57784384 lr 0.00035\n",
      "iteration 200200 training loss 0.84343654 lr 0.00035\n",
      "iteration 200300 training loss 0.48030195 lr 0.00035\n",
      "iteration 200400 training loss 0.60680825 lr 0.00035\n",
      "iteration 200500 training loss 0.530398 lr 0.00035\n",
      "iteration 200600 training loss 0.5081761 lr 0.00035\n",
      "iteration 200700 training loss 0.41796282 lr 0.00035\n",
      "iteration 200800 training loss 0.71599555 lr 0.00035\n",
      "iteration 200900 training loss 0.6000643 lr 0.00035\n",
      "iteration 201000 training loss 0.67235804 lr 0.00035\n",
      "iteration 201100 training loss 0.6556431 lr 0.00035\n",
      "iteration 201200 training loss 0.6171642 lr 0.00035\n",
      "iteration 201300 training loss 0.4758025 lr 0.00035\n",
      "iteration 201400 training loss 0.6467496 lr 0.00035\n",
      "iteration 201500 training loss 0.6496942 lr 0.00035\n",
      "iteration 201600 training loss 0.68725616 lr 0.00035\n",
      "iteration 201700 training loss 0.62944853 lr 0.00035\n",
      "iteration 201800 training loss 0.46056813 lr 0.00035\n",
      "iteration 201900 training loss 0.6162011 lr 0.00035\n",
      "iteration 202000 training loss 0.4139517 lr 0.00035\n",
      "iteration 202100 training loss 0.4705756 lr 0.00035\n",
      "iteration 202200 training loss 0.46888202 lr 0.00035\n",
      "iteration 202300 training loss 0.5861677 lr 0.00035\n",
      "iteration 202400 training loss 0.8846874 lr 0.00035\n",
      "iteration 202500 training loss 0.8557603 lr 0.00035\n",
      "iteration 202600 training loss 0.72672856 lr 0.00035\n",
      "iteration 202700 training loss 0.8173364 lr 0.00035\n",
      "iteration 202800 training loss 0.70978045 lr 0.00035\n",
      "iteration 202900 training loss 0.8542835 lr 0.00035\n",
      "iteration 203000 training loss 0.834952 lr 0.00035\n",
      "iteration 203100 training loss 0.5663947 lr 0.00035\n",
      "iteration 203200 training loss 0.8815997 lr 0.00035\n",
      "iteration 203300 training loss 1.0189471 lr 0.00035\n",
      "iteration 203400 training loss 0.9425108 lr 0.00035\n",
      "iteration 203500 training loss 0.81077504 lr 0.00035\n",
      "iteration 203600 training loss 0.9196357 lr 0.00035\n",
      "iteration 203700 training loss 0.8148145 lr 0.00035\n",
      "iteration 203800 training loss 0.771234 lr 0.00035\n",
      "iteration 203900 training loss 0.766189 lr 0.00035\n",
      "iteration 204000 training loss 1.0247271 lr 0.00035\n",
      "iteration 204100 training loss 0.7523676 lr 0.00035\n",
      "iteration 204200 training loss 0.81725496 lr 0.00035\n",
      "iteration 204300 training loss 1.038058 lr 0.00035\n",
      "iteration 204400 training loss 0.8135742 lr 0.00035\n",
      "iteration 204500 training loss 0.86071885 lr 0.00035\n",
      "iteration 204600 training loss 1.0882732 lr 0.00035\n",
      "iteration 204700 training loss 1.1574951 lr 0.00035\n",
      "iteration 204800 training loss 0.9960067 lr 0.00035\n",
      "iteration 204900 training loss 0.68321824 lr 0.00035\n",
      "iteration 205000 training loss 0.89839834 lr 0.00035\n",
      "iteration 205100 training loss 0.90669626 lr 0.00035\n",
      "iteration 205200 training loss 1.079842 lr 0.00035\n",
      "iteration 205300 training loss 1.0503702 lr 0.00035\n",
      "iteration 205400 training loss 1.1717196 lr 0.00035\n",
      "iteration 205500 training loss 0.9659643 lr 0.00035\n",
      "iteration 205600 training loss 0.9902543 lr 0.00035\n",
      "iteration 205700 training loss 0.965224 lr 0.00035\n",
      "iteration 205800 training loss 1.0109204 lr 0.00035\n",
      "iteration 205900 training loss 1.0043778 lr 0.00035\n",
      "iteration 206000 training loss 0.8479918 lr 0.00035\n",
      "iteration 206100 training loss 1.0155184 lr 0.00035\n",
      "iteration 206200 training loss 0.73887664 lr 0.00035\n",
      "iteration 206300 training loss 0.64759636 lr 0.00035\n",
      "iteration 206400 training loss 0.6912993 lr 0.00035\n",
      "iteration 206500 training loss 0.9531283 lr 0.00035\n",
      "iteration 206600 training loss 0.8178252 lr 0.00035\n",
      "iteration 206700 training loss 0.98388517 lr 0.00035\n",
      "iteration 206800 training loss 0.8879356 lr 0.00035\n",
      "iteration 206900 training loss 0.6679892 lr 0.00035\n",
      "iteration 207000 training loss 0.7250909 lr 0.00035\n",
      "iteration 207100 training loss 0.59322304 lr 0.00035\n",
      "iteration 207200 training loss 0.788783 lr 0.00035\n",
      "iteration 207300 training loss 0.56928545 lr 0.00035\n",
      "iteration 207400 training loss 0.62317044 lr 0.00035\n",
      "iteration 207500 training loss 0.54826796 lr 0.00035\n",
      "iteration 207600 training loss 0.63801914 lr 0.00035\n",
      "iteration 207700 training loss 0.64867646 lr 0.00035\n",
      "iteration 207800 training loss 0.6458068 lr 0.00035\n",
      "iteration 207900 training loss 0.75313956 lr 0.00035\n",
      "iteration 208000 training loss 0.825809 lr 0.00035\n",
      "iteration 208100 training loss 0.6260347 lr 0.00035\n",
      "iteration 208200 training loss 0.74841285 lr 0.00035\n",
      "iteration 208300 training loss 0.5835272 lr 0.00035\n",
      "iteration 208400 training loss 0.86771345 lr 0.00035\n",
      "iteration 208500 training loss 0.98594016 lr 0.00035\n",
      "iteration 208600 training loss 0.6246605 lr 0.00035\n",
      "iteration 208700 training loss 0.75567424 lr 0.00035\n",
      "iteration 208800 training loss 0.927863 lr 0.00035\n",
      "iteration 208900 training loss 1.0632781 lr 0.00035\n",
      "iteration 209000 training loss 0.7725393 lr 0.00035\n",
      "iteration 209100 training loss 0.76067764 lr 0.00035\n",
      "iteration 209200 training loss 0.914486 lr 0.00035\n",
      "iteration 209300 training loss 1.1102165 lr 0.00035\n",
      "iteration 209400 training loss 1.0246365 lr 0.00035\n",
      "iteration 209500 training loss 0.7489921 lr 0.00035\n",
      "iteration 209600 training loss 1.0004252 lr 0.00035\n",
      "iteration 209700 training loss 0.86598766 lr 0.00035\n",
      "iteration 209800 training loss 0.82334405 lr 0.00035\n",
      "iteration 209900 training loss 0.95491946 lr 0.00035\n",
      "iteration 210000 training loss 0.90593195 lr 0.00035\n",
      "layout:xla:default 0.07289535412270784\n",
      "layout:nlp:random 0.6960337222019288\n",
      "layout:xla:random 0.3923752197072096\n",
      "layout:nlp:default 0.37371046686696097\n",
      "epoch 2, it 210000 validation loss -0.384\n",
      "iteration 210100 training loss 0.8595114 lr 0.00035\n",
      "iteration 210200 training loss 0.9877729 lr 0.00035\n",
      "iteration 210300 training loss 0.86025745 lr 0.00035\n",
      "iteration 210400 training loss 0.8170179 lr 0.00035\n",
      "iteration 210500 training loss 0.64046264 lr 0.00035\n",
      "iteration 210600 training loss 0.6330038 lr 0.00035\n",
      "iteration 210700 training loss 0.7284939 lr 0.00035\n",
      "iteration 210800 training loss 0.8608314 lr 0.00035\n",
      "iteration 210900 training loss 0.73943675 lr 0.00035\n",
      "iteration 211000 training loss 0.76249546 lr 0.00035\n",
      "iteration 211100 training loss 0.69683284 lr 0.00035\n",
      "iteration 211200 training loss 0.99603724 lr 0.00035\n",
      "iteration 211300 training loss 0.8050887 lr 0.00035\n",
      "iteration 211400 training loss 0.85797757 lr 0.00035\n",
      "iteration 211500 training loss 0.8328848 lr 0.00035\n",
      "iteration 211600 training loss 0.5451438 lr 0.00035\n",
      "iteration 211700 training loss 0.8165214 lr 0.00035\n",
      "iteration 211800 training loss 0.8694526 lr 0.00035\n",
      "iteration 211900 training loss 0.6741514 lr 0.00035\n",
      "iteration 212000 training loss 0.8046577 lr 0.00035\n",
      "iteration 212100 training loss 0.76489353 lr 0.00035\n",
      "iteration 212200 training loss 0.73726106 lr 0.00035\n",
      "iteration 212300 training loss 0.8244533 lr 0.00035\n",
      "iteration 212400 training loss 0.6848728 lr 0.00035\n",
      "iteration 212500 training loss 0.74990964 lr 0.00035\n",
      "iteration 212600 training loss 0.976832 lr 0.00035\n",
      "iteration 212700 training loss 0.8594104 lr 0.00035\n",
      "iteration 212800 training loss 0.6385052 lr 0.00035\n",
      "iteration 212900 training loss 0.6875167 lr 0.00035\n",
      "iteration 213000 training loss 0.48649064 lr 0.00035\n",
      "iteration 213100 training loss 0.52210397 lr 0.00035\n",
      "iteration 213200 training loss 0.587609 lr 0.00035\n",
      "iteration 213300 training loss 0.39071232 lr 0.00035\n",
      "iteration 213400 training loss 0.33210972 lr 0.00035\n",
      "iteration 213500 training loss 0.36050686 lr 0.00035\n",
      "iteration 213600 training loss 0.34406328 lr 0.00035\n",
      "iteration 213700 training loss 0.35659024 lr 0.00035\n",
      "iteration 213800 training loss 0.43096954 lr 0.00035\n",
      "iteration 213900 training loss 0.5900766 lr 0.00035\n",
      "iteration 214000 training loss 0.4750605 lr 0.00035\n",
      "iteration 214100 training loss 0.62371045 lr 0.00035\n",
      "iteration 214200 training loss 0.6450356 lr 0.00035\n",
      "iteration 214300 training loss 0.7573979 lr 0.00035\n",
      "iteration 214400 training loss 0.6684177 lr 0.00035\n",
      "iteration 214500 training loss 0.7118866 lr 0.00035\n",
      "iteration 214600 training loss 0.75049084 lr 0.00035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 214700 training loss 0.6136293 lr 0.00035\n",
      "iteration 214800 training loss 0.88463163 lr 0.00035\n",
      "iteration 214900 training loss 0.7373832 lr 0.00035\n",
      "iteration 215000 training loss 0.6739674 lr 0.00035\n",
      "iteration 215100 training loss 0.6674118 lr 0.00035\n",
      "iteration 215200 training loss 0.39971098 lr 0.00035\n",
      "iteration 215300 training loss 0.616747 lr 0.00035\n",
      "iteration 215400 training loss 0.96906984 lr 0.00035\n",
      "iteration 215500 training loss 0.532159 lr 0.00035\n",
      "iteration 215600 training loss 0.5148783 lr 0.00035\n",
      "iteration 215700 training loss 0.5114762 lr 0.00035\n",
      "iteration 215800 training loss 0.7028774 lr 0.00035\n",
      "iteration 215900 training loss 0.81846684 lr 0.00035\n",
      "iteration 216000 training loss 0.7961555 lr 0.00035\n",
      "iteration 216100 training loss 0.9184693 lr 0.00035\n",
      "iteration 216200 training loss 0.56116295 lr 0.00035\n",
      "iteration 216300 training loss 0.63187844 lr 0.00035\n",
      "iteration 216400 training loss 0.5778483 lr 0.00035\n",
      "iteration 216500 training loss 0.67147607 lr 0.00035\n",
      "iteration 216600 training loss 0.58166784 lr 0.00035\n",
      "iteration 216700 training loss 0.6503359 lr 0.00035\n",
      "iteration 216800 training loss 0.90454483 lr 0.00035\n",
      "iteration 216900 training loss 0.70803463 lr 0.00035\n",
      "iteration 217000 training loss 0.70271224 lr 0.00035\n",
      "iteration 217100 training loss 0.70861304 lr 0.00035\n",
      "iteration 217200 training loss 0.6622871 lr 0.00035\n",
      "iteration 217300 training loss 0.9203827 lr 0.00035\n",
      "iteration 217400 training loss 0.85685253 lr 0.00035\n",
      "iteration 217500 training loss 0.79090846 lr 0.00035\n",
      "iteration 217600 training loss 0.90436584 lr 0.00035\n",
      "iteration 217700 training loss 0.7911158 lr 0.00035\n",
      "iteration 217800 training loss 1.0459627 lr 0.00035\n",
      "iteration 217900 training loss 0.91186357 lr 0.00035\n",
      "iteration 218000 training loss 0.78690255 lr 0.00035\n",
      "iteration 218100 training loss 0.6280962 lr 0.00035\n",
      "iteration 218200 training loss 0.8935685 lr 0.00035\n",
      "iteration 218300 training loss 0.8713085 lr 0.00035\n",
      "iteration 218400 training loss 0.9650583 lr 0.00035\n",
      "iteration 218500 training loss 0.86878157 lr 0.00035\n",
      "iteration 218600 training loss 0.7307053 lr 0.00035\n",
      "iteration 218700 training loss 0.63218594 lr 0.00035\n",
      "iteration 218800 training loss 0.904355 lr 0.00035\n",
      "iteration 218900 training loss 0.44471425 lr 0.00035\n",
      "iteration 219000 training loss 0.7809277 lr 0.00035\n",
      "iteration 219100 training loss 0.8762739 lr 0.00035\n",
      "iteration 219200 training loss 0.7145148 lr 0.00035\n",
      "iteration 219300 training loss 0.569398 lr 0.00035\n",
      "iteration 219400 training loss 0.92572564 lr 0.00035\n",
      "iteration 219500 training loss 0.96705323 lr 0.00035\n",
      "iteration 219600 training loss 0.7495699 lr 0.00035\n",
      "iteration 219700 training loss 0.8464234 lr 0.00035\n",
      "iteration 219800 training loss 1.0006974 lr 0.00035\n",
      "iteration 219900 training loss 1.0451523 lr 0.00035\n",
      "iteration 220000 training loss 0.6835926 lr 0.00035\n",
      "layout:xla:default 0.07747208856853798\n",
      "layout:xla:random 0.38781686831429596\n",
      "layout:nlp:random 0.7042929282295491\n",
      "layout:nlp:default 0.33758386097102455\n",
      "epoch 2, it 220000 validation loss -0.377\n",
      "iteration 220100 training loss 1.111 lr 0.00031\n",
      "iteration 220200 training loss 0.972914 lr 0.00031\n",
      "iteration 220300 training loss 1.0644224 lr 0.00031\n",
      "iteration 220400 training loss 1.5377636 lr 0.00031\n",
      "iteration 220500 training loss 0.5445742 lr 0.00031\n",
      "iteration 220600 training loss 0.7118859 lr 0.00031\n",
      "iteration 220700 training loss 0.67193866 lr 0.00031\n",
      "iteration 220800 training loss 0.70052147 lr 0.00031\n",
      "iteration 220900 training loss 0.8574363 lr 0.00031\n",
      "iteration 221000 training loss 0.8854538 lr 0.00031\n",
      "iteration 221100 training loss 0.94122297 lr 0.00031\n",
      "iteration 221200 training loss 0.79169977 lr 0.00031\n",
      "iteration 221300 training loss 0.9908493 lr 0.00031\n",
      "iteration 221400 training loss 0.81120956 lr 0.00031\n",
      "iteration 221500 training loss 0.7453431 lr 0.00031\n",
      "iteration 221600 training loss 0.86039144 lr 0.00031\n",
      "iteration 221700 training loss 1.077173 lr 0.00031\n",
      "iteration 221800 training loss 1.0125749 lr 0.00031\n",
      "iteration 221900 training loss 0.8074862 lr 0.00031\n",
      "iteration 222000 training loss 1.1019199 lr 0.00031\n",
      "iteration 222100 training loss 0.6521544 lr 0.00031\n",
      "iteration 222200 training loss 1.0103616 lr 0.00031\n",
      "iteration 222300 training loss 0.81672686 lr 0.00031\n",
      "iteration 222400 training loss 1.096318 lr 0.00031\n",
      "iteration 222500 training loss 0.6804377 lr 0.00031\n",
      "iteration 222600 training loss 0.65783226 lr 0.00031\n",
      "iteration 222700 training loss 0.77415985 lr 0.00031\n",
      "iteration 222800 training loss 0.6301523 lr 0.00031\n",
      "iteration 222900 training loss 0.82380253 lr 0.00031\n",
      "iteration 223000 training loss 0.6041059 lr 0.00031\n",
      "iteration 223100 training loss 0.5399376 lr 0.00031\n",
      "iteration 223200 training loss 0.52796936 lr 0.00031\n",
      "iteration 223300 training loss 0.74303854 lr 0.00031\n",
      "iteration 223400 training loss 0.57880276 lr 0.00031\n",
      "iteration 223500 training loss 0.6526071 lr 0.00031\n",
      "iteration 223600 training loss 0.8896717 lr 0.00031\n",
      "iteration 223700 training loss 1.148587 lr 0.00031\n",
      "iteration 223800 training loss 0.9335498 lr 0.00031\n",
      "iteration 223900 training loss 0.8297348 lr 0.00031\n",
      "iteration 224000 training loss 0.85546017 lr 0.00031\n",
      "iteration 224100 training loss 0.75506574 lr 0.00031\n",
      "iteration 224200 training loss 0.5204826 lr 0.00031\n",
      "iteration 224300 training loss 0.7964331 lr 0.00031\n",
      "iteration 224400 training loss 0.6044116 lr 0.00031\n",
      "iteration 224500 training loss 0.7850615 lr 0.00031\n",
      "iteration 224600 training loss 0.6140751 lr 0.00031\n",
      "iteration 224700 training loss 0.7839212 lr 0.00031\n",
      "iteration 224800 training loss 0.6386404 lr 0.00031\n",
      "iteration 224900 training loss 0.86256504 lr 0.00031\n",
      "iteration 225000 training loss 0.643331 lr 0.00031\n",
      "iteration 225100 training loss 0.66623425 lr 0.00031\n",
      "iteration 225200 training loss 0.74801904 lr 0.00031\n",
      "iteration 225300 training loss 0.8108543 lr 0.00031\n",
      "iteration 225400 training loss 0.54069585 lr 0.00031\n",
      "iteration 225500 training loss 0.6775929 lr 0.00031\n",
      "iteration 225600 training loss 0.5649198 lr 0.00031\n",
      "iteration 225700 training loss 0.5903958 lr 0.00031\n",
      "iteration 225800 training loss 0.5197299 lr 0.00031\n",
      "iteration 225900 training loss 0.4208828 lr 0.00031\n",
      "iteration 226000 training loss 0.44625652 lr 0.00031\n",
      "iteration 226100 training loss 0.48661038 lr 0.00031\n",
      "iteration 226200 training loss 0.7587376 lr 0.00031\n",
      "iteration 226300 training loss 0.80638814 lr 0.00031\n",
      "iteration 226400 training loss 0.41243532 lr 0.00031\n",
      "iteration 226500 training loss 0.66574883 lr 0.00031\n",
      "iteration 226600 training loss 0.48471326 lr 0.00031\n",
      "iteration 226700 training loss 0.58724487 lr 0.00031\n",
      "iteration 226800 training loss 0.6776986 lr 0.00031\n",
      "iteration 226900 training loss 0.75736636 lr 0.00031\n",
      "iteration 227000 training loss 0.73353416 lr 0.00031\n",
      "iteration 227100 training loss 0.6173628 lr 0.00031\n",
      "iteration 227200 training loss 0.6423186 lr 0.00031\n",
      "iteration 227300 training loss 0.5589476 lr 0.00031\n",
      "iteration 227400 training loss 0.5985636 lr 0.00031\n",
      "iteration 227500 training loss 0.54049623 lr 0.00031\n",
      "iteration 227600 training loss 0.5887737 lr 0.00031\n",
      "iteration 227700 training loss 0.67440736 lr 0.00031\n",
      "iteration 227800 training loss 0.5576824 lr 0.00031\n",
      "iteration 227900 training loss 0.7320821 lr 0.00031\n",
      "iteration 228000 training loss 0.6066074 lr 0.00031\n",
      "iteration 228100 training loss 0.47366512 lr 0.00031\n",
      "iteration 228200 training loss 0.8294365 lr 0.00031\n",
      "iteration 228300 training loss 0.4764533 lr 0.00031\n",
      "iteration 228400 training loss 0.38910866 lr 0.00031\n",
      "iteration 228500 training loss 0.36440364 lr 0.00031\n",
      "iteration 228600 training loss 0.40118644 lr 0.00031\n",
      "iteration 228700 training loss 0.2935457 lr 0.00031\n",
      "iteration 228800 training loss 0.28286487 lr 0.00031\n",
      "iteration 228900 training loss 0.44665575 lr 0.00031\n",
      "iteration 229000 training loss 0.45307237 lr 0.00031\n",
      "iteration 229100 training loss 0.32574877 lr 0.00031\n",
      "iteration 229200 training loss 0.24360682 lr 0.00031\n",
      "iteration 229300 training loss 0.4497168 lr 0.00031\n",
      "iteration 229400 training loss 0.34329158 lr 0.00031\n",
      "iteration 229500 training loss 1.1403891 lr 0.00031\n",
      "iteration 229600 training loss 0.88788706 lr 0.00031\n",
      "iteration 229700 training loss 0.6975566 lr 0.00031\n",
      "iteration 229800 training loss 0.7635404 lr 0.00031\n",
      "iteration 229900 training loss 0.9714985 lr 0.00031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 230000 training loss 0.8558126 lr 0.00031\n",
      "layout:xla:random 0.2628728252173623\n",
      "layout:xla:default 0.05150931969668439\n",
      "layout:nlp:default 0.34728287131165647\n",
      "layout:nlp:random 0.7194813183614548\n",
      "epoch 2, it 230000 validation loss -0.345\n",
      "iteration 230100 training loss 0.80588824 lr 0.00031\n",
      "iteration 230200 training loss 0.861818 lr 0.00031\n",
      "iteration 230300 training loss 0.96445423 lr 0.00031\n",
      "iteration 230400 training loss 1.107816 lr 0.00031\n",
      "iteration 230500 training loss 0.88081586 lr 0.00031\n",
      "iteration 230600 training loss 1.0236217 lr 0.00031\n",
      "iteration 230700 training loss 0.84255785 lr 0.00031\n",
      "iteration 230800 training loss 0.90252763 lr 0.00031\n",
      "iteration 230900 training loss 0.8552403 lr 0.00031\n",
      "iteration 231000 training loss 0.748129 lr 0.00031\n",
      "iteration 231100 training loss 1.001112 lr 0.00031\n",
      "iteration 231200 training loss 0.9971707 lr 0.00031\n",
      "iteration 231300 training loss 0.9052374 lr 0.00031\n",
      "iteration 231400 training loss 1.1401969 lr 0.00031\n",
      "iteration 231500 training loss 1.021529 lr 0.00031\n",
      "iteration 231600 training loss 0.69901246 lr 0.00031\n",
      "iteration 231700 training loss 1.0241889 lr 0.00031\n",
      "iteration 231800 training loss 0.96361804 lr 0.00031\n",
      "iteration 231900 training loss 1.0283917 lr 0.00031\n",
      "iteration 232000 training loss 1.0138373 lr 0.00031\n",
      "iteration 232100 training loss 0.871148 lr 0.00031\n",
      "iteration 232200 training loss 0.686057 lr 0.00031\n",
      "iteration 232300 training loss 1.0135754 lr 0.00031\n",
      "iteration 232400 training loss 0.9114779 lr 0.00031\n",
      "iteration 232500 training loss 0.70222145 lr 0.00031\n",
      "iteration 232600 training loss 0.6488387 lr 0.00031\n",
      "iteration 232700 training loss 0.7125116 lr 0.00031\n",
      "iteration 232800 training loss 0.3729767 lr 0.00031\n",
      "iteration 232900 training loss 0.53683406 lr 0.00031\n",
      "iteration 233000 training loss 0.8583179 lr 0.00031\n",
      "iteration 233100 training loss 0.7649128 lr 0.00031\n",
      "iteration 233200 training loss 0.6240169 lr 0.00031\n",
      "iteration 233300 training loss 0.8117017 lr 0.00031\n",
      "iteration 233400 training loss 0.6902944 lr 0.00031\n",
      "iteration 233500 training loss 0.75309026 lr 0.00031\n",
      "iteration 233600 training loss 0.864434 lr 0.00031\n",
      "iteration 233700 training loss 0.591879 lr 0.00031\n",
      "iteration 233800 training loss 0.5879592 lr 0.00031\n",
      "iteration 233900 training loss 0.6885146 lr 0.00031\n",
      "iteration 234000 training loss 0.9573092 lr 0.00031\n",
      "iteration 234100 training loss 0.635067 lr 0.00031\n",
      "iteration 234200 training loss 0.91201353 lr 0.00031\n",
      "iteration 234300 training loss 0.7206964 lr 0.00031\n",
      "iteration 234400 training loss 1.0310116 lr 0.00031\n",
      "iteration 234500 training loss 0.8440047 lr 0.00031\n",
      "iteration 234600 training loss 0.82627714 lr 0.00031\n",
      "iteration 234700 training loss 0.62542254 lr 0.00031\n",
      "iteration 234800 training loss 0.6875735 lr 0.00031\n",
      "iteration 234900 training loss 0.6673466 lr 0.00031\n",
      "iteration 235000 training loss 0.76859146 lr 0.00031\n",
      "iteration 235100 training loss 0.44024616 lr 0.00031\n",
      "iteration 235200 training loss 0.3337599 lr 0.00031\n",
      "iteration 235300 training loss 0.41769764 lr 0.00031\n",
      "iteration 235400 training loss 0.3995796 lr 0.00031\n",
      "iteration 235500 training loss 0.38087335 lr 0.00031\n",
      "iteration 235600 training loss 0.24298896 lr 0.00031\n",
      "iteration 235700 training loss 0.5221129 lr 0.00031\n",
      "iteration 235800 training loss 0.63383245 lr 0.00031\n",
      "iteration 235900 training loss 0.637349 lr 0.00031\n",
      "iteration 236000 training loss 0.5969003 lr 0.00031\n",
      "iteration 236100 training loss 0.62751764 lr 0.00031\n",
      "iteration 236200 training loss 0.6960333 lr 0.00031\n",
      "iteration 236300 training loss 0.56972486 lr 0.00031\n",
      "iteration 236400 training loss 0.6271177 lr 0.00031\n",
      "iteration 236500 training loss 0.5691358 lr 0.00031\n",
      "iteration 236600 training loss 0.52776074 lr 0.00031\n",
      "iteration 236700 training loss 0.50801706 lr 0.00031\n",
      "iteration 236800 training loss 0.59264785 lr 0.00031\n",
      "iteration 236900 training loss 0.6191542 lr 0.00031\n",
      "iteration 237000 training loss 0.49133524 lr 0.00031\n",
      "iteration 237100 training loss 0.66429555 lr 0.00031\n",
      "iteration 237200 training loss 0.93925655 lr 0.00031\n",
      "iteration 237300 training loss 0.8821081 lr 0.00031\n",
      "iteration 237400 training loss 0.84378576 lr 0.00031\n",
      "iteration 237500 training loss 0.7899275 lr 0.00031\n",
      "iteration 237600 training loss 0.8155749 lr 0.00031\n",
      "iteration 237700 training loss 0.8522585 lr 0.00031\n",
      "iteration 237800 training loss 0.9241708 lr 0.00031\n",
      "iteration 237900 training loss 0.982467 lr 0.00031\n",
      "iteration 238000 training loss 0.91284823 lr 0.00031\n",
      "iteration 238100 training loss 0.8387451 lr 0.00031\n",
      "iteration 238200 training loss 0.83939314 lr 0.00031\n",
      "iteration 238300 training loss 0.8234094 lr 0.00031\n",
      "iteration 238400 training loss 0.89702064 lr 0.00031\n",
      "iteration 238500 training loss 0.6239224 lr 0.00031\n",
      "iteration 238600 training loss 0.8172074 lr 0.00031\n",
      "iteration 238700 training loss 0.8379546 lr 0.00031\n",
      "iteration 238800 training loss 0.95922196 lr 0.00031\n",
      "iteration 238900 training loss 0.9719754 lr 0.00031\n",
      "iteration 239000 training loss 0.8999436 lr 0.00031\n",
      "iteration 239100 training loss 0.7883884 lr 0.00031\n",
      "iteration 239200 training loss 0.88079035 lr 0.00031\n",
      "iteration 239300 training loss 0.87804943 lr 0.00031\n",
      "iteration 239400 training loss 0.9699391 lr 0.00031\n",
      "iteration 239500 training loss 0.960957 lr 0.00031\n",
      "iteration 239600 training loss 0.9172949 lr 0.00031\n",
      "iteration 239700 training loss 0.9340274 lr 0.00031\n",
      "iteration 239800 training loss 1.0124176 lr 0.00031\n",
      "iteration 239900 training loss 0.85694736 lr 0.00031\n",
      "iteration 240000 training loss 0.89601403 lr 0.00031\n",
      "layout:xla:random 0.3693914806941381\n",
      "layout:xla:default 0.08855656118359938\n",
      "layout:nlp:default 0.3934837399136627\n",
      "layout:nlp:random 0.7336054990554206\n",
      "epoch 3, it 240000 validation loss -0.396\n",
      "iteration 240100 training loss 1.067953 lr 0.00028\n",
      "iteration 240200 training loss 0.7695905 lr 0.00028\n",
      "iteration 240300 training loss 0.52873737 lr 0.00028\n",
      "iteration 240400 training loss 0.85066366 lr 0.00028\n",
      "iteration 240500 training loss 0.59211326 lr 0.00028\n",
      "iteration 240600 training loss 0.70027256 lr 0.00028\n",
      "iteration 240700 training loss 0.8137046 lr 0.00028\n",
      "iteration 240800 training loss 0.6128686 lr 0.00028\n",
      "iteration 240900 training loss 1.0044562 lr 0.00028\n",
      "iteration 241000 training loss 0.5843167 lr 0.00028\n",
      "iteration 241100 training loss 0.6522152 lr 0.00028\n",
      "iteration 241200 training loss 0.9662067 lr 0.00028\n",
      "iteration 241300 training loss 0.6298305 lr 0.00028\n",
      "iteration 241400 training loss 0.6287005 lr 0.00028\n",
      "iteration 241500 training loss 0.6963916 lr 0.00028\n",
      "iteration 241600 training loss 0.64516675 lr 0.00028\n",
      "iteration 241700 training loss 0.7068644 lr 0.00028\n",
      "iteration 241800 training loss 0.7348472 lr 0.00028\n",
      "iteration 241900 training loss 1.1885118 lr 0.00028\n",
      "iteration 242000 training loss 0.9342849 lr 0.00028\n",
      "iteration 242100 training loss 0.7135168 lr 0.00028\n",
      "iteration 242200 training loss 0.78249323 lr 0.00028\n",
      "iteration 242300 training loss 0.6005501 lr 0.00028\n",
      "iteration 242400 training loss 0.86093926 lr 0.00028\n",
      "iteration 242500 training loss 0.80115306 lr 0.00028\n",
      "iteration 242600 training loss 0.7791393 lr 0.00028\n",
      "iteration 242700 training loss 0.7904397 lr 0.00028\n",
      "iteration 242800 training loss 0.5714069 lr 0.00028\n",
      "iteration 242900 training loss 0.6680478 lr 0.00028\n",
      "iteration 243000 training loss 0.66685176 lr 0.00028\n",
      "iteration 243100 training loss 0.6660335 lr 0.00028\n",
      "iteration 243200 training loss 0.59614396 lr 0.00028\n",
      "iteration 243300 training loss 0.6480696 lr 0.00028\n",
      "iteration 243400 training loss 0.565283 lr 0.00028\n",
      "iteration 243500 training loss 0.9469014 lr 0.00028\n",
      "iteration 243600 training loss 0.9666333 lr 0.00028\n",
      "iteration 243700 training loss 0.7284532 lr 0.00028\n",
      "iteration 243800 training loss 0.819609 lr 0.00028\n",
      "iteration 243900 training loss 0.9907209 lr 0.00028\n",
      "iteration 244000 training loss 0.68420005 lr 0.00028\n",
      "iteration 244100 training loss 0.7119311 lr 0.00028\n",
      "iteration 244200 training loss 0.8232298 lr 0.00028\n",
      "iteration 244300 training loss 0.8271322 lr 0.00028\n",
      "iteration 244400 training loss 0.8507306 lr 0.00028\n",
      "iteration 244500 training loss 0.884563 lr 0.00028\n",
      "iteration 244600 training loss 0.9196587 lr 0.00028\n",
      "iteration 244700 training loss 0.7378683 lr 0.00028\n",
      "iteration 244800 training loss 0.9272951 lr 0.00028\n",
      "iteration 244900 training loss 0.8293542 lr 0.00028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 245000 training loss 0.90897197 lr 0.00028\n",
      "iteration 245100 training loss 0.84407866 lr 0.00028\n",
      "iteration 245200 training loss 0.78265244 lr 0.00028\n",
      "iteration 245300 training loss 0.88515073 lr 0.00028\n",
      "iteration 245400 training loss 0.98258185 lr 0.00028\n",
      "iteration 245500 training loss 0.96049225 lr 0.00028\n",
      "iteration 245600 training loss 0.8673521 lr 0.00028\n",
      "iteration 245700 training loss 1.0398561 lr 0.00028\n",
      "iteration 245800 training loss 0.8361942 lr 0.00028\n",
      "iteration 245900 training loss 1.1701635 lr 0.00028\n",
      "iteration 246000 training loss 0.91795295 lr 0.00028\n",
      "iteration 246100 training loss 0.9570489 lr 0.00028\n",
      "iteration 246200 training loss 0.6039889 lr 0.00028\n",
      "iteration 246300 training loss 0.75446695 lr 0.00028\n",
      "iteration 246400 training loss 0.7188936 lr 0.00028\n",
      "iteration 246500 training loss 0.739536 lr 0.00028\n",
      "iteration 246600 training loss 1.0010927 lr 0.00028\n",
      "iteration 246700 training loss 0.6518087 lr 0.00028\n",
      "iteration 246800 training loss 0.94159555 lr 0.00028\n",
      "iteration 246900 training loss 0.7453629 lr 0.00028\n",
      "iteration 247000 training loss 0.78223187 lr 0.00028\n",
      "iteration 247100 training loss 0.77267873 lr 0.00028\n",
      "iteration 247200 training loss 0.71340644 lr 0.00028\n",
      "iteration 247300 training loss 0.76504236 lr 0.00028\n",
      "iteration 247400 training loss 0.7588879 lr 0.00028\n",
      "iteration 247500 training loss 0.791077 lr 0.00028\n",
      "iteration 247600 training loss 0.527731 lr 0.00028\n",
      "iteration 247700 training loss 0.5774207 lr 0.00028\n",
      "iteration 247800 training loss 0.6767303 lr 0.00028\n",
      "iteration 247900 training loss 0.6153105 lr 0.00028\n",
      "iteration 248000 training loss 0.8107759 lr 0.00028\n",
      "iteration 248100 training loss 0.55507094 lr 0.00028\n",
      "iteration 248200 training loss 0.71838945 lr 0.00028\n",
      "iteration 248300 training loss 0.6214341 lr 0.00028\n",
      "iteration 248400 training loss 1.0219055 lr 0.00028\n",
      "iteration 248500 training loss 0.618216 lr 0.00028\n",
      "iteration 248600 training loss 0.7606564 lr 0.00028\n",
      "iteration 248700 training loss 0.5775563 lr 0.00028\n",
      "iteration 248800 training loss 0.5621065 lr 0.00028\n",
      "iteration 248900 training loss 0.5914385 lr 0.00028\n",
      "iteration 249000 training loss 0.6021423 lr 0.00028\n",
      "iteration 249100 training loss 0.7360606 lr 0.00028\n",
      "iteration 249200 training loss 1.2388138 lr 0.00028\n",
      "iteration 249300 training loss 0.55352944 lr 0.00028\n",
      "iteration 249400 training loss 0.71004057 lr 0.00028\n",
      "iteration 249500 training loss 0.8243045 lr 0.00028\n",
      "iteration 249600 training loss 0.947292 lr 0.00028\n",
      "iteration 249700 training loss 0.84911025 lr 0.00028\n",
      "iteration 249800 training loss 0.95456254 lr 0.00028\n",
      "iteration 249900 training loss 0.7583537 lr 0.00028\n",
      "iteration 250000 training loss 0.7323138 lr 0.00028\n",
      "layout:xla:default 0.11679637847826982\n",
      "layout:nlp:default 0.3748095726535835\n",
      "layout:nlp:random 0.7161605170617039\n",
      "layout:xla:random 0.2827342055278731\n",
      "epoch 3, it 250000 validation loss -0.373\n",
      "iteration 250100 training loss 0.6687357 lr 0.00028\n",
      "iteration 250200 training loss 0.6201359 lr 0.00028\n",
      "iteration 250300 training loss 0.5351263 lr 0.00028\n",
      "iteration 250400 training loss 0.753587 lr 0.00028\n",
      "iteration 250500 training loss 0.8204514 lr 0.00028\n",
      "iteration 250600 training loss 0.73800784 lr 0.00028\n",
      "iteration 250700 training loss 0.7819129 lr 0.00028\n",
      "iteration 250800 training loss 0.8365038 lr 0.00028\n",
      "iteration 250900 training loss 0.65760976 lr 0.00028\n",
      "iteration 251000 training loss 0.7626227 lr 0.00028\n",
      "iteration 251100 training loss 1.0010378 lr 0.00028\n",
      "iteration 251200 training loss 0.7064286 lr 0.00028\n",
      "iteration 251300 training loss 0.88242173 lr 0.00028\n",
      "iteration 251400 training loss 0.7320248 lr 0.00028\n",
      "iteration 251500 training loss 0.7134905 lr 0.00028\n",
      "iteration 251600 training loss 0.75545365 lr 0.00028\n",
      "iteration 251700 training loss 0.7403369 lr 0.00028\n",
      "iteration 251800 training loss 0.867634 lr 0.00028\n",
      "iteration 251900 training loss 0.55180305 lr 0.00028\n",
      "iteration 252000 training loss 0.7574825 lr 0.00028\n",
      "iteration 252100 training loss 0.6046186 lr 0.00028\n",
      "iteration 252200 training loss 0.4168228 lr 0.00028\n",
      "iteration 252300 training loss 0.62822014 lr 0.00028\n",
      "iteration 252400 training loss 0.6808678 lr 0.00028\n",
      "iteration 252500 training loss 0.5476994 lr 0.00028\n",
      "iteration 252600 training loss 0.75017065 lr 0.00028\n",
      "iteration 252700 training loss 0.7408225 lr 0.00028\n",
      "iteration 252800 training loss 0.7891167 lr 0.00028\n",
      "iteration 252900 training loss 0.69737005 lr 0.00028\n",
      "iteration 253000 training loss 0.71272755 lr 0.00028\n",
      "iteration 253100 training loss 0.63319147 lr 0.00028\n",
      "iteration 253200 training loss 0.91729665 lr 0.00028\n",
      "iteration 253300 training loss 0.78984 lr 0.00028\n",
      "iteration 253400 training loss 0.51726353 lr 0.00028\n",
      "iteration 253500 training loss 0.87798446 lr 0.00028\n",
      "iteration 253600 training loss 0.7916764 lr 0.00028\n",
      "iteration 253700 training loss 0.55312777 lr 0.00028\n",
      "iteration 253800 training loss 0.641414 lr 0.00028\n",
      "iteration 253900 training loss 0.66486955 lr 0.00028\n",
      "iteration 254000 training loss 0.72193193 lr 0.00028\n",
      "iteration 254100 training loss 0.6768441 lr 0.00028\n",
      "iteration 254200 training loss 0.8096902 lr 0.00028\n",
      "iteration 254300 training loss 0.8558743 lr 0.00028\n",
      "iteration 254400 training loss 1.0386641 lr 0.00028\n",
      "iteration 254500 training loss 0.85116225 lr 0.00028\n",
      "iteration 254600 training loss 0.98187345 lr 0.00028\n",
      "iteration 254700 training loss 0.88021404 lr 0.00028\n",
      "iteration 254800 training loss 0.77080125 lr 0.00028\n",
      "iteration 254900 training loss 0.7997811 lr 0.00028\n",
      "iteration 255000 training loss 0.96827364 lr 0.00028\n",
      "iteration 255100 training loss 0.68866324 lr 0.00028\n",
      "iteration 255200 training loss 0.9913808 lr 0.00028\n",
      "iteration 255300 training loss 0.8319278 lr 0.00028\n",
      "iteration 255400 training loss 1.0442802 lr 0.00028\n",
      "iteration 255500 training loss 0.7854425 lr 0.00028\n",
      "iteration 255600 training loss 0.6294244 lr 0.00028\n",
      "iteration 255700 training loss 0.9499282 lr 0.00028\n",
      "iteration 255800 training loss 0.7947219 lr 0.00028\n",
      "iteration 255900 training loss 0.78991354 lr 0.00028\n",
      "iteration 256000 training loss 0.5557017 lr 0.00028\n",
      "iteration 256100 training loss 0.5698758 lr 0.00028\n",
      "iteration 256200 training loss 0.8658893 lr 0.00028\n",
      "iteration 256300 training loss 0.55524683 lr 0.00028\n",
      "iteration 256400 training loss 0.83451813 lr 0.00028\n",
      "iteration 256500 training loss 0.8266743 lr 0.00028\n",
      "iteration 256600 training loss 0.9029976 lr 0.00028\n",
      "iteration 256700 training loss 0.891506 lr 0.00028\n",
      "iteration 256800 training loss 0.72289044 lr 0.00028\n",
      "iteration 256900 training loss 0.5898354 lr 0.00028\n",
      "iteration 257000 training loss 0.55886877 lr 0.00028\n",
      "iteration 257100 training loss 0.7530048 lr 0.00028\n",
      "iteration 257200 training loss 0.65949214 lr 0.00028\n",
      "iteration 257300 training loss 0.7813432 lr 0.00028\n",
      "iteration 257400 training loss 0.9999115 lr 0.00028\n",
      "iteration 257500 training loss 0.72481996 lr 0.00028\n",
      "iteration 257600 training loss 1.0322127 lr 0.00028\n",
      "iteration 257700 training loss 1.0601181 lr 0.00028\n",
      "iteration 257800 training loss 0.97244483 lr 0.00028\n",
      "iteration 257900 training loss 0.9162625 lr 0.00028\n",
      "iteration 258000 training loss 0.8454762 lr 0.00028\n",
      "iteration 258100 training loss 0.9005715 lr 0.00028\n",
      "iteration 258200 training loss 0.9197879 lr 0.00028\n",
      "iteration 258300 training loss 0.87168103 lr 0.00028\n",
      "iteration 258400 training loss 0.8953788 lr 0.00028\n",
      "iteration 258500 training loss 0.7622325 lr 0.00028\n",
      "iteration 258600 training loss 0.9482123 lr 0.00028\n",
      "iteration 258700 training loss 0.7342307 lr 0.00028\n",
      "iteration 258800 training loss 0.74336433 lr 0.00028\n",
      "iteration 258900 training loss 0.7099262 lr 0.00028\n",
      "iteration 259000 training loss 0.8326861 lr 0.00028\n",
      "iteration 259100 training loss 0.98559195 lr 0.00028\n",
      "iteration 259200 training loss 0.6085373 lr 0.00028\n",
      "iteration 259300 training loss 0.82411385 lr 0.00028\n",
      "iteration 259400 training loss 0.8424031 lr 0.00028\n",
      "iteration 259500 training loss 0.86986995 lr 0.00028\n",
      "iteration 259600 training loss 0.7160528 lr 0.00028\n",
      "iteration 259700 training loss 0.5587336 lr 0.00028\n",
      "iteration 259800 training loss 0.45306358 lr 0.00028\n",
      "iteration 259900 training loss 0.57167315 lr 0.00028\n",
      "iteration 260000 training loss 0.6462052 lr 0.00028\n",
      "layout:xla:random 0.3165813624643744\n",
      "layout:xla:default 0.07069235783154865\n",
      "layout:nlp:random 0.7231072095118226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layout:nlp:default 0.36457117019011154\n",
      "epoch 3, it 260000 validation loss -0.369\n",
      "iteration 260100 training loss 0.70696086 lr 0.00025\n",
      "iteration 260200 training loss 0.59836435 lr 0.00025\n",
      "iteration 260300 training loss 0.5362259 lr 0.00025\n",
      "iteration 260400 training loss 0.50126183 lr 0.00025\n",
      "iteration 260500 training loss 0.6345616 lr 0.00025\n",
      "iteration 260600 training loss 0.85864615 lr 0.00025\n",
      "iteration 260700 training loss 0.73623985 lr 0.00025\n",
      "iteration 260800 training loss 0.6802935 lr 0.00025\n",
      "iteration 260900 training loss 0.5821549 lr 0.00025\n",
      "iteration 261000 training loss 0.7826317 lr 0.00025\n",
      "iteration 261100 training loss 0.7038887 lr 0.00025\n",
      "iteration 261200 training loss 0.72478765 lr 0.00025\n",
      "iteration 261300 training loss 0.7280031 lr 0.00025\n",
      "iteration 261400 training loss 0.817129 lr 0.00025\n",
      "iteration 261500 training loss 0.7591832 lr 0.00025\n",
      "iteration 261600 training loss 0.47426438 lr 0.00025\n",
      "iteration 261700 training loss 0.68735945 lr 0.00025\n",
      "iteration 261800 training loss 0.7635152 lr 0.00025\n",
      "iteration 261900 training loss 0.7554484 lr 0.00025\n",
      "iteration 262000 training loss 0.6372948 lr 0.00025\n",
      "iteration 262100 training loss 0.84385407 lr 0.00025\n",
      "iteration 262200 training loss 0.63957876 lr 0.00025\n",
      "iteration 262300 training loss 0.64671797 lr 0.00025\n",
      "iteration 262400 training loss 0.7489806 lr 0.00025\n",
      "iteration 262500 training loss 0.83563167 lr 0.00025\n",
      "iteration 262600 training loss 0.7153495 lr 0.00025\n",
      "iteration 262700 training loss 0.80990124 lr 0.00025\n",
      "iteration 262800 training loss 0.7784949 lr 0.00025\n",
      "iteration 262900 training loss 0.67989767 lr 0.00025\n",
      "iteration 263000 training loss 0.737597 lr 0.00025\n",
      "iteration 263100 training loss 0.7929476 lr 0.00025\n",
      "iteration 263200 training loss 1.1527989 lr 0.00025\n",
      "iteration 263300 training loss 0.7398577 lr 0.00025\n",
      "iteration 263400 training loss 0.61450046 lr 0.00025\n",
      "iteration 263500 training loss 0.88065416 lr 0.00025\n",
      "iteration 263600 training loss 0.65267736 lr 0.00025\n",
      "iteration 263700 training loss 0.8153228 lr 0.00025\n",
      "iteration 263800 training loss 0.79673946 lr 0.00025\n",
      "iteration 263900 training loss 1.0181826 lr 0.00025\n",
      "iteration 264000 training loss 0.74934316 lr 0.00025\n",
      "iteration 264100 training loss 0.85598254 lr 0.00025\n",
      "iteration 264200 training loss 0.7623563 lr 0.00025\n",
      "iteration 264300 training loss 0.6112137 lr 0.00025\n",
      "iteration 264400 training loss 0.84763485 lr 0.00025\n",
      "iteration 264500 training loss 0.7237972 lr 0.00025\n",
      "iteration 264600 training loss 0.6423002 lr 0.00025\n",
      "iteration 264700 training loss 0.82274336 lr 0.00025\n",
      "iteration 264800 training loss 0.6196116 lr 0.00025\n",
      "iteration 264900 training loss 0.88405967 lr 0.00025\n",
      "iteration 265000 training loss 0.95734906 lr 0.00025\n",
      "iteration 265100 training loss 0.9027089 lr 0.00025\n",
      "iteration 265200 training loss 0.67446244 lr 0.00025\n",
      "iteration 265300 training loss 0.9689887 lr 0.00025\n",
      "iteration 265400 training loss 0.9049882 lr 0.00025\n",
      "iteration 265500 training loss 0.6761252 lr 0.00025\n",
      "iteration 265600 training loss 0.5241422 lr 0.00025\n",
      "iteration 265700 training loss 0.5093413 lr 0.00025\n",
      "iteration 265800 training loss 0.7771393 lr 0.00025\n",
      "iteration 265900 training loss 0.6409976 lr 0.00025\n",
      "iteration 266000 training loss 0.66609967 lr 0.00025\n",
      "iteration 266100 training loss 0.7982537 lr 0.00025\n",
      "iteration 266200 training loss 0.58206445 lr 0.00025\n",
      "iteration 266300 training loss 0.72031033 lr 0.00025\n",
      "iteration 266400 training loss 0.5347021 lr 0.00025\n",
      "iteration 266500 training loss 0.58682835 lr 0.00025\n",
      "iteration 266600 training loss 0.6351942 lr 0.00025\n",
      "iteration 266700 training loss 0.80822736 lr 0.00025\n",
      "iteration 266800 training loss 0.60278463 lr 0.00025\n",
      "iteration 266900 training loss 0.7122402 lr 0.00025\n",
      "iteration 267000 training loss 0.62505233 lr 0.00025\n",
      "iteration 267100 training loss 0.84887666 lr 0.00025\n",
      "iteration 267200 training loss 0.6451972 lr 0.00025\n",
      "iteration 267300 training loss 0.5727403 lr 0.00025\n",
      "iteration 267400 training loss 0.82218033 lr 0.00025\n",
      "iteration 267500 training loss 0.92080563 lr 0.00025\n",
      "iteration 267600 training loss 0.48780075 lr 0.00025\n",
      "iteration 267700 training loss 0.58548915 lr 0.00025\n",
      "iteration 267800 training loss 0.65324783 lr 0.00025\n",
      "iteration 267900 training loss 0.68073547 lr 0.00025\n",
      "iteration 268000 training loss 0.6941603 lr 0.00025\n",
      "iteration 268100 training loss 0.66465986 lr 0.00025\n",
      "iteration 268200 training loss 0.71072304 lr 0.00025\n",
      "iteration 268300 training loss 0.5942013 lr 0.00025\n",
      "iteration 268400 training loss 0.9226146 lr 0.00025\n",
      "iteration 268500 training loss 0.90278333 lr 0.00025\n",
      "iteration 268600 training loss 0.7397652 lr 0.00025\n",
      "iteration 268700 training loss 0.8451837 lr 0.00025\n",
      "iteration 268800 training loss 0.6829709 lr 0.00025\n",
      "iteration 268900 training loss 0.69888395 lr 0.00025\n",
      "iteration 269000 training loss 0.7434402 lr 0.00025\n",
      "iteration 269100 training loss 0.63165265 lr 0.00025\n",
      "iteration 269200 training loss 0.7777996 lr 0.00025\n",
      "iteration 269300 training loss 0.84777874 lr 0.00025\n",
      "iteration 269400 training loss 0.44994268 lr 0.00025\n",
      "iteration 269500 training loss 0.8708473 lr 0.00025\n",
      "iteration 269600 training loss 0.5900562 lr 0.00025\n",
      "iteration 269700 training loss 0.65930176 lr 0.00025\n",
      "iteration 269800 training loss 0.71649796 lr 0.00025\n",
      "iteration 269900 training loss 0.5285902 lr 0.00025\n",
      "iteration 270000 training loss 0.60036814 lr 0.00025\n",
      "layout:xla:default 0.10126396434993694\n",
      "layout:xla:random 0.39445362983971305\n",
      "layout:nlp:default 0.3395892351332227\n",
      "layout:nlp:random 0.7686337168859311\n",
      "epoch 3, it 270000 validation loss -0.401\n",
      "iteration 270100 training loss 0.6492826 lr 0.00025\n",
      "iteration 270200 training loss 0.82851547 lr 0.00025\n",
      "iteration 270300 training loss 0.63164127 lr 0.00025\n",
      "iteration 270400 training loss 0.72480935 lr 0.00025\n",
      "iteration 270500 training loss 0.70572716 lr 0.00025\n",
      "iteration 270600 training loss 0.5925145 lr 0.00025\n",
      "iteration 270700 training loss 0.51151425 lr 0.00025\n",
      "iteration 270800 training loss 0.65136707 lr 0.00025\n",
      "iteration 270900 training loss 0.6933538 lr 0.00025\n",
      "iteration 271000 training loss 0.5273595 lr 0.00025\n",
      "iteration 271100 training loss 0.5101191 lr 0.00025\n",
      "iteration 271200 training loss 0.61612076 lr 0.00025\n",
      "iteration 271300 training loss 0.4679465 lr 0.00025\n",
      "iteration 271400 training loss 0.75436944 lr 0.00025\n",
      "iteration 271500 training loss 0.7927313 lr 0.00025\n",
      "iteration 271600 training loss 0.7157106 lr 0.00025\n",
      "iteration 271700 training loss 0.73348135 lr 0.00025\n",
      "iteration 271800 training loss 0.64100623 lr 0.00025\n",
      "iteration 271900 training loss 0.7081008 lr 0.00025\n",
      "iteration 272000 training loss 0.7743036 lr 0.00025\n",
      "iteration 272100 training loss 0.67211705 lr 0.00025\n",
      "iteration 272200 training loss 0.71468526 lr 0.00025\n",
      "iteration 272300 training loss 0.5967063 lr 0.00025\n",
      "iteration 272400 training loss 0.6451523 lr 0.00025\n",
      "iteration 272500 training loss 0.8267636 lr 0.00025\n",
      "iteration 272600 training loss 0.8941214 lr 0.00025\n",
      "iteration 272700 training loss 0.69368994 lr 0.00025\n",
      "iteration 272800 training loss 0.8287167 lr 0.00025\n",
      "iteration 272900 training loss 0.93422186 lr 0.00025\n",
      "iteration 273000 training loss 0.678892 lr 0.00025\n",
      "iteration 273100 training loss 0.70149094 lr 0.00025\n",
      "iteration 273200 training loss 0.81948304 lr 0.00025\n",
      "iteration 273300 training loss 0.5952334 lr 0.00025\n",
      "iteration 273400 training loss 1.0217681 lr 0.00025\n",
      "iteration 273500 training loss 0.55585235 lr 0.00025\n",
      "iteration 273600 training loss 0.6596418 lr 0.00025\n",
      "iteration 273700 training loss 0.4532696 lr 0.00025\n",
      "iteration 273800 training loss 0.68942076 lr 0.00025\n",
      "iteration 273900 training loss 0.63150954 lr 0.00025\n",
      "iteration 274000 training loss 0.689207 lr 0.00025\n",
      "iteration 274100 training loss 0.41782787 lr 0.00025\n",
      "iteration 274200 training loss 0.6502263 lr 0.00025\n",
      "iteration 274300 training loss 0.6214658 lr 0.00025\n",
      "iteration 274400 training loss 0.7825703 lr 0.00025\n",
      "iteration 274500 training loss 0.9916028 lr 0.00025\n",
      "iteration 274600 training loss 0.8120749 lr 0.00025\n",
      "iteration 274700 training loss 0.8547543 lr 0.00025\n",
      "iteration 274800 training loss 0.8289068 lr 0.00025\n",
      "iteration 274900 training loss 0.85604036 lr 0.00025\n",
      "iteration 275000 training loss 0.84940845 lr 0.00025\n",
      "iteration 275100 training loss 0.522543 lr 0.00025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 275200 training loss 0.5775357 lr 0.00025\n",
      "iteration 275300 training loss 0.6898065 lr 0.00025\n",
      "iteration 275400 training loss 0.6409032 lr 0.00025\n",
      "iteration 275500 training loss 0.90383756 lr 0.00025\n",
      "iteration 275600 training loss 0.73739 lr 0.00025\n",
      "iteration 275700 training loss 0.9026621 lr 0.00025\n",
      "iteration 275800 training loss 0.75564224 lr 0.00025\n",
      "iteration 275900 training loss 0.8735506 lr 0.00025\n",
      "iteration 276000 training loss 0.7283232 lr 0.00025\n",
      "iteration 276100 training loss 0.9435486 lr 0.00025\n",
      "iteration 276200 training loss 0.7466603 lr 0.00025\n",
      "iteration 276300 training loss 0.8876463 lr 0.00025\n",
      "iteration 276400 training loss 0.53000534 lr 0.00025\n",
      "iteration 276500 training loss 0.6260423 lr 0.00025\n",
      "iteration 276600 training loss 0.71555996 lr 0.00025\n",
      "iteration 276700 training loss 0.69046825 lr 0.00025\n",
      "iteration 276800 training loss 0.68903595 lr 0.00025\n",
      "iteration 276900 training loss 0.80636656 lr 0.00025\n",
      "iteration 277000 training loss 0.75748545 lr 0.00025\n",
      "iteration 277100 training loss 0.6726684 lr 0.00025\n",
      "iteration 277200 training loss 0.91898286 lr 0.00025\n",
      "iteration 277300 training loss 0.7641314 lr 0.00025\n",
      "iteration 277400 training loss 0.6806707 lr 0.00025\n",
      "iteration 277500 training loss 0.6925803 lr 0.00025\n",
      "iteration 277600 training loss 0.7278171 lr 0.00025\n",
      "iteration 277700 training loss 0.68145925 lr 0.00025\n",
      "iteration 277800 training loss 0.7219542 lr 0.00025\n",
      "iteration 277900 training loss 0.81311274 lr 0.00025\n",
      "iteration 278000 training loss 1.0365047 lr 0.00025\n",
      "iteration 278100 training loss 0.70572186 lr 0.00025\n",
      "iteration 278200 training loss 0.8128784 lr 0.00025\n",
      "iteration 278300 training loss 0.77816564 lr 0.00025\n",
      "iteration 278400 training loss 0.7386938 lr 0.00025\n",
      "iteration 278500 training loss 0.8105654 lr 0.00025\n",
      "iteration 278600 training loss 0.70239574 lr 0.00025\n",
      "iteration 278700 training loss 0.7130525 lr 0.00025\n",
      "iteration 278800 training loss 0.48201913 lr 0.00025\n",
      "iteration 278900 training loss 0.6038618 lr 0.00025\n",
      "iteration 279000 training loss 0.46145442 lr 0.00025\n",
      "iteration 279100 training loss 0.4885672 lr 0.00025\n",
      "iteration 279200 training loss 0.57677513 lr 0.00025\n",
      "iteration 279300 training loss 0.7481665 lr 0.00025\n",
      "iteration 279400 training loss 0.57170725 lr 0.00025\n",
      "iteration 279500 training loss 0.68507034 lr 0.00025\n",
      "iteration 279600 training loss 0.49443084 lr 0.00025\n",
      "iteration 279700 training loss 0.72696394 lr 0.00025\n",
      "iteration 279800 training loss 0.7958029 lr 0.00025\n",
      "iteration 279900 training loss 0.7842679 lr 0.00025\n",
      "iteration 280000 training loss 0.73087776 lr 0.00025\n",
      "layout:xla:default 0.058152159883387934\n",
      "layout:nlp:random 0.7518875824982736\n",
      "layout:nlp:default 0.38171392410573585\n",
      "layout:xla:random 0.2927717435964426\n",
      "epoch 3, it 280000 validation loss -0.371\n",
      "iteration 280100 training loss 0.56941545 lr 0.00023\n",
      "iteration 280200 training loss 0.5303048 lr 0.00023\n",
      "iteration 280300 training loss 0.50779873 lr 0.00023\n",
      "iteration 280400 training loss 0.69817555 lr 0.00023\n",
      "iteration 280500 training loss 1.0569125 lr 0.00023\n",
      "iteration 280600 training loss 0.7327004 lr 0.00023\n",
      "iteration 280700 training loss 0.547225 lr 0.00023\n",
      "iteration 280800 training loss 0.89683515 lr 0.00023\n",
      "iteration 280900 training loss 0.7943255 lr 0.00023\n",
      "iteration 281000 training loss 0.65572685 lr 0.00023\n",
      "iteration 281100 training loss 0.70514774 lr 0.00023\n",
      "iteration 281200 training loss 0.6859835 lr 0.00023\n",
      "iteration 281300 training loss 0.62999535 lr 0.00023\n",
      "iteration 281400 training loss 0.82799643 lr 0.00023\n",
      "iteration 281500 training loss 0.8046781 lr 0.00023\n",
      "iteration 281600 training loss 0.80095536 lr 0.00023\n",
      "iteration 281700 training loss 0.9790782 lr 0.00023\n",
      "iteration 281800 training loss 1.4215289 lr 0.00023\n",
      "iteration 281900 training loss 0.7906172 lr 0.00023\n",
      "iteration 282000 training loss 1.0576614 lr 0.00023\n",
      "iteration 282100 training loss 1.206526 lr 0.00023\n",
      "iteration 282200 training loss 0.90903586 lr 0.00023\n",
      "iteration 282300 training loss 0.9446265 lr 0.00023\n",
      "iteration 282400 training loss 0.9587418 lr 0.00023\n",
      "iteration 282500 training loss 1.0501609 lr 0.00023\n",
      "iteration 282600 training loss 0.8480045 lr 0.00023\n",
      "iteration 282700 training loss 0.77106863 lr 0.00023\n",
      "iteration 282800 training loss 1.02533 lr 0.00023\n",
      "iteration 282900 training loss 0.9905855 lr 0.00023\n",
      "iteration 283000 training loss 0.9304158 lr 0.00023\n",
      "iteration 283100 training loss 1.0461192 lr 0.00023\n",
      "iteration 283200 training loss 0.7070287 lr 0.00023\n",
      "iteration 283300 training loss 1.0017743 lr 0.00023\n",
      "iteration 283400 training loss 0.9377129 lr 0.00023\n",
      "iteration 283500 training loss 0.8830467 lr 0.00023\n",
      "iteration 283600 training loss 1.2321506 lr 0.00023\n",
      "iteration 283700 training loss 1.058995 lr 0.00023\n",
      "iteration 283800 training loss 0.92259735 lr 0.00023\n",
      "iteration 283900 training loss 0.81958205 lr 0.00023\n",
      "iteration 284000 training loss 0.896436 lr 0.00023\n",
      "iteration 284100 training loss 0.9356472 lr 0.00023\n",
      "iteration 284200 training loss 0.7518397 lr 0.00023\n",
      "iteration 284300 training loss 0.6859242 lr 0.00023\n",
      "iteration 284400 training loss 0.7891636 lr 0.00023\n",
      "iteration 284500 training loss 1.1467988 lr 0.00023\n",
      "iteration 284600 training loss 0.8158591 lr 0.00023\n",
      "iteration 284700 training loss 0.6515387 lr 0.00023\n",
      "iteration 284800 training loss 0.91535056 lr 0.00023\n",
      "iteration 284900 training loss 0.87267476 lr 0.00023\n",
      "iteration 285000 training loss 0.9334225 lr 0.00023\n",
      "iteration 285100 training loss 0.98190373 lr 0.00023\n",
      "iteration 285200 training loss 0.94824994 lr 0.00023\n",
      "iteration 285300 training loss 0.87447685 lr 0.00023\n",
      "iteration 285400 training loss 0.58540595 lr 0.00023\n",
      "iteration 285500 training loss 0.6688477 lr 0.00023\n",
      "iteration 285600 training loss 0.7018521 lr 0.00023\n",
      "iteration 285700 training loss 0.71107614 lr 0.00023\n",
      "iteration 285800 training loss 0.8411428 lr 0.00023\n",
      "iteration 285900 training loss 0.75359404 lr 0.00023\n",
      "iteration 286000 training loss 0.46882898 lr 0.00023\n",
      "iteration 286100 training loss 0.5526713 lr 0.00023\n",
      "iteration 286200 training loss 0.7894029 lr 0.00023\n",
      "iteration 286300 training loss 0.70677084 lr 0.00023\n",
      "iteration 286400 training loss 0.82718754 lr 0.00023\n",
      "iteration 286500 training loss 0.59481233 lr 0.00023\n",
      "iteration 286600 training loss 0.86234957 lr 0.00023\n",
      "iteration 286700 training loss 0.705803 lr 0.00023\n",
      "iteration 286800 training loss 0.8966475 lr 0.00023\n",
      "iteration 286900 training loss 0.99371314 lr 0.00023\n",
      "iteration 287000 training loss 1.206622 lr 0.00023\n",
      "iteration 287100 training loss 1.0823116 lr 0.00023\n",
      "iteration 287200 training loss 0.66249895 lr 0.00023\n",
      "iteration 287300 training loss 0.82319707 lr 0.00023\n",
      "iteration 287400 training loss 0.81194985 lr 0.00023\n",
      "iteration 287500 training loss 0.8163633 lr 0.00023\n",
      "iteration 287600 training loss 0.8824855 lr 0.00023\n",
      "iteration 287700 training loss 0.7205125 lr 0.00023\n",
      "iteration 287800 training loss 1.1320564 lr 0.00023\n",
      "iteration 287900 training loss 0.7652231 lr 0.00023\n",
      "iteration 288000 training loss 0.693 lr 0.00023\n",
      "iteration 288100 training loss 0.98273754 lr 0.00023\n",
      "iteration 288200 training loss 0.6787862 lr 0.00023\n",
      "iteration 288300 training loss 0.97175974 lr 0.00023\n",
      "iteration 288400 training loss 0.56612295 lr 0.00023\n",
      "iteration 288500 training loss 0.77134323 lr 0.00023\n",
      "iteration 288600 training loss 0.7244971 lr 0.00023\n",
      "iteration 288700 training loss 0.8085491 lr 0.00023\n",
      "iteration 288800 training loss 0.72418517 lr 0.00023\n",
      "iteration 288900 training loss 0.5623795 lr 0.00023\n",
      "iteration 289000 training loss 0.71557564 lr 0.00023\n",
      "iteration 289100 training loss 0.77572954 lr 0.00023\n",
      "iteration 289200 training loss 0.46864563 lr 0.00023\n",
      "iteration 289300 training loss 0.5834814 lr 0.00023\n",
      "iteration 289400 training loss 0.842834 lr 0.00023\n",
      "iteration 289500 training loss 0.73134637 lr 0.00023\n",
      "iteration 289600 training loss 0.79310447 lr 0.00023\n",
      "iteration 289700 training loss 0.82374775 lr 0.00023\n",
      "iteration 289800 training loss 0.7023692 lr 0.00023\n",
      "iteration 289900 training loss 0.9619851 lr 0.00023\n",
      "iteration 290000 training loss 0.7021611 lr 0.00023\n",
      "layout:nlp:random 0.6554400542662312\n",
      "layout:xla:random 0.30112082315823113\n",
      "layout:nlp:default 0.3808670002198896\n",
      "layout:xla:default 0.13888887723136353\n",
      "epoch 3, it 290000 validation loss -0.369\n",
      "iteration 290100 training loss 0.89653903 lr 0.00023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 290200 training loss 0.6326549 lr 0.00023\n",
      "iteration 290300 training loss 0.82597494 lr 0.00023\n",
      "iteration 290400 training loss 0.7330602 lr 0.00023\n",
      "iteration 290500 training loss 0.63069665 lr 0.00023\n",
      "iteration 290600 training loss 0.8613393 lr 0.00023\n",
      "iteration 290700 training loss 0.78262883 lr 0.00023\n",
      "iteration 290800 training loss 0.6633496 lr 0.00023\n",
      "iteration 290900 training loss 0.89817876 lr 0.00023\n",
      "iteration 291000 training loss 0.8421829 lr 0.00023\n",
      "iteration 291100 training loss 0.592128 lr 0.00023\n",
      "iteration 291200 training loss 1.0527642 lr 0.00023\n",
      "iteration 291300 training loss 0.57429457 lr 0.00023\n",
      "iteration 291400 training loss 0.67229766 lr 0.00023\n",
      "iteration 291500 training loss 0.54746187 lr 0.00023\n",
      "iteration 291600 training loss 0.6782688 lr 0.00023\n",
      "iteration 291700 training loss 0.77325094 lr 0.00023\n",
      "iteration 291800 training loss 0.57967776 lr 0.00023\n",
      "iteration 291900 training loss 0.36036634 lr 0.00023\n",
      "iteration 292000 training loss 0.6401385 lr 0.00023\n",
      "iteration 292100 training loss 0.2891925 lr 0.00023\n",
      "iteration 292200 training loss 0.44863674 lr 0.00023\n",
      "iteration 292300 training loss 0.5123929 lr 0.00023\n",
      "iteration 292400 training loss 0.4908497 lr 0.00023\n",
      "iteration 292500 training loss 0.534819 lr 0.00023\n",
      "iteration 292600 training loss 0.39522254 lr 0.00023\n",
      "iteration 292700 training loss 0.6805692 lr 0.00023\n",
      "iteration 292800 training loss 0.7196412 lr 0.00023\n",
      "iteration 292900 training loss 0.60999626 lr 0.00023\n",
      "iteration 293000 training loss 0.6343675 lr 0.00023\n",
      "iteration 293100 training loss 0.74205357 lr 0.00023\n",
      "iteration 293200 training loss 0.51617676 lr 0.00023\n",
      "iteration 293300 training loss 0.77815473 lr 0.00023\n",
      "iteration 293400 training loss 0.6594294 lr 0.00023\n",
      "iteration 293500 training loss 0.6239032 lr 0.00023\n",
      "iteration 293600 training loss 0.64604694 lr 0.00023\n",
      "iteration 293700 training loss 0.7102526 lr 0.00023\n",
      "iteration 293800 training loss 0.6901937 lr 0.00023\n",
      "iteration 293900 training loss 0.596645 lr 0.00023\n",
      "iteration 294000 training loss 0.6652409 lr 0.00023\n",
      "iteration 294100 training loss 0.6345237 lr 0.00023\n",
      "iteration 294200 training loss 0.58206034 lr 0.00023\n",
      "iteration 294300 training loss 0.6743337 lr 0.00023\n",
      "iteration 294400 training loss 0.75595474 lr 0.00023\n",
      "iteration 294500 training loss 0.6416344 lr 0.00023\n",
      "iteration 294600 training loss 0.7873359 lr 0.00023\n",
      "iteration 294700 training loss 0.7696846 lr 0.00023\n",
      "iteration 294800 training loss 0.5249895 lr 0.00023\n",
      "iteration 294900 training loss 0.44004792 lr 0.00023\n",
      "iteration 295000 training loss 0.5294495 lr 0.00023\n",
      "iteration 295100 training loss 0.5578533 lr 0.00023\n",
      "iteration 295200 training loss 0.35315907 lr 0.00023\n",
      "iteration 295300 training loss 0.35420194 lr 0.00023\n",
      "iteration 295400 training loss 0.88651365 lr 0.00023\n",
      "iteration 295500 training loss 0.53456753 lr 0.00023\n",
      "iteration 295600 training loss 0.7684952 lr 0.00023\n",
      "iteration 295700 training loss 0.7224699 lr 0.00023\n",
      "iteration 295800 training loss 0.6778717 lr 0.00023\n",
      "iteration 295900 training loss 1.0008038 lr 0.00023\n",
      "iteration 296000 training loss 0.8893844 lr 0.00023\n",
      "iteration 296100 training loss 0.9024977 lr 0.00023\n",
      "iteration 296200 training loss 0.7421275 lr 0.00023\n",
      "iteration 296300 training loss 0.89954966 lr 0.00023\n",
      "iteration 296400 training loss 0.521096 lr 0.00023\n",
      "iteration 296500 training loss 0.61547667 lr 0.00023\n",
      "iteration 296600 training loss 0.95263773 lr 0.00023\n",
      "iteration 296700 training loss 0.7248642 lr 0.00023\n",
      "iteration 296800 training loss 0.69575864 lr 0.00023\n",
      "iteration 296900 training loss 0.8353441 lr 0.00023\n",
      "iteration 297000 training loss 0.6039228 lr 0.00023\n",
      "iteration 297100 training loss 0.66035664 lr 0.00023\n",
      "iteration 297200 training loss 0.78714275 lr 0.00023\n",
      "iteration 297300 training loss 0.6399113 lr 0.00023\n",
      "iteration 297400 training loss 0.59306234 lr 0.00023\n",
      "iteration 297500 training loss 0.7367097 lr 0.00023\n",
      "iteration 297600 training loss 0.8986689 lr 0.00023\n",
      "iteration 297700 training loss 0.8061705 lr 0.00023\n",
      "iteration 297800 training loss 1.2108518 lr 0.00023\n",
      "iteration 297900 training loss 0.8581954 lr 0.00023\n",
      "iteration 298000 training loss 0.90734404 lr 0.00023\n",
      "iteration 298100 training loss 1.1753151 lr 0.00023\n",
      "iteration 298200 training loss 0.79173726 lr 0.00023\n",
      "iteration 298300 training loss 0.9639538 lr 0.00023\n",
      "iteration 298400 training loss 0.8546987 lr 0.00023\n",
      "iteration 298500 training loss 0.7813003 lr 0.00023\n",
      "iteration 298600 training loss 0.92723024 lr 0.00023\n",
      "iteration 298700 training loss 0.6529374 lr 0.00023\n",
      "iteration 298800 training loss 0.6468096 lr 0.00023\n",
      "iteration 298900 training loss 0.64077145 lr 0.00023\n",
      "iteration 299000 training loss 0.8345541 lr 0.00023\n",
      "iteration 299100 training loss 0.8085874 lr 0.00023\n",
      "iteration 299200 training loss 0.95214075 lr 0.00023\n",
      "iteration 299300 training loss 0.93299747 lr 0.00023\n",
      "iteration 299400 training loss 0.96487576 lr 0.00023\n",
      "iteration 299500 training loss 1.0581591 lr 0.00023\n",
      "iteration 299600 training loss 1.0303342 lr 0.00023\n",
      "iteration 299700 training loss 1.1525193 lr 0.00023\n",
      "iteration 299800 training loss 0.8675185 lr 0.00023\n",
      "iteration 299900 training loss 1.1254741 lr 0.00023\n",
      "iteration 300000 training loss 0.95011264 lr 0.00023\n",
      "layout:xla:default 0.07787713927783256\n",
      "layout:nlp:default 0.3392455731856473\n",
      "layout:nlp:random 0.6587379176410147\n",
      "layout:xla:random 0.35332780456288243\n",
      "epoch 3, it 300000 validation loss -0.357\n",
      "iteration 300100 training loss 0.61195713 lr 0.00021\n",
      "iteration 300200 training loss 0.97099274 lr 0.00021\n",
      "iteration 300300 training loss 0.8068944 lr 0.00021\n",
      "iteration 300400 training loss 1.018847 lr 0.00021\n",
      "iteration 300500 training loss 0.86699176 lr 0.00021\n",
      "iteration 300600 training loss 0.6790977 lr 0.00021\n",
      "iteration 300700 training loss 0.7845202 lr 0.00021\n",
      "iteration 300800 training loss 0.57768035 lr 0.00021\n",
      "iteration 300900 training loss 0.7839217 lr 0.00021\n",
      "iteration 301000 training loss 0.5484201 lr 0.00021\n",
      "iteration 301100 training loss 0.41382226 lr 0.00021\n",
      "iteration 301200 training loss 0.47892284 lr 0.00021\n",
      "iteration 301300 training loss 0.92487645 lr 0.00021\n",
      "iteration 301400 training loss 0.7545367 lr 0.00021\n",
      "iteration 301500 training loss 0.7023696 lr 0.00021\n",
      "iteration 301600 training loss 0.53673184 lr 0.00021\n",
      "iteration 301700 training loss 0.8876111 lr 0.00021\n",
      "iteration 301800 training loss 0.76704997 lr 0.00021\n",
      "iteration 301900 training loss 0.99154454 lr 0.00021\n",
      "iteration 302000 training loss 0.79487187 lr 0.00021\n",
      "iteration 302100 training loss 0.89761084 lr 0.00021\n",
      "iteration 302200 training loss 0.8868736 lr 0.00021\n",
      "iteration 302300 training loss 0.9365777 lr 0.00021\n",
      "iteration 302400 training loss 0.93177587 lr 0.00021\n",
      "iteration 302500 training loss 0.81254077 lr 0.00021\n",
      "iteration 302600 training loss 0.7040853 lr 0.00021\n",
      "iteration 302700 training loss 0.8504476 lr 0.00021\n",
      "iteration 302800 training loss 0.9395704 lr 0.00021\n",
      "iteration 302900 training loss 0.79122376 lr 0.00021\n",
      "iteration 303000 training loss 0.8325805 lr 0.00021\n",
      "iteration 303100 training loss 0.9032578 lr 0.00021\n",
      "iteration 303200 training loss 0.6988005 lr 0.00021\n",
      "iteration 303300 training loss 0.9767376 lr 0.00021\n",
      "iteration 303400 training loss 0.75696814 lr 0.00021\n",
      "iteration 303500 training loss 0.6117972 lr 0.00021\n",
      "iteration 303600 training loss 0.45370921 lr 0.00021\n",
      "iteration 303700 training loss 0.7503768 lr 0.00021\n",
      "iteration 303800 training loss 0.6227781 lr 0.00021\n",
      "iteration 303900 training loss 0.5856099 lr 0.00021\n",
      "iteration 304000 training loss 0.8122899 lr 0.00021\n",
      "iteration 304100 training loss 0.7429273 lr 0.00021\n",
      "iteration 304200 training loss 0.5781012 lr 0.00021\n",
      "iteration 304300 training loss 0.715039 lr 0.00021\n",
      "iteration 304400 training loss 0.72464234 lr 0.00021\n",
      "iteration 304500 training loss 0.39286175 lr 0.00021\n",
      "iteration 304600 training loss 0.70000607 lr 0.00021\n",
      "iteration 304700 training loss 0.43288958 lr 0.00021\n",
      "iteration 304800 training loss 0.88126034 lr 0.00021\n",
      "iteration 304900 training loss 0.551581 lr 0.00021\n",
      "iteration 305000 training loss 0.53925246 lr 0.00021\n",
      "iteration 305100 training loss 0.33932158 lr 0.00021\n",
      "iteration 305200 training loss 0.6163297 lr 0.00021\n",
      "iteration 305300 training loss 0.54480845 lr 0.00021\n",
      "iteration 305400 training loss 0.71668655 lr 0.00021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 305500 training loss 0.43165115 lr 0.00021\n",
      "iteration 305600 training loss 0.9162144 lr 0.00021\n",
      "iteration 305700 training loss 0.5147502 lr 0.00021\n",
      "iteration 305800 training loss 0.8541335 lr 0.00021\n",
      "iteration 305900 training loss 0.5644357 lr 0.00021\n",
      "iteration 306000 training loss 0.7016275 lr 0.00021\n",
      "iteration 306100 training loss 0.6574329 lr 0.00021\n",
      "iteration 306200 training loss 0.5928159 lr 0.00021\n",
      "iteration 306300 training loss 0.76224554 lr 0.00021\n",
      "iteration 306400 training loss 0.5882985 lr 0.00021\n",
      "iteration 306500 training loss 0.6479033 lr 0.00021\n",
      "iteration 306600 training loss 0.73460585 lr 0.00021\n",
      "iteration 306700 training loss 0.83868533 lr 0.00021\n",
      "iteration 306800 training loss 0.6713133 lr 0.00021\n",
      "iteration 306900 training loss 0.39280677 lr 0.00021\n",
      "iteration 307000 training loss 0.7081641 lr 0.00021\n",
      "iteration 307100 training loss 1.0342664 lr 0.00021\n",
      "iteration 307200 training loss 0.47628722 lr 0.00021\n",
      "iteration 307300 training loss 0.17471652 lr 0.00021\n",
      "iteration 307400 training loss 0.45133778 lr 0.00021\n",
      "iteration 307500 training loss 0.40438175 lr 0.00021\n",
      "iteration 307600 training loss 0.32630518 lr 0.00021\n",
      "iteration 307700 training loss 0.7093877 lr 0.00021\n",
      "iteration 307800 training loss 0.4444381 lr 0.00021\n",
      "iteration 307900 training loss 0.49075735 lr 0.00021\n",
      "iteration 308000 training loss 0.517642 lr 0.00021\n",
      "iteration 308100 training loss 0.73299575 lr 0.00021\n",
      "iteration 308200 training loss 0.6313644 lr 0.00021\n",
      "iteration 308300 training loss 0.9938972 lr 0.00021\n",
      "iteration 308400 training loss 0.8541827 lr 0.00021\n",
      "iteration 308500 training loss 0.7252497 lr 0.00021\n",
      "iteration 308600 training loss 0.8518863 lr 0.00021\n",
      "iteration 308700 training loss 0.80126935 lr 0.00021\n",
      "iteration 308800 training loss 1.1244847 lr 0.00021\n",
      "iteration 308900 training loss 0.89907223 lr 0.00021\n",
      "iteration 309000 training loss 1.145941 lr 0.00021\n",
      "iteration 309100 training loss 0.78877574 lr 0.00021\n",
      "iteration 309200 training loss 0.79100335 lr 0.00021\n",
      "iteration 309300 training loss 0.67780846 lr 0.00021\n",
      "iteration 309400 training loss 1.0197166 lr 0.00021\n",
      "iteration 309500 training loss 0.9295198 lr 0.00021\n",
      "iteration 309600 training loss 0.87426364 lr 0.00021\n",
      "iteration 309700 training loss 1.0633839 lr 0.00021\n",
      "iteration 309800 training loss 1.085467 lr 0.00021\n",
      "iteration 309900 training loss 0.9660218 lr 0.00021\n",
      "iteration 310000 training loss 1.000657 lr 0.00021\n",
      "layout:xla:default 0.10844712435025151\n",
      "layout:xla:random 0.32674985872425866\n",
      "layout:nlp:random 0.6637215207512439\n",
      "layout:nlp:default 0.36163736607495\n",
      "epoch 3, it 310000 validation loss -0.365\n",
      "iteration 310100 training loss 0.80537 lr 0.00021\n",
      "iteration 310200 training loss 0.75146854 lr 0.00021\n",
      "iteration 310300 training loss 0.8081237 lr 0.00021\n",
      "iteration 310400 training loss 0.8816707 lr 0.00021\n",
      "iteration 310500 training loss 1.3070467 lr 0.00021\n",
      "iteration 310600 training loss 0.7757763 lr 0.00021\n",
      "iteration 310700 training loss 1.1828585 lr 0.00021\n",
      "iteration 310800 training loss 0.82352805 lr 0.00021\n",
      "iteration 310900 training loss 1.2536652 lr 0.00021\n",
      "iteration 311000 training loss 0.7646023 lr 0.00021\n",
      "iteration 311100 training loss 0.7902425 lr 0.00021\n",
      "iteration 311200 training loss 0.67143875 lr 0.00021\n",
      "iteration 311300 training loss 0.745108 lr 0.00021\n",
      "iteration 311400 training loss 0.74092674 lr 0.00021\n",
      "iteration 311500 training loss 0.77078193 lr 0.00021\n",
      "iteration 311600 training loss 0.6107751 lr 0.00021\n",
      "iteration 311700 training loss 0.7197799 lr 0.00021\n",
      "iteration 311800 training loss 0.9144945 lr 0.00021\n",
      "iteration 311900 training loss 0.6531473 lr 0.00021\n",
      "iteration 312000 training loss 0.661735 lr 0.00021\n",
      "iteration 312100 training loss 0.87825495 lr 0.00021\n",
      "iteration 312200 training loss 0.47095224 lr 0.00021\n",
      "iteration 312300 training loss 0.83307064 lr 0.00021\n",
      "iteration 312400 training loss 0.67219585 lr 0.00021\n",
      "iteration 312500 training loss 0.7611591 lr 0.00021\n",
      "iteration 312600 training loss 0.6703952 lr 0.00021\n",
      "iteration 312700 training loss 0.652934 lr 0.00021\n",
      "iteration 312800 training loss 0.68224716 lr 0.00021\n",
      "iteration 312900 training loss 0.61742204 lr 0.00021\n",
      "iteration 313000 training loss 0.7324752 lr 0.00021\n",
      "iteration 313100 training loss 0.75243795 lr 0.00021\n",
      "iteration 313200 training loss 0.69079596 lr 0.00021\n",
      "iteration 313300 training loss 0.8614345 lr 0.00021\n",
      "iteration 313400 training loss 0.6948696 lr 0.00021\n",
      "iteration 313500 training loss 0.93672353 lr 0.00021\n",
      "iteration 313600 training loss 0.46702614 lr 0.00021\n",
      "iteration 313700 training loss 0.7069507 lr 0.00021\n",
      "iteration 313800 training loss 0.48000565 lr 0.00021\n",
      "iteration 313900 training loss 0.4033567 lr 0.00021\n",
      "iteration 314000 training loss 0.5215503 lr 0.00021\n",
      "iteration 314100 training loss 0.56481016 lr 0.00021\n",
      "iteration 314200 training loss 0.8960894 lr 0.00021\n",
      "iteration 314300 training loss 0.72903246 lr 0.00021\n",
      "iteration 314400 training loss 0.5921304 lr 0.00021\n",
      "iteration 314500 training loss 0.8376153 lr 0.00021\n",
      "iteration 314600 training loss 0.71566176 lr 0.00021\n",
      "iteration 314700 training loss 0.5726772 lr 0.00021\n",
      "iteration 314800 training loss 0.73908323 lr 0.00021\n",
      "iteration 314900 training loss 0.7771329 lr 0.00021\n",
      "iteration 315000 training loss 0.547361 lr 0.00021\n",
      "iteration 315100 training loss 0.44080368 lr 0.00021\n",
      "iteration 315200 training loss 0.69869983 lr 0.00021\n",
      "iteration 315300 training loss 0.68797004 lr 0.00021\n",
      "iteration 315400 training loss 0.478228 lr 0.00021\n",
      "iteration 315500 training loss 0.43725497 lr 0.00021\n",
      "iteration 315600 training loss 0.65705633 lr 0.00021\n",
      "iteration 315700 training loss 0.87577397 lr 0.00021\n",
      "iteration 315800 training loss 0.9854141 lr 0.00021\n",
      "iteration 315900 training loss 1.0451133 lr 0.00021\n",
      "iteration 316000 training loss 0.7644932 lr 0.00021\n",
      "iteration 316100 training loss 0.99559 lr 0.00021\n",
      "iteration 316200 training loss 0.7938019 lr 0.00021\n",
      "iteration 316300 training loss 0.8855165 lr 0.00021\n",
      "iteration 316400 training loss 0.8005686 lr 0.00021\n",
      "iteration 316500 training loss 0.6723373 lr 0.00021\n",
      "iteration 316600 training loss 0.8277059 lr 0.00021\n",
      "iteration 316700 training loss 0.7100369 lr 0.00021\n",
      "iteration 316800 training loss 0.82813907 lr 0.00021\n",
      "iteration 316900 training loss 0.799049 lr 0.00021\n",
      "iteration 317000 training loss 0.82527965 lr 0.00021\n",
      "iteration 317100 training loss 0.6814735 lr 0.00021\n",
      "iteration 317200 training loss 0.80725324 lr 0.00021\n",
      "iteration 317300 training loss 1.0480672 lr 0.00021\n",
      "iteration 317400 training loss 1.1020771 lr 0.00021\n",
      "iteration 317500 training loss 1.1839199 lr 0.00021\n",
      "iteration 317600 training loss 0.92384297 lr 0.00021\n",
      "iteration 317700 training loss 1.2699944 lr 0.00021\n",
      "iteration 317800 training loss 0.84414244 lr 0.00021\n",
      "iteration 317900 training loss 0.6984364 lr 0.00021\n",
      "iteration 318000 training loss 0.9858683 lr 0.00021\n",
      "iteration 318100 training loss 0.7412952 lr 0.00021\n",
      "iteration 318200 training loss 0.7928377 lr 0.00021\n",
      "iteration 318300 training loss 0.8525158 lr 0.00021\n",
      "iteration 318400 training loss 1.1176138 lr 0.00021\n",
      "iteration 318500 training loss 0.8037593 lr 0.00021\n",
      "iteration 318600 training loss 0.7928578 lr 0.00021\n",
      "iteration 318700 training loss 1.0792042 lr 0.00021\n",
      "iteration 318800 training loss 0.9050204 lr 0.00021\n",
      "iteration 318900 training loss 0.8231243 lr 0.00021\n",
      "iteration 319000 training loss 0.7063199 lr 0.00021\n",
      "iteration 319100 training loss 0.807244 lr 0.00021\n",
      "iteration 319200 training loss 0.8884291 lr 0.00021\n",
      "iteration 319300 training loss 0.8825968 lr 0.00021\n",
      "iteration 319400 training loss 0.5444631 lr 0.00021\n",
      "iteration 319500 training loss 0.97282934 lr 0.00021\n",
      "iteration 319600 training loss 0.8703643 lr 0.00021\n",
      "iteration 319700 training loss 0.8386101 lr 0.00021\n",
      "iteration 319800 training loss 0.87339956 lr 0.00021\n",
      "iteration 319900 training loss 0.6910862 lr 0.00021\n",
      "iteration 320000 training loss 0.67990565 lr 0.00021\n",
      "layout:xla:default 0.1431316438391444\n",
      "layout:xla:random 0.38169254097429745\n",
      "layout:nlp:default 0.3550762907909814\n",
      "layout:nlp:random 0.7464477979298964\n",
      "epoch 4, it 320000 validation loss -0.407\n",
      "iteration 320100 training loss 0.55677235 lr 0.00019\n",
      "iteration 320200 training loss 0.8777419 lr 0.00019\n",
      "iteration 320300 training loss 0.6116413 lr 0.00019\n",
      "iteration 320400 training loss 0.9448667 lr 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 320500 training loss 1.1941842 lr 0.00019\n",
      "iteration 320600 training loss 0.90414137 lr 0.00019\n",
      "iteration 320700 training loss 0.63769305 lr 0.00019\n",
      "iteration 320800 training loss 0.65876496 lr 0.00019\n",
      "iteration 320900 training loss 0.5320417 lr 0.00019\n",
      "iteration 321000 training loss 0.7505883 lr 0.00019\n",
      "iteration 321100 training loss 0.6386868 lr 0.00019\n",
      "iteration 321200 training loss 0.6496288 lr 0.00019\n",
      "iteration 321300 training loss 0.5273742 lr 0.00019\n",
      "iteration 321400 training loss 0.77342516 lr 0.00019\n",
      "iteration 321500 training loss 0.67951864 lr 0.00019\n",
      "iteration 321600 training loss 0.72571224 lr 0.00019\n",
      "iteration 321700 training loss 0.5458142 lr 0.00019\n",
      "iteration 321800 training loss 0.7181969 lr 0.00019\n",
      "iteration 321900 training loss 0.7662554 lr 0.00019\n",
      "iteration 322000 training loss 0.7771006 lr 0.00019\n",
      "iteration 322100 training loss 0.9368824 lr 0.00019\n",
      "iteration 322200 training loss 1.0162377 lr 0.00019\n",
      "iteration 322300 training loss 0.74595284 lr 0.00019\n",
      "iteration 322400 training loss 0.8758834 lr 0.00019\n",
      "iteration 322500 training loss 0.6781957 lr 0.00019\n",
      "iteration 322600 training loss 0.67664254 lr 0.00019\n",
      "iteration 322700 training loss 0.9106693 lr 0.00019\n",
      "iteration 322800 training loss 0.63372976 lr 0.00019\n",
      "iteration 322900 training loss 0.91290367 lr 0.00019\n",
      "iteration 323000 training loss 0.9011065 lr 0.00019\n",
      "iteration 323100 training loss 0.58496004 lr 0.00019\n",
      "iteration 323200 training loss 0.68142843 lr 0.00019\n",
      "iteration 323300 training loss 0.86241096 lr 0.00019\n",
      "iteration 323400 training loss 0.8898707 lr 0.00019\n",
      "iteration 323500 training loss 0.59056115 lr 0.00019\n",
      "iteration 323600 training loss 0.83395886 lr 0.00019\n",
      "iteration 323700 training loss 0.94095564 lr 0.00019\n",
      "iteration 323800 training loss 1.0534863 lr 0.00019\n",
      "iteration 323900 training loss 0.72317666 lr 0.00019\n",
      "iteration 324000 training loss 1.1024096 lr 0.00019\n",
      "iteration 324100 training loss 0.97972655 lr 0.00019\n",
      "iteration 324200 training loss 0.85359067 lr 0.00019\n",
      "iteration 324300 training loss 0.8093496 lr 0.00019\n",
      "iteration 324400 training loss 1.0129247 lr 0.00019\n",
      "iteration 324500 training loss 1.0942045 lr 0.00019\n",
      "iteration 324600 training loss 0.76527095 lr 0.00019\n",
      "iteration 324700 training loss 0.9902972 lr 0.00019\n",
      "iteration 324800 training loss 0.7200086 lr 0.00019\n",
      "iteration 324900 training loss 0.7639795 lr 0.00019\n",
      "iteration 325000 training loss 0.80633754 lr 0.00019\n",
      "iteration 325100 training loss 0.44755945 lr 0.00019\n",
      "iteration 325200 training loss 0.8157675 lr 0.00019\n",
      "iteration 325300 training loss 0.80393624 lr 0.00019\n",
      "iteration 325400 training loss 0.8678762 lr 0.00019\n",
      "iteration 325500 training loss 0.5600921 lr 0.00019\n",
      "iteration 325600 training loss 0.62738824 lr 0.00019\n",
      "iteration 325700 training loss 0.8479499 lr 0.00019\n",
      "iteration 325800 training loss 0.691984 lr 0.00019\n",
      "iteration 325900 training loss 0.7731042 lr 0.00019\n",
      "iteration 326000 training loss 0.57008433 lr 0.00019\n",
      "iteration 326100 training loss 0.7688162 lr 0.00019\n",
      "iteration 326200 training loss 0.7075194 lr 0.00019\n",
      "iteration 326300 training loss 0.64547336 lr 0.00019\n",
      "iteration 326400 training loss 0.5395078 lr 0.00019\n",
      "iteration 326500 training loss 0.51112205 lr 0.00019\n",
      "iteration 326600 training loss 0.79498357 lr 0.00019\n",
      "iteration 326700 training loss 0.72597265 lr 0.00019\n",
      "iteration 326800 training loss 0.62553996 lr 0.00019\n",
      "iteration 326900 training loss 0.8496643 lr 0.00019\n",
      "iteration 327000 training loss 0.48804155 lr 0.00019\n",
      "iteration 327100 training loss 0.85606086 lr 0.00019\n",
      "iteration 327200 training loss 0.6180224 lr 0.00019\n",
      "iteration 327300 training loss 0.634203 lr 0.00019\n",
      "iteration 327400 training loss 0.6907427 lr 0.00019\n",
      "iteration 327500 training loss 0.5138932 lr 0.00019\n",
      "iteration 327600 training loss 1.6573846 lr 0.00019\n",
      "iteration 327700 training loss 0.8508929 lr 0.00019\n",
      "iteration 327800 training loss 0.51677704 lr 0.00019\n",
      "iteration 327900 training loss 0.7517052 lr 0.00019\n",
      "iteration 328000 training loss 0.6714932 lr 0.00019\n",
      "iteration 328100 training loss 0.5997379 lr 0.00019\n",
      "iteration 328200 training loss 0.94035673 lr 0.00019\n",
      "iteration 328300 training loss 0.8356934 lr 0.00019\n",
      "iteration 328400 training loss 0.9079515 lr 0.00019\n",
      "iteration 328500 training loss 0.90740204 lr 0.00019\n",
      "iteration 328600 training loss 0.7398841 lr 0.00019\n",
      "iteration 328700 training loss 0.81075823 lr 0.00019\n",
      "iteration 328800 training loss 0.74136037 lr 0.00019\n",
      "iteration 328900 training loss 0.97356737 lr 0.00019\n",
      "iteration 329000 training loss 0.88468045 lr 0.00019\n",
      "iteration 329100 training loss 0.76913184 lr 0.00019\n",
      "iteration 329200 training loss 1.0626067 lr 0.00019\n",
      "iteration 329300 training loss 0.8529306 lr 0.00019\n",
      "iteration 329400 training loss 0.88558054 lr 0.00019\n",
      "iteration 329500 training loss 1.0090581 lr 0.00019\n",
      "iteration 329600 training loss 0.8530226 lr 0.00019\n",
      "iteration 329700 training loss 0.668319 lr 0.00019\n",
      "iteration 329800 training loss 0.8443982 lr 0.00019\n",
      "iteration 329900 training loss 0.92494756 lr 0.00019\n",
      "iteration 330000 training loss 0.7024676 lr 0.00019\n",
      "layout:xla:default 0.10592296059663711\n",
      "layout:xla:random 0.27134724136389926\n",
      "layout:nlp:default 0.3773811817222559\n",
      "layout:nlp:random 0.7141979362824225\n",
      "epoch 4, it 330000 validation loss -0.367\n",
      "iteration 330100 training loss 0.5717568 lr 0.00019\n",
      "iteration 330200 training loss 0.81236655 lr 0.00019\n",
      "iteration 330300 training loss 0.812213 lr 0.00019\n",
      "iteration 330400 training loss 0.98873484 lr 0.00019\n",
      "iteration 330500 training loss 0.5657258 lr 0.00019\n",
      "iteration 330600 training loss 0.7715691 lr 0.00019\n",
      "iteration 330700 training loss 1.0321544 lr 0.00019\n",
      "iteration 330800 training loss 0.71259415 lr 0.00019\n",
      "iteration 330900 training loss 0.72698605 lr 0.00019\n",
      "iteration 331000 training loss 1.0025066 lr 0.00019\n",
      "iteration 331100 training loss 0.55802107 lr 0.00019\n",
      "iteration 331200 training loss 0.574004 lr 0.00019\n",
      "iteration 331300 training loss 0.64470667 lr 0.00019\n",
      "iteration 331400 training loss 0.61714077 lr 0.00019\n",
      "iteration 331500 training loss 0.63921976 lr 0.00019\n",
      "iteration 331600 training loss 0.62885326 lr 0.00019\n",
      "iteration 331700 training loss 0.76052415 lr 0.00019\n",
      "iteration 331800 training loss 0.77893925 lr 0.00019\n",
      "iteration 331900 training loss 0.76957256 lr 0.00019\n",
      "iteration 332000 training loss 0.6712827 lr 0.00019\n",
      "iteration 332100 training loss 0.77756566 lr 0.00019\n",
      "iteration 332200 training loss 0.7678745 lr 0.00019\n",
      "iteration 332300 training loss 0.89293593 lr 0.00019\n",
      "iteration 332400 training loss 0.614874 lr 0.00019\n",
      "iteration 332500 training loss 0.89517874 lr 0.00019\n",
      "iteration 332600 training loss 1.1159267 lr 0.00019\n",
      "iteration 332700 training loss 0.5825445 lr 0.00019\n",
      "iteration 332800 training loss 0.78102475 lr 0.00019\n",
      "iteration 332900 training loss 0.82467216 lr 0.00019\n",
      "iteration 333000 training loss 0.8765772 lr 0.00019\n",
      "iteration 333100 training loss 0.9188711 lr 0.00019\n",
      "iteration 333200 training loss 0.79775345 lr 0.00019\n",
      "iteration 333300 training loss 0.8608024 lr 0.00019\n",
      "iteration 333400 training loss 0.610647 lr 0.00019\n",
      "iteration 333500 training loss 0.5153517 lr 0.00019\n",
      "iteration 333600 training loss 0.7489223 lr 0.00019\n",
      "iteration 333700 training loss 0.916577 lr 0.00019\n",
      "iteration 333800 training loss 0.9474176 lr 0.00019\n",
      "iteration 333900 training loss 0.71726763 lr 0.00019\n",
      "iteration 334000 training loss 0.757534 lr 0.00019\n",
      "iteration 334100 training loss 0.5172214 lr 0.00019\n",
      "iteration 334200 training loss 0.92856264 lr 0.00019\n",
      "iteration 334300 training loss 0.924472 lr 0.00019\n",
      "iteration 334400 training loss 0.9626555 lr 0.00019\n",
      "iteration 334500 training loss 0.70503646 lr 0.00019\n",
      "iteration 334600 training loss 0.7923646 lr 0.00019\n",
      "iteration 334700 training loss 0.68389475 lr 0.00019\n",
      "iteration 334800 training loss 0.7108981 lr 0.00019\n",
      "iteration 334900 training loss 0.6960344 lr 0.00019\n",
      "iteration 335000 training loss 0.6132629 lr 0.00019\n",
      "iteration 335100 training loss 0.9309927 lr 0.00019\n",
      "iteration 335200 training loss 0.751905 lr 0.00019\n",
      "iteration 335300 training loss 0.9724363 lr 0.00019\n",
      "iteration 335400 training loss 0.8473419 lr 0.00019\n",
      "iteration 335500 training loss 0.78979856 lr 0.00019\n",
      "iteration 335600 training loss 0.7203469 lr 0.00019\n",
      "iteration 335700 training loss 0.7040737 lr 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 335800 training loss 0.7900518 lr 0.00019\n",
      "iteration 335900 training loss 0.66329515 lr 0.00019\n",
      "iteration 336000 training loss 0.57067466 lr 0.00019\n",
      "iteration 336100 training loss 0.77573884 lr 0.00019\n",
      "iteration 336200 training loss 0.88366467 lr 0.00019\n",
      "iteration 336300 training loss 0.8270088 lr 0.00019\n",
      "iteration 336400 training loss 0.9013302 lr 0.00019\n",
      "iteration 336500 training loss 0.9618534 lr 0.00019\n",
      "iteration 336600 training loss 0.9757745 lr 0.00019\n",
      "iteration 336700 training loss 0.9297878 lr 0.00019\n",
      "iteration 336800 training loss 0.74982315 lr 0.00019\n",
      "iteration 336900 training loss 0.6672216 lr 0.00019\n",
      "iteration 337000 training loss 0.89475673 lr 0.00019\n",
      "iteration 337100 training loss 0.8789794 lr 0.00019\n",
      "iteration 337200 training loss 0.7288328 lr 0.00019\n",
      "iteration 337300 training loss 0.813778 lr 0.00019\n",
      "iteration 337400 training loss 0.7626294 lr 0.00019\n",
      "iteration 337500 training loss 0.9197431 lr 0.00019\n",
      "iteration 337600 training loss 0.8061287 lr 0.00019\n",
      "iteration 337700 training loss 0.8712235 lr 0.00019\n",
      "iteration 337800 training loss 0.8626566 lr 0.00019\n",
      "iteration 337900 training loss 0.79780537 lr 0.00019\n",
      "iteration 338000 training loss 0.7920904 lr 0.00019\n",
      "iteration 338100 training loss 0.9187849 lr 0.00019\n",
      "iteration 338200 training loss 0.7719666 lr 0.00019\n",
      "iteration 338300 training loss 0.7343328 lr 0.00019\n",
      "iteration 338400 training loss 0.5843173 lr 0.00019\n",
      "iteration 338500 training loss 0.5443093 lr 0.00019\n",
      "iteration 338600 training loss 1.1533152 lr 0.00019\n",
      "iteration 338700 training loss 0.46827075 lr 0.00019\n",
      "iteration 338800 training loss 0.5376391 lr 0.00019\n",
      "iteration 338900 training loss 0.9022606 lr 0.00019\n",
      "iteration 339000 training loss 0.4269034 lr 0.00019\n",
      "iteration 339100 training loss 0.60718286 lr 0.00019\n",
      "iteration 339200 training loss 0.7912106 lr 0.00019\n",
      "iteration 339300 training loss 0.9742233 lr 0.00019\n",
      "iteration 339400 training loss 1.0920507 lr 0.00019\n",
      "iteration 339500 training loss 0.82210773 lr 0.00019\n",
      "iteration 339600 training loss 0.9030501 lr 0.00019\n",
      "iteration 339700 training loss 0.66459227 lr 0.00019\n",
      "iteration 339800 training loss 0.72082186 lr 0.00019\n",
      "iteration 339900 training loss 0.88300836 lr 0.00019\n",
      "iteration 340000 training loss 0.54401267 lr 0.00019\n",
      "layout:nlp:random 0.7525058463008587\n",
      "layout:xla:random 0.34267876526079616\n",
      "layout:xla:default 0.05054048416496594\n",
      "layout:nlp:default 0.3480301156925295\n",
      "epoch 4, it 340000 validation loss -0.373\n",
      "iteration 340100 training loss 0.623414 lr 0.00017\n",
      "iteration 340200 training loss 0.73529863 lr 0.00017\n",
      "iteration 340300 training loss 0.6225494 lr 0.00017\n",
      "iteration 340400 training loss 0.5724533 lr 0.00017\n",
      "iteration 340500 training loss 0.79270256 lr 0.00017\n",
      "iteration 340600 training loss 0.6433794 lr 0.00017\n",
      "iteration 340700 training loss 0.7625742 lr 0.00017\n",
      "iteration 340800 training loss 0.7874928 lr 0.00017\n",
      "iteration 340900 training loss 0.7488573 lr 0.00017\n",
      "iteration 341000 training loss 0.72966015 lr 0.00017\n",
      "iteration 341100 training loss 0.5171024 lr 0.00017\n",
      "iteration 341200 training loss 0.7538432 lr 0.00017\n",
      "iteration 341300 training loss 0.8046039 lr 0.00017\n",
      "iteration 341400 training loss 0.8426577 lr 0.00017\n",
      "iteration 341500 training loss 0.49740782 lr 0.00017\n",
      "iteration 341600 training loss 0.802986 lr 0.00017\n",
      "iteration 341700 training loss 0.8839825 lr 0.00017\n",
      "iteration 341800 training loss 0.8877519 lr 0.00017\n",
      "iteration 341900 training loss 0.8941289 lr 0.00017\n",
      "iteration 342000 training loss 1.0230561 lr 0.00017\n",
      "iteration 342100 training loss 0.8772568 lr 0.00017\n",
      "iteration 342200 training loss 0.7769143 lr 0.00017\n",
      "iteration 342300 training loss 0.6141486 lr 0.00017\n",
      "iteration 342400 training loss 0.70580184 lr 0.00017\n",
      "iteration 342500 training loss 0.60835266 lr 0.00017\n",
      "iteration 342600 training loss 0.85804784 lr 0.00017\n",
      "iteration 342700 training loss 1.0345669 lr 0.00017\n",
      "iteration 342800 training loss 0.905924 lr 0.00017\n",
      "iteration 342900 training loss 0.81804425 lr 0.00017\n",
      "iteration 343000 training loss 0.7517043 lr 0.00017\n",
      "iteration 343100 training loss 0.77878034 lr 0.00017\n",
      "iteration 343200 training loss 0.8476001 lr 0.00017\n",
      "iteration 343300 training loss 0.8205295 lr 0.00017\n",
      "iteration 343400 training loss 0.7846942 lr 0.00017\n",
      "iteration 343500 training loss 0.8767564 lr 0.00017\n",
      "iteration 343600 training loss 0.8568358 lr 0.00017\n",
      "iteration 343700 training loss 0.9205912 lr 0.00017\n",
      "iteration 343800 training loss 0.58035195 lr 0.00017\n",
      "iteration 343900 training loss 0.98806214 lr 0.00017\n",
      "iteration 344000 training loss 0.57022566 lr 0.00017\n",
      "iteration 344100 training loss 0.99876124 lr 0.00017\n",
      "iteration 344200 training loss 0.74178123 lr 0.00017\n",
      "iteration 344300 training loss 0.8462385 lr 0.00017\n",
      "iteration 344400 training loss 0.91570413 lr 0.00017\n",
      "iteration 344500 training loss 0.79557514 lr 0.00017\n",
      "iteration 344600 training loss 0.58976394 lr 0.00017\n",
      "iteration 344700 training loss 0.5978631 lr 0.00017\n",
      "iteration 344800 training loss 0.68670744 lr 0.00017\n",
      "iteration 344900 training loss 0.5853127 lr 0.00017\n",
      "iteration 345000 training loss 0.74739957 lr 0.00017\n",
      "iteration 345100 training loss 0.70615786 lr 0.00017\n",
      "iteration 345200 training loss 0.8455849 lr 0.00017\n",
      "iteration 345300 training loss 0.60001194 lr 0.00017\n",
      "iteration 345400 training loss 0.86638457 lr 0.00017\n",
      "iteration 345500 training loss 0.88719773 lr 0.00017\n",
      "iteration 345600 training loss 1.2100695 lr 0.00017\n",
      "iteration 345700 training loss 0.6918877 lr 0.00017\n",
      "iteration 345800 training loss 0.76907355 lr 0.00017\n",
      "iteration 345900 training loss 1.1511929 lr 0.00017\n",
      "iteration 346000 training loss 0.6499136 lr 0.00017\n",
      "iteration 346100 training loss 0.6741128 lr 0.00017\n",
      "iteration 346200 training loss 0.8026748 lr 0.00017\n",
      "iteration 346300 training loss 0.85484266 lr 0.00017\n",
      "iteration 346400 training loss 0.6585535 lr 0.00017\n",
      "iteration 346500 training loss 0.7496081 lr 0.00017\n",
      "iteration 346600 training loss 0.6730273 lr 0.00017\n",
      "iteration 346700 training loss 0.90678674 lr 0.00017\n",
      "iteration 346800 training loss 0.7801277 lr 0.00017\n",
      "iteration 346900 training loss 0.7484346 lr 0.00017\n",
      "iteration 347000 training loss 0.75713485 lr 0.00017\n",
      "iteration 347100 training loss 1.023673 lr 0.00017\n",
      "iteration 347200 training loss 1.1013305 lr 0.00017\n",
      "iteration 347300 training loss 0.7808313 lr 0.00017\n",
      "iteration 347400 training loss 0.7008855 lr 0.00017\n",
      "iteration 347500 training loss 0.60390097 lr 0.00017\n",
      "iteration 347600 training loss 0.5262441 lr 0.00017\n",
      "iteration 347700 training loss 0.7527811 lr 0.00017\n",
      "iteration 347800 training loss 0.5912682 lr 0.00017\n",
      "iteration 347900 training loss 0.8002616 lr 0.00017\n",
      "iteration 348000 training loss 0.98704946 lr 0.00017\n",
      "iteration 348100 training loss 0.56324303 lr 0.00017\n",
      "iteration 348200 training loss 0.6914478 lr 0.00017\n",
      "iteration 348300 training loss 0.72285885 lr 0.00017\n",
      "iteration 348400 training loss 0.5984163 lr 0.00017\n",
      "iteration 348500 training loss 0.35841405 lr 0.00017\n",
      "iteration 348600 training loss 0.6006107 lr 0.00017\n",
      "iteration 348700 training loss 0.686764 lr 0.00017\n",
      "iteration 348800 training loss 0.7111373 lr 0.00017\n",
      "iteration 348900 training loss 0.47515008 lr 0.00017\n",
      "iteration 349000 training loss 0.5689203 lr 0.00017\n",
      "iteration 349100 training loss 0.77370363 lr 0.00017\n",
      "iteration 349200 training loss 0.63981265 lr 0.00017\n",
      "iteration 349300 training loss 0.64782715 lr 0.00017\n",
      "iteration 349400 training loss 0.48437265 lr 0.00017\n",
      "iteration 349500 training loss 0.62093514 lr 0.00017\n",
      "iteration 349600 training loss 0.6007916 lr 0.00017\n",
      "iteration 349700 training loss 0.58612597 lr 0.00017\n",
      "iteration 349800 training loss 0.61103374 lr 0.00017\n",
      "iteration 349900 training loss 0.6260723 lr 0.00017\n",
      "iteration 350000 training loss 0.9906158 lr 0.00017\n",
      "layout:xla:default 0.07244591628016095\n",
      "layout:xla:random 0.3943218683801861\n",
      "layout:nlp:random 0.7753210923748661\n",
      "layout:nlp:default 0.36487487584876666\n",
      "epoch 4, it 350000 validation loss -0.402\n",
      "iteration 350100 training loss 0.8664526 lr 0.00017\n",
      "iteration 350200 training loss 0.8735757 lr 0.00017\n",
      "iteration 350300 training loss 0.6660315 lr 0.00017\n",
      "iteration 350400 training loss 0.50268996 lr 0.00017\n",
      "iteration 350500 training loss 0.5748622 lr 0.00017\n",
      "iteration 350600 training loss 0.4966255 lr 0.00017\n",
      "iteration 350700 training loss 0.7235637 lr 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 350800 training loss 0.6773884 lr 0.00017\n",
      "iteration 350900 training loss 0.70145553 lr 0.00017\n",
      "iteration 351000 training loss 1.0855219 lr 0.00017\n",
      "iteration 351100 training loss 0.8060739 lr 0.00017\n",
      "iteration 351200 training loss 0.85780704 lr 0.00017\n",
      "iteration 351300 training loss 0.9267133 lr 0.00017\n",
      "iteration 351400 training loss 0.9762705 lr 0.00017\n",
      "iteration 351500 training loss 1.0084265 lr 0.00017\n",
      "iteration 351600 training loss 0.76519454 lr 0.00017\n",
      "iteration 351700 training loss 0.69486225 lr 0.00017\n",
      "iteration 351800 training loss 0.9781295 lr 0.00017\n",
      "iteration 351900 training loss 0.8001758 lr 0.00017\n",
      "iteration 352000 training loss 0.71105945 lr 0.00017\n",
      "iteration 352100 training loss 0.9042968 lr 0.00017\n",
      "iteration 352200 training loss 0.744916 lr 0.00017\n",
      "iteration 352300 training loss 0.8052299 lr 0.00017\n",
      "iteration 352400 training loss 0.54109424 lr 0.00017\n",
      "iteration 352500 training loss 0.72560936 lr 0.00017\n",
      "iteration 352600 training loss 0.63374835 lr 0.00017\n",
      "iteration 352700 training loss 0.8377696 lr 0.00017\n",
      "iteration 352800 training loss 0.47364575 lr 0.00017\n",
      "iteration 352900 training loss 0.6707175 lr 0.00017\n",
      "iteration 353000 training loss 0.70065576 lr 0.00017\n",
      "iteration 353100 training loss 0.8317797 lr 0.00017\n",
      "iteration 353200 training loss 0.9489 lr 0.00017\n",
      "iteration 353300 training loss 0.9077706 lr 0.00017\n",
      "iteration 353400 training loss 0.733907 lr 0.00017\n",
      "iteration 353500 training loss 0.58432907 lr 0.00017\n",
      "iteration 353600 training loss 0.6666476 lr 0.00017\n",
      "iteration 353700 training loss 1.0042535 lr 0.00017\n",
      "iteration 353800 training loss 0.8794522 lr 0.00017\n",
      "iteration 353900 training loss 0.97742707 lr 0.00017\n",
      "iteration 354000 training loss 0.86317384 lr 0.00017\n",
      "iteration 354100 training loss 0.5244775 lr 0.00017\n",
      "iteration 354200 training loss 0.71943665 lr 0.00017\n",
      "iteration 354300 training loss 0.898504 lr 0.00017\n",
      "iteration 354400 training loss 0.9608394 lr 0.00017\n",
      "iteration 354500 training loss 0.84319234 lr 0.00017\n",
      "iteration 354600 training loss 0.97955525 lr 0.00017\n",
      "iteration 354700 training loss 0.9418129 lr 0.00017\n",
      "iteration 354800 training loss 1.0323433 lr 0.00017\n",
      "iteration 354900 training loss 0.9077614 lr 0.00017\n",
      "iteration 355000 training loss 0.6091319 lr 0.00017\n",
      "iteration 355100 training loss 0.7280175 lr 0.00017\n",
      "iteration 355200 training loss 0.7382626 lr 0.00017\n",
      "iteration 355300 training loss 0.69368035 lr 0.00017\n",
      "iteration 355400 training loss 0.674237 lr 0.00017\n",
      "iteration 355500 training loss 0.6104365 lr 0.00017\n",
      "iteration 355600 training loss 0.84242177 lr 0.00017\n",
      "iteration 355700 training loss 0.8706722 lr 0.00017\n",
      "iteration 355800 training loss 0.7649758 lr 0.00017\n",
      "iteration 355900 training loss 0.67134 lr 0.00017\n",
      "iteration 356000 training loss 0.65981317 lr 0.00017\n",
      "iteration 356100 training loss 0.7176467 lr 0.00017\n",
      "iteration 356200 training loss 0.82119584 lr 0.00017\n",
      "iteration 356300 training loss 0.860104 lr 0.00017\n",
      "iteration 356400 training loss 0.7975355 lr 0.00017\n",
      "iteration 356500 training loss 0.6912322 lr 0.00017\n",
      "iteration 356600 training loss 0.82254404 lr 0.00017\n",
      "iteration 356700 training loss 0.99449843 lr 0.00017\n",
      "iteration 356800 training loss 0.7001989 lr 0.00017\n",
      "iteration 356900 training loss 0.86465734 lr 0.00017\n",
      "iteration 357000 training loss 0.82133335 lr 0.00017\n",
      "iteration 357100 training loss 0.7891187 lr 0.00017\n",
      "iteration 357200 training loss 0.60863715 lr 0.00017\n",
      "iteration 357300 training loss 0.6374379 lr 0.00017\n",
      "iteration 357400 training loss 0.5467965 lr 0.00017\n",
      "iteration 357500 training loss 0.5052999 lr 0.00017\n",
      "iteration 357600 training loss 0.68731433 lr 0.00017\n",
      "iteration 357700 training loss 0.46967188 lr 0.00017\n",
      "iteration 357800 training loss 0.677312 lr 0.00017\n",
      "iteration 357900 training loss 0.74005747 lr 0.00017\n",
      "iteration 358000 training loss 0.73670346 lr 0.00017\n",
      "iteration 358100 training loss 0.5310989 lr 0.00017\n",
      "iteration 358200 training loss 0.46539724 lr 0.00017\n",
      "iteration 358300 training loss 0.5399998 lr 0.00017\n",
      "iteration 358400 training loss 0.6774019 lr 0.00017\n",
      "iteration 358500 training loss 0.6413733 lr 0.00017\n",
      "iteration 358600 training loss 0.46437 lr 0.00017\n",
      "iteration 358700 training loss 0.40526354 lr 0.00017\n",
      "iteration 358800 training loss 0.55562794 lr 0.00017\n",
      "iteration 358900 training loss 0.44250354 lr 0.00017\n",
      "iteration 359000 training loss 0.63363045 lr 0.00017\n",
      "iteration 359100 training loss 0.483571 lr 0.00017\n",
      "iteration 359200 training loss 0.6942916 lr 0.00017\n",
      "iteration 359300 training loss 0.6248768 lr 0.00017\n",
      "iteration 359400 training loss 0.7434817 lr 0.00017\n",
      "iteration 359500 training loss 0.8093529 lr 0.00017\n",
      "iteration 359600 training loss 0.70519817 lr 0.00017\n",
      "iteration 359700 training loss 0.58654046 lr 0.00017\n",
      "iteration 359800 training loss 0.844802 lr 0.00017\n",
      "iteration 359900 training loss 0.6842455 lr 0.00017\n",
      "iteration 360000 training loss 0.7812525 lr 0.00017\n",
      "layout:xla:default 0.09497635282746952\n",
      "layout:nlp:default 0.37430742199383454\n",
      "layout:nlp:random 0.7484510612931659\n",
      "layout:xla:random 0.40072722185096243\n",
      "epoch 4, it 360000 validation loss -0.405\n",
      "iteration 360100 training loss 0.7535339 lr 0.00015\n",
      "iteration 360200 training loss 1.0032363 lr 0.00015\n",
      "iteration 360300 training loss 0.74164915 lr 0.00015\n",
      "iteration 360400 training loss 0.55488926 lr 0.00015\n",
      "iteration 360500 training loss 0.9613394 lr 0.00015\n",
      "iteration 360600 training loss 0.74558455 lr 0.00015\n",
      "iteration 360700 training loss 0.9939474 lr 0.00015\n",
      "iteration 360800 training loss 1.1207612 lr 0.00015\n",
      "iteration 360900 training loss 1.0190009 lr 0.00015\n",
      "iteration 361000 training loss 0.7641438 lr 0.00015\n",
      "iteration 361100 training loss 0.5700583 lr 0.00015\n",
      "iteration 361200 training loss 0.80912054 lr 0.00015\n",
      "iteration 361300 training loss 0.7092307 lr 0.00015\n",
      "iteration 361400 training loss 0.74817145 lr 0.00015\n",
      "iteration 361500 training loss 0.63408947 lr 0.00015\n",
      "iteration 361600 training loss 0.7604515 lr 0.00015\n",
      "iteration 361700 training loss 0.78632474 lr 0.00015\n",
      "iteration 361800 training loss 0.91449875 lr 0.00015\n",
      "iteration 361900 training loss 0.8772467 lr 0.00015\n",
      "iteration 362000 training loss 1.0741795 lr 0.00015\n",
      "iteration 362100 training loss 1.03069 lr 0.00015\n",
      "iteration 362200 training loss 1.0600227 lr 0.00015\n",
      "iteration 362300 training loss 0.83784044 lr 0.00015\n",
      "iteration 362400 training loss 0.7957674 lr 0.00015\n",
      "iteration 362500 training loss 0.7712967 lr 0.00015\n",
      "iteration 362600 training loss 0.85725397 lr 0.00015\n",
      "iteration 362700 training loss 0.7157959 lr 0.00015\n",
      "iteration 362800 training loss 0.7938367 lr 0.00015\n",
      "iteration 362900 training loss 0.80109036 lr 0.00015\n",
      "iteration 363000 training loss 0.93711233 lr 0.00015\n",
      "iteration 363100 training loss 0.90688413 lr 0.00015\n",
      "iteration 363200 training loss 0.7710015 lr 0.00015\n",
      "iteration 363300 training loss 0.9671595 lr 0.00015\n",
      "iteration 363400 training loss 0.93104863 lr 0.00015\n",
      "iteration 363500 training loss 0.9435387 lr 0.00015\n",
      "iteration 363600 training loss 0.79054475 lr 0.00015\n",
      "iteration 363700 training loss 0.6904541 lr 0.00015\n",
      "iteration 363800 training loss 0.6544534 lr 0.00015\n",
      "iteration 363900 training loss 0.91108805 lr 0.00015\n",
      "iteration 364000 training loss 0.6668177 lr 0.00015\n",
      "iteration 364100 training loss 0.4135395 lr 0.00015\n",
      "iteration 364200 training loss 0.45664862 lr 0.00015\n",
      "iteration 364300 training loss 0.7101134 lr 0.00015\n",
      "iteration 364400 training loss 0.8631175 lr 0.00015\n",
      "iteration 364500 training loss 0.77421266 lr 0.00015\n",
      "iteration 364600 training loss 0.5594787 lr 0.00015\n",
      "iteration 364700 training loss 0.5083703 lr 0.00015\n",
      "iteration 364800 training loss 0.63551366 lr 0.00015\n",
      "iteration 364900 training loss 0.8571918 lr 0.00015\n",
      "iteration 365000 training loss 0.67195636 lr 0.00015\n",
      "iteration 365100 training loss 0.76942843 lr 0.00015\n",
      "iteration 365200 training loss 0.94467825 lr 0.00015\n",
      "iteration 365300 training loss 0.6341039 lr 0.00015\n",
      "iteration 365400 training loss 0.79444903 lr 0.00015\n",
      "iteration 365500 training loss 0.83998656 lr 0.00015\n",
      "iteration 365600 training loss 0.7428883 lr 0.00015\n",
      "iteration 365700 training loss 0.8865221 lr 0.00015\n",
      "iteration 365800 training loss 0.8272485 lr 0.00015\n",
      "iteration 365900 training loss 0.8871143 lr 0.00015\n",
      "iteration 366000 training loss 0.7723581 lr 0.00015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 366100 training loss 0.78052694 lr 0.00015\n",
      "iteration 366200 training loss 0.9634779 lr 0.00015\n",
      "iteration 366300 training loss 0.86488825 lr 0.00015\n",
      "iteration 366400 training loss 0.75379926 lr 0.00015\n",
      "iteration 366500 training loss 0.7699041 lr 0.00015\n",
      "iteration 366600 training loss 0.7226995 lr 0.00015\n",
      "iteration 366700 training loss 0.8293307 lr 0.00015\n",
      "iteration 366800 training loss 0.7611094 lr 0.00015\n",
      "iteration 366900 training loss 0.8558017 lr 0.00015\n",
      "iteration 367000 training loss 0.9581557 lr 0.00015\n",
      "iteration 367100 training loss 0.7456766 lr 0.00015\n",
      "iteration 367200 training loss 0.8009522 lr 0.00015\n",
      "iteration 367300 training loss 0.5809332 lr 0.00015\n",
      "iteration 367400 training loss 0.4344338 lr 0.00015\n",
      "iteration 367500 training loss 0.719432 lr 0.00015\n",
      "iteration 367600 training loss 0.7833617 lr 0.00015\n",
      "iteration 367700 training loss 0.7470369 lr 0.00015\n",
      "iteration 367800 training loss 1.0936007 lr 0.00015\n",
      "iteration 367900 training loss 0.9006301 lr 0.00015\n",
      "iteration 368000 training loss 0.9175462 lr 0.00015\n",
      "iteration 368100 training loss 0.7598508 lr 0.00015\n",
      "iteration 368200 training loss 0.90733534 lr 0.00015\n",
      "iteration 368300 training loss 0.91679806 lr 0.00015\n",
      "iteration 368400 training loss 0.7133627 lr 0.00015\n",
      "iteration 368500 training loss 0.8341215 lr 0.00015\n",
      "iteration 368600 training loss 1.0264848 lr 0.00015\n",
      "iteration 368700 training loss 0.7808118 lr 0.00015\n",
      "iteration 368800 training loss 0.8187468 lr 0.00015\n",
      "iteration 368900 training loss 0.9564151 lr 0.00015\n",
      "iteration 369000 training loss 0.8600066 lr 0.00015\n",
      "iteration 369100 training loss 0.68496966 lr 0.00015\n",
      "iteration 369200 training loss 0.83962274 lr 0.00015\n",
      "iteration 369300 training loss 1.094027 lr 0.00015\n",
      "iteration 369400 training loss 0.5170876 lr 0.00015\n",
      "iteration 369500 training loss 0.73869646 lr 0.00015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/kaggle_model_runtime/models.py:106\u001b[0m, in \u001b[0;36mMLP.train\u001b[0;34m(self, training_dataset, validation_dataset)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m training_dataset:\n\u001b[0;32m--> 106\u001b[0m         training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;66;03m# TODO: use tensorboard -> tune learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.train(dataset.train_data, dataset.valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.normalization_layer_config_nodes.mean.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba29597",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mlp.dense_layer_3.kernel.numpy().flatten(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c005ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tile_ids, config_indexes, config_descriptors, valid_mask, graph_descriptor, normalized_runtimes in dataset.train_data:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tile_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_descriptors.numpy()[5, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mlp.normalization_layer_config_nodes(config_descriptors)\n",
    "x = mlp.dense_layer_1(x)\n",
    "x = mlp.relu_layer(x)  # (batch_size, n_config_nodes_upper_limit, n_units)\n",
    "\n",
    "float_mask = tf.cast(valid_mask, tf.float32)  # (batch_size, n_config_nodes_upper_limit)\n",
    "float_mask = tf.expand_dims(float_mask, axis=-1)\n",
    "x = x * float_mask\n",
    "x = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "normal_graph_descriptor = mlp.normalization_layer_graph_descriptor(graph_descriptor)\n",
    "x = tf.concat([x, normal_graph_descriptor], axis=-1)\n",
    "x = mlp.dense_layer_2(x)\n",
    "x = mlp.relu_layer(x)\n",
    "x = mlp.dense_layer_3(x)\n",
    "x = tf.reshape(x, (-1,))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ea294",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.random.permutation(np.arange(10))\n",
    "new_order = order.copy()\n",
    "new_order[0] = order[1]\n",
    "new_order[1] = order[0]\n",
    "kendalltau(order, new_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3418de",
   "metadata": {},
   "source": [
    "## Evaluate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac0e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = mlp.predict_over_dataset(dataset.valid_data, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043565f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>492038.000000</td>\n",
       "      <td>492038.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-112.697769</td>\n",
       "      <td>16.887457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>602.293213</td>\n",
       "      <td>2.030228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1973.848999</td>\n",
       "      <td>13.622271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-87.980957</td>\n",
       "      <td>15.501094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-33.737616</td>\n",
       "      <td>16.255013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.880163</td>\n",
       "      <td>17.911939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>905.975952</td>\n",
       "      <td>23.752274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          prediction         target\n",
       "count  492038.000000  492038.000000\n",
       "mean     -112.697769      16.887457\n",
       "std       602.293213       2.030228\n",
       "min     -1973.848999      13.622271\n",
       "25%       -87.980957      15.501094\n",
       "50%       -33.737616      16.255013\n",
       "75%         7.880163      17.911939\n",
       "max       905.975952      23.752274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[['prediction', 'target']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1d9033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(val_df['target'], bins=50)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(val_df['prediction'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11aff8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'layout:xla:random:bert_pretraining.4x4.fp16',\n",
       "       b'layout:xla:default:mlperf_bert_batch_24_2x2',\n",
       "       b'layout:xla:default:resnet50.4x4.fp16',\n",
       "       b'layout:xla:random:resnet_v1_50_official_batch_128_bf16',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:xla:random:inception_v3_batch_128_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train',\n",
       "       b'layout:xla:default:unet_3d.4x4.bf16',\n",
       "       b'layout:xla:random:tf2_bert_pretrain_dynamic_batch_size',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train',\n",
       "       b'layout:nlp:random:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:xla:random:mlperf_bert_batch_24_2x2',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:nlp:default:bert_en_cased_L-12_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:nlp:default:albert_en_xlarge_batch_size_16_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train',\n",
       "       b'layout:nlp:default:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:nlp:random:albert_en_xlarge_batch_size_16_test',\n",
       "       b'layout:xla:default:tf2_bert_pretrain_dynamic_batch_size',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test',\n",
       "       b'layout:nlp:random:bert_en_cased_L-12_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:xla:random:resnet50.4x4.fp16',\n",
       "       b'layout:xla:default:resnet_v1_50_official_batch_128_bf16',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:random:talking-heads_large_batch_size_16_train',\n",
       "       b'layout:xla:default:inception_v3_batch_128_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:default:talking-heads_large_batch_size_16_train',\n",
       "       b'layout:xla:default:bert_pretraining.4x4.fp16',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:xla:random:unet_3d.4x4.bf16'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72249379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test'\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_lin = np.linspace(15, 25, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "random_sample = val_df.sample(10_000)\n",
    "graph_id = np.random.choice(val_df['ID'].unique())\n",
    "random_sample = val_df[val_df['ID'] == graph_id]\n",
    "\n",
    "plt.scatter(\n",
    "    random_sample.target,\n",
    "    np.clip(random_sample.prediction, a_min=-10000.0, a_max=1000.0),\n",
    "    alpha=0.1,\n",
    "    #c=random_sample['ID'].apply(lambda x: x.decode('UTF-8').split(':')[1] == 'xla').values.astype(float)\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(graph_id)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0639b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = val_df.sample(5_000)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.abs(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('abs error')\n",
    "x_lin = np.linspace(0, 0.7, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.square(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('squared error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c359b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "layout:nlp:default:albert_en_xlarge_batch_size_16_test                               33536;50988;50988;47337;50943;7745;34857;42646...\n",
       "layout:nlp:default:bert_en_cased_L-12_H-768_A-12_batch_size_16_test                  29992;77971;28567;92042;47832;70419;58849;5034...\n",
       "layout:nlp:default:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train              11551;25515;20334;25470;9856;25483;25475;12594...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test      66138;89711;77907;14679;56842;1985;61718;35386...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train     18705;47041;47003;81;30639;47042;47016;47031;2...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test      95837;86823;7236;2086;11199;54321;49714;21665;...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train     4772;4782;22060;36642;299;4774;22073;4758;4758...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test      43343;27800;42932;49311;28256;67186;81334;8130...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train    1330;1341;1360;1360;1368;8396;1332;1364;4734;1...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test     92116;70831;72890;93920;8070;46133;21074;71296...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train    256;250;458;464;458;4040;19371;18145;4057;1030...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train      20354;98608;5138;65537;24115;24203;65974;69659...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train      18235;25647;39870;19072;12249;25348;25659;1822...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train      1774;1774;920;920;12398;44885;43081;64158;2847...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train      13830;13857;15098;10638;15116;15083;15003;5971...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train      35669;61709;61696;35650;36860;35654;35580;6177...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test       804;78036;78036;70134;78039;44919;8624;78656;3...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test      65705;34353;89642;99320;28673;65124;28687;1473...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train     35626;34785;24863;18919;18908;6210;38958;34769...\n",
       "layout:nlp:default:talking-heads_large_batch_size_16_train                           1122;5582;3904;3904;3904;3904;3904;1194;1194;1...\n",
       "layout:nlp:random:albert_en_xlarge_batch_size_16_test                                40835;49283;47109;14005;29944;18559;47950;1234...\n",
       "layout:nlp:random:bert_en_cased_L-12_H-768_A-12_batch_size_16_test                   54057;38210;85268;89117;286;19653;76284;62330;...\n",
       "layout:nlp:random:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train               1375;23022;17697;13372;18430;364;19760;6612;17...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test       89986;64851;49911;97551;65415;65415;95169;1465...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train      22113;28750;11709;25743;31903;24209;43882;9809...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test       90013;90013;32313;22274;79285;73027;55852;3684...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train      4810;8069;23461;35863;10945;12718;40813;22756;...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test       73332;59541;39396;13289;46125;61283;89761;3165...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train     29129;18516;548;12641;4679;16637;26394;26394;2...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test      63415;38181;38181;37561;35277;30184;52182;4845...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train     1179;2545;16003;5334;3351;3351;13486;13486;109...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train       6917;46662;1547;85034;95039;95039;60604;39169;...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train       75454;59161;59161;82906;72483;11505;84127;8533...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train       34026;29847;16682;58028;5817;60717;60717;41490...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train       34282;63079;23481;69519;53033;62265;62265;1335...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train       42339;53186;57134;58984;22302;41149;60009;1551...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test        16421;16417;60992;1301;88678;99653;17979;5587;...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test       70843;96606;68129;41422;67697;67697;26004;1766...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train      37991;37991;20759;13324;32437;32437;41786;3432...\n",
       "layout:nlp:random:talking-heads_large_batch_size_16_train                            5772;2511;10156;9040;10156;3057;1918;1092;5773...\n",
       "layout:xla:default:bert_pretraining.4x4.fp16                                         10004;18719;18705;18705;18715;18699;18716;8061...\n",
       "layout:xla:default:inception_v3_batch_128_train                                      1253;4243;5123;5116;450;436;4832;3325;4505;511...\n",
       "layout:xla:default:mlperf_bert_batch_24_2x2                                          265;270;5608;2004;5574;2010;1982;2018;263;252;...\n",
       "layout:xla:default:resnet50.4x4.fp16                                                 3555;3528;1498;3523;5498;5492;5505;4006;3864;3...\n",
       "layout:xla:default:resnet_v1_50_official_batch_128_bf16                              3635;966;1932;7635;1211;7633;1207;5428;970;101...\n",
       "layout:xla:default:tf2_bert_pretrain_dynamic_batch_size                              10141;11080;10618;3635;3657;10598;10598;10598;...\n",
       "layout:xla:default:unet_3d.4x4.bf16                                                  335;339;334;342;257;331;333;446;457;343;332;34...\n",
       "layout:xla:random:bert_pretraining.4x4.fp16                                          633;633;8640;11051;8942;5520;9137;9137;8346;12...\n",
       "layout:xla:random:inception_v3_batch_128_train                                       3428;1959;1232;3035;3329;2983;2857;701;2002;10...\n",
       "layout:xla:random:mlperf_bert_batch_24_2x2                                           5234;4980;5255;2960;3632;4251;1078;2708;413;45...\n",
       "layout:xla:random:resnet50.4x4.fp16                                                  1054;5406;3853;3594;255;4786;3099;2787;4873;33...\n",
       "layout:xla:random:resnet_v1_50_official_batch_128_bf16                               2014;2628;2693;5703;3367;7235;6080;1100;1096;9...\n",
       "layout:xla:random:tf2_bert_pretrain_dynamic_batch_size                               3478;14394;15096;3393;3393;15567;15567;17183;1...\n",
       "layout:xla:random:unet_3d.4x4.bf16                                                   144;215;261;344;22;203;103;372;34;443;95;2;149...\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_configs(df):\n",
    "    top = df.sort_values('prediction')\n",
    "    top = top['config_index'].values.tolist()\n",
    "    top = [str(i) for i in top]\n",
    "    return ';'.join(top)\n",
    "\n",
    "val_prediction = val_df.groupby('ID').apply(sort_configs)\n",
    "val_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb521f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3])).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layout_score_group(df):\n",
    "    score, _ = kendalltau(df['prediction'], df['target'])\n",
    "    return score\n",
    "\n",
    "val_df['subset'] = val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3]))\n",
    "for subset in val_df['subset'].unique():\n",
    "    mean = np.mean(val_df[val_df['subset'] == subset].groupby('ID').apply(compute_layout_score_group))\n",
    "    print(subset, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16107638",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.368, 0.137, 0.738, 0.346, 0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_score(candidate_order, layout_dict):\n",
    "    runtimes = layout_dict['config_runtime']\n",
    "    best_ranking = np.argsort(runtimes)\n",
    "    assert len(candidate_order) == len(runtimes)\n",
    "    score, _ = kendalltau(candidate_order, best_ranking)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4289a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_order = np.argsort(layout_dict['config_runtime'])\n",
    "plt.scatter(true_order, candidate_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d439d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layout_set = 'valid'\n",
    "true_orders = []\n",
    "layout_ids = []\n",
    "for dirpath, dirnames, filenames in os.walk('predict-ai-model-runtime/npz_all/npz/layout'):\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "    \n",
    "    if dirpath.split('/')[-1] != layout_set:\n",
    "        continue\n",
    "        \n",
    "    layout_id_prefix = ':'.join(dirpath.split('/')[-4:-1])\n",
    "    for filename in os.listdir(dirpath):\n",
    "        print(filename)\n",
    "        layout_id = layout_id_prefix+':'+filename[:-4]\n",
    "        layout_dict = dict(np.load(os.path.join(dirpath, filename)))\n",
    "        runtimes = layout_dict['config_runtime']\n",
    "        best_ranking = np.argsort(runtimes)\n",
    "        best_ranking = ';'.join([str(i) for i in best_ranking])\n",
    "        true_orders.append(best_ranking)\n",
    "        layout_ids.append(layout_id)\n",
    "        \n",
    "true_order_df = pd.DataFrame(\n",
    "    data=np.stack([layout_ids, true_orders], axis=-1),\n",
    "    columns=['ID', 'true_order']\n",
    ")\n",
    "true_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6177a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layout_id = true_order_df.sample()['ID'].values[0]\n",
    "layout_id = 'layout:xla:default:resnet50.4x4.fp16'\n",
    "true_order = [int(i) for i in true_order_df[true_order_df['ID'] == layout_id]['true_order'].values[0].split(';')]\n",
    "candidate_order = [int(i) for i in val_prediction[layout_id].split(';')]\n",
    "\n",
    "plt.scatter(true_order, candidate_order)\n",
    "plt.xlabel('true order')\n",
    "plt.ylabel('candidate order')\n",
    "plt.title(f'{layout_id}, len {len(true_order)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d506a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dict = dict(np.load('predict-ai-model-runtime/npz_all/npz/layout/nlp/default/valid/small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train.npz'))\n",
    "layout_dict['node_config_feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[val_df['ID'] == b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18339",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_layout['score'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434f868",
   "metadata": {},
   "source": [
    "## Inference over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac93738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>config_index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'layout:xla:random:db59a991b7c607634f13570d52...</td>\n",
       "      <td>18</td>\n",
       "      <td>-1966.709473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'layout:xla:random:db59a991b7c607634f13570d52...</td>\n",
       "      <td>13</td>\n",
       "      <td>-1966.423828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'layout:xla:random:db59a991b7c607634f13570d52...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1963.524170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'layout:xla:random:db59a991b7c607634f13570d52...</td>\n",
       "      <td>14</td>\n",
       "      <td>-1966.268677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'layout:xla:random:db59a991b7c607634f13570d52...</td>\n",
       "      <td>11</td>\n",
       "      <td>-1963.152588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>b'layout:xla:random:e8a3a1401b5e79f66d7037e424...</td>\n",
       "      <td>949</td>\n",
       "      <td>1908.983521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>b'layout:nlp:default:32531d07a084b319dce484f53...</td>\n",
       "      <td>992</td>\n",
       "      <td>23.739241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>b'layout:xla:random:e8a3a1401b5e79f66d7037e424...</td>\n",
       "      <td>988</td>\n",
       "      <td>1863.730957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>b'layout:xla:random:e8a3a1401b5e79f66d7037e424...</td>\n",
       "      <td>994</td>\n",
       "      <td>1900.896606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>b'layout:xla:random:e8a3a1401b5e79f66d7037e424...</td>\n",
       "      <td>987</td>\n",
       "      <td>1948.935669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50002 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ID  config_index  \\\n",
       "0      b'layout:xla:random:db59a991b7c607634f13570d52...            18   \n",
       "1      b'layout:xla:random:db59a991b7c607634f13570d52...            13   \n",
       "2      b'layout:xla:random:db59a991b7c607634f13570d52...             0   \n",
       "3      b'layout:xla:random:db59a991b7c607634f13570d52...            14   \n",
       "4      b'layout:xla:random:db59a991b7c607634f13570d52...            11   \n",
       "...                                                  ...           ...   \n",
       "49997  b'layout:xla:random:e8a3a1401b5e79f66d7037e424...           949   \n",
       "49998  b'layout:nlp:default:32531d07a084b319dce484f53...           992   \n",
       "49999  b'layout:xla:random:e8a3a1401b5e79f66d7037e424...           988   \n",
       "50000  b'layout:xla:random:e8a3a1401b5e79f66d7037e424...           994   \n",
       "50001  b'layout:xla:random:e8a3a1401b5e79f66d7037e424...           987   \n",
       "\n",
       "        prediction  \n",
       "0     -1966.709473  \n",
       "1     -1966.423828  \n",
       "2     -1963.524170  \n",
       "3     -1966.268677  \n",
       "4     -1963.152588  \n",
       "...            ...  \n",
       "49997  1908.983521  \n",
       "49998    23.739241  \n",
       "49999  1863.730957  \n",
       "50000  1900.896606  \n",
       "50001  1948.935669  \n",
       "\n",
       "[50002 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = mlp.predict_over_dataset(dataset.test_data, return_labels=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b21ba9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "layout:nlp:default:016ac66a44a906a695afd2228509046a    401;161;6;430;404;787;678;135;222;888;353;978;...\n",
       "layout:nlp:default:171b0513d8874a427ccfa46d136fbadc    985;977;81;971;611;913;482;305;21;267;70;296;7...\n",
       "layout:nlp:default:23559853d9702baaaacbb0c83fd32266    63;382;29;964;742;850;412;726;622;504;829;282;...\n",
       "layout:nlp:default:29886a50d55cfe77a9497bc906c76ce9    498;295;727;733;286;953;435;96;204;34;596;6;56...\n",
       "layout:nlp:default:32531d07a084b319dce484f53a4cf3fc    749;194;92;621;999;202;778;282;796;396;630;373...\n",
       "layout:nlp:default:38524e2ff135ded55b5286407e7af6b7    335;265;300;216;328;728;812;693;832;575;926;74...\n",
       "layout:nlp:default:3a0c5517a87df8d82fd637b83298a3ba    611;439;327;538;39;463;70;726;255;843;831;354;...\n",
       "layout:nlp:default:492c7a94d559aa4a88769142d2a68362    8;24;347;160;114;965;770;180;196;742;633;168;4...\n",
       "layout:nlp:default:58cc2e418c3a8a19b871e15964b534ad    657;981;980;945;561;162;889;927;111;717;28;278...\n",
       "layout:nlp:default:60880ed76de53f4d7a1b960b24f20f7d    976;80;176;155;902;384;913;817;169;577;58;482;...\n",
       "layout:nlp:default:6c1101f6231f4d1722c3b9f6d1e25026    972;772;817;659;429;518;11;589;227;9;477;593;9...\n",
       "layout:nlp:default:7105451001e119f65b66570d170b94a8    646;748;392;76;709;682;252;755;609;806;610;298...\n",
       "layout:nlp:default:71b79ca6db513e7979c3702c595150c2    894;922;897;928;291;577;522;955;192;45;74;989;...\n",
       "layout:nlp:default:7f6284ebe027b1e9a3850fc703858a59    232;405;492;937;592;691;294;207;930;170;938;39...\n",
       "layout:nlp:default:b2fdde3b72980907578648774101543e    35;317;376;532;710;889;912;503;921;75;653;51;9...\n",
       "layout:nlp:default:d15316c12eefdef1ba549eb433797f77    906;747;87;451;739;353;367;40;288;209;125;662;...\n",
       "layout:nlp:default:f6c146fc5cf10be4f3accbaca9897311    992;154;367;662;611;759;954;780;486;373;54;737...\n",
       "layout:nlp:random:016ac66a44a906a695afd2228509046a     26;644;47;540;273;231;383;848;378;924;813;56;4...\n",
       "layout:nlp:random:171b0513d8874a427ccfa46d136fbadc     75;565;260;140;434;629;311;861;902;940;768;2;6...\n",
       "layout:nlp:random:23559853d9702baaaacbb0c83fd32266     214;204;350;215;703;924;51;377;126;624;297;344...\n",
       "layout:nlp:random:29886a50d55cfe77a9497bc906c76ce9     4;689;84;223;488;531;617;235;392;205;100;170;5...\n",
       "layout:nlp:random:32531d07a084b319dce484f53a4cf3fc     691;853;22;26;466;325;907;464;543;536;546;915;...\n",
       "layout:nlp:random:38524e2ff135ded55b5286407e7af6b7     874;669;693;162;604;926;482;260;934;848;411;19...\n",
       "layout:nlp:random:3a0c5517a87df8d82fd637b83298a3ba     405;242;615;859;654;75;588;897;221;425;40;796;...\n",
       "layout:nlp:random:492c7a94d559aa4a88769142d2a68362     720;756;656;802;798;543;331;430;40;119;514;367...\n",
       "layout:nlp:random:58cc2e418c3a8a19b871e15964b534ad     458;123;167;238;18;506;893;712;519;120;508;829...\n",
       "layout:nlp:random:60880ed76de53f4d7a1b960b24f20f7d     645;270;742;320;356;219;836;623;55;84;703;457;...\n",
       "layout:nlp:random:6c1101f6231f4d1722c3b9f6d1e25026     138;362;444;589;222;611;166;649;6;267;142;380;...\n",
       "layout:nlp:random:7105451001e119f65b66570d170b94a8     46;392;930;2;617;636;955;540;467;340;247;713;9...\n",
       "layout:nlp:random:71b79ca6db513e7979c3702c595150c2     993;775;354;314;305;924;219;96;527;302;471;779...\n",
       "layout:nlp:random:7f6284ebe027b1e9a3850fc703858a59     12;901;179;102;499;196;422;81;880;114;378;359;...\n",
       "layout:nlp:random:b2fdde3b72980907578648774101543e     765;937;339;379;700;564;951;10;2;256;333;215;6...\n",
       "layout:nlp:random:d15316c12eefdef1ba549eb433797f77     901;825;425;511;717;58;140;912;131;979;925;92;...\n",
       "layout:nlp:random:f6c146fc5cf10be4f3accbaca9897311     749;158;352;846;634;691;617;726;487;827;387;46...\n",
       "layout:xla:default:05ae41e26dd3c4c06390371a0423233c    358;259;258;498;323;709;244;400;202;786;796;96...\n",
       "layout:xla:default:3e7156ac468dfb75cf5c9615e1e5887d    870;192;36;132;959;604;495;209;665;538;445;498...\n",
       "layout:xla:default:5335ed13823b0a518ee3c79ba4425f34    42;420;980;284;604;229;385;826;426;932;40;344;...\n",
       "layout:xla:default:937ee0eb0d5d6151b7b8252933b5c1c9    685;692;118;670;231;509;926;298;33;402;843;677...\n",
       "layout:xla:default:cd708819d3f5103afd6460b15e74eaf3    822;54;572;397;535;292;885;609;122;56;767;619;...\n",
       "layout:xla:default:db59a991b7c607634f13570d52ce885f    73;392;377;3;234;53;167;368;363;819;541;745;91...\n",
       "layout:xla:default:e8a3a1401b5e79f66d7037e424f3b6df    507;929;579;183;252;79;994;908;974;441;976;110...\n",
       "layout:xla:default:fbaa8bb6a1aed9988281085c91065c05    824;506;72;474;902;816;504;550;415;425;35;791;...\n",
       "layout:xla:random:05ae41e26dd3c4c06390371a0423233c     151;75;581;941;398;766;104;767;649;148;947;797...\n",
       "layout:xla:random:3e7156ac468dfb75cf5c9615e1e5887d     75;856;385;29;904;802;588;271;62;535;379;488;5...\n",
       "layout:xla:random:5335ed13823b0a518ee3c79ba4425f34     436;727;319;81;214;302;529;171;793;626;868;103...\n",
       "layout:xla:random:937ee0eb0d5d6151b7b8252933b5c1c9     190;324;604;383;803;410;451;13;581;618;536;170...\n",
       "layout:xla:random:cd708819d3f5103afd6460b15e74eaf3     747;147;335;382;157;222;110;552;764;132;516;23...\n",
       "layout:xla:random:db59a991b7c607634f13570d52ce885f     998;138;245;585;322;202;814;99;578;832;72;804;...\n",
       "layout:xla:random:e8a3a1401b5e79f66d7037e424f3b6df     991;413;52;340;354;617;501;845;556;109;655;493...\n",
       "layout:xla:random:fbaa8bb6a1aed9988281085c91065c05     64;190;990;565;724;567;224;974;341;377;633;860...\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = test_df.groupby('ID').apply(sort_configs)\n",
    "test_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d09fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_prediction, columns=['TopConfigs']).to_csv('layout_none_test_prediction_10_06_08_00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(mlp.dense_layer_1.kernel.numpy().flatten()), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f73f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
