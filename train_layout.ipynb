{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db851207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 23:42:17.943973: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-07 23:42:18.848700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dataset import LayoutDataset\n",
    "from models import LayoutMLP\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dda8a3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 23:42:21.265824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:21.346515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:21.346758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:21.348437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:21.348639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:21.348824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:22.089545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:22.089763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:22.089948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-07 23:42:22.090107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "dataset = LayoutDataset(\n",
    "    batch_size, train_sample_fraction=1.0, \n",
    "    subset=None, build_tfrecords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d32ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = LayoutMLP(batch_size, learning_rate=5e-4, mask_max_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a121fb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 23:42:27.880187: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1447936000 exceeds 10% of free system memory.\n",
      "2023-10-07 23:42:29.349582: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1447936000 exceeds 10% of free system memory.\n",
      "2023-10-07 23:42:34.376423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-07 23:42:34.383713: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f233224c530 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-07 23:42:34.383920: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2023-10-07 23:42:34.399792: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-07 23:42:34.619156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-07 23:42:34.746887: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 training loss 1.3482306 lr 0.00050\n",
      "iteration 200 training loss 1.3291728 lr 0.00050\n",
      "iteration 300 training loss 1.3040167 lr 0.00050\n",
      "iteration 400 training loss 1.3274386 lr 0.00050\n",
      "iteration 500 training loss 1.1975561 lr 0.00050\n",
      "iteration 600 training loss 1.1489128 lr 0.00050\n",
      "iteration 700 training loss 1.0346915 lr 0.00050\n",
      "iteration 800 training loss 1.1466416 lr 0.00050\n",
      "iteration 900 training loss 1.286201 lr 0.00050\n",
      "iteration 1000 training loss 1.1631628 lr 0.00050\n",
      "iteration 1100 training loss 1.1424394 lr 0.00050\n",
      "iteration 1200 training loss 1.228657 lr 0.00050\n",
      "iteration 1300 training loss 1.0136105 lr 0.00050\n",
      "iteration 1400 training loss 1.2040591 lr 0.00050\n",
      "iteration 1500 training loss 1.0876817 lr 0.00050\n",
      "iteration 1600 training loss 1.2141 lr 0.00050\n",
      "iteration 1700 training loss 0.9309182 lr 0.00050\n",
      "iteration 1800 training loss 0.999359 lr 0.00050\n",
      "iteration 1900 training loss 0.97178775 lr 0.00050\n",
      "iteration 2000 training loss 1.0314639 lr 0.00050\n",
      "iteration 2100 training loss 1.1256437 lr 0.00050\n",
      "iteration 2200 training loss 0.95506656 lr 0.00050\n",
      "iteration 2300 training loss 1.0960537 lr 0.00050\n",
      "iteration 2400 training loss 1.1322536 lr 0.00050\n",
      "iteration 2500 training loss 1.108947 lr 0.00050\n",
      "iteration 2600 training loss 0.9691904 lr 0.00050\n",
      "iteration 2700 training loss 0.88188136 lr 0.00050\n",
      "iteration 2800 training loss 0.97127837 lr 0.00050\n",
      "iteration 2900 training loss 1.1086346 lr 0.00050\n",
      "iteration 3000 training loss 0.92350566 lr 0.00050\n",
      "iteration 3100 training loss 1.1300421 lr 0.00050\n",
      "iteration 3200 training loss 1.0359758 lr 0.00050\n",
      "iteration 3300 training loss 0.9734512 lr 0.00050\n",
      "iteration 3400 training loss 1.0179276 lr 0.00050\n",
      "iteration 3500 training loss 1.0977696 lr 0.00050\n",
      "iteration 3600 training loss 0.9157433 lr 0.00050\n",
      "iteration 3700 training loss 0.8945622 lr 0.00050\n",
      "iteration 3800 training loss 0.9539601 lr 0.00050\n",
      "iteration 3900 training loss 0.9699064 lr 0.00050\n",
      "iteration 4000 training loss 0.97206676 lr 0.00050\n",
      "iteration 4100 training loss 0.9270687 lr 0.00050\n",
      "iteration 4200 training loss 0.8226256 lr 0.00050\n",
      "iteration 4300 training loss 0.7997595 lr 0.00050\n",
      "iteration 4400 training loss 0.9862085 lr 0.00050\n",
      "iteration 4500 training loss 0.9757854 lr 0.00050\n",
      "iteration 4600 training loss 0.88436127 lr 0.00050\n",
      "iteration 4700 training loss 0.928019 lr 0.00050\n",
      "iteration 4800 training loss 0.9917997 lr 0.00050\n",
      "iteration 4900 training loss 0.91159946 lr 0.00050\n",
      "iteration 5000 training loss 0.7666602 lr 0.00050\n",
      "iteration 5100 training loss 0.8676039 lr 0.00050\n",
      "iteration 5200 training loss 1.0474285 lr 0.00050\n",
      "iteration 5300 training loss 0.874055 lr 0.00050\n",
      "iteration 5400 training loss 1.0388488 lr 0.00050\n",
      "iteration 5500 training loss 0.8459979 lr 0.00050\n",
      "iteration 5600 training loss 0.9379427 lr 0.00050\n",
      "iteration 5700 training loss 0.8817038 lr 0.00050\n",
      "iteration 5800 training loss 0.83223534 lr 0.00050\n",
      "iteration 5900 training loss 0.8894452 lr 0.00050\n",
      "iteration 6000 training loss 0.9281268 lr 0.00050\n",
      "iteration 6100 training loss 0.87118167 lr 0.00050\n",
      "iteration 6200 training loss 0.9775965 lr 0.00050\n",
      "iteration 6300 training loss 0.99442524 lr 0.00050\n",
      "iteration 6400 training loss 1.1482718 lr 0.00050\n",
      "iteration 6500 training loss 1.0327452 lr 0.00050\n",
      "iteration 6600 training loss 1.0353291 lr 0.00050\n",
      "iteration 6700 training loss 1.0835289 lr 0.00050\n",
      "iteration 6800 training loss 0.91180056 lr 0.00050\n",
      "iteration 6900 training loss 0.8715743 lr 0.00050\n",
      "iteration 7000 training loss 0.93719816 lr 0.00050\n",
      "iteration 7100 training loss 1.0215626 lr 0.00050\n",
      "iteration 7200 training loss 1.0622519 lr 0.00050\n",
      "iteration 7300 training loss 0.85559374 lr 0.00050\n",
      "iteration 7400 training loss 0.7526775 lr 0.00050\n",
      "iteration 7500 training loss 0.81033427 lr 0.00050\n",
      "iteration 7600 training loss 1.1307222 lr 0.00050\n",
      "iteration 7700 training loss 1.0004479 lr 0.00050\n",
      "iteration 7800 training loss 1.0603719 lr 0.00050\n",
      "iteration 7900 training loss 0.8056435 lr 0.00050\n",
      "iteration 8000 training loss 1.1424047 lr 0.00050\n",
      "iteration 8100 training loss 1.0566117 lr 0.00050\n",
      "iteration 8200 training loss 0.87054926 lr 0.00050\n",
      "iteration 8300 training loss 0.8390393 lr 0.00050\n",
      "iteration 8400 training loss 1.1217225 lr 0.00050\n",
      "iteration 8500 training loss 0.8725804 lr 0.00050\n",
      "iteration 8600 training loss 1.0378327 lr 0.00050\n",
      "iteration 8700 training loss 0.8548136 lr 0.00050\n",
      "iteration 8800 training loss 1.0013795 lr 0.00050\n",
      "iteration 8900 training loss 0.9205991 lr 0.00050\n",
      "iteration 9000 training loss 0.9132606 lr 0.00050\n",
      "iteration 9100 training loss 0.99611425 lr 0.00050\n",
      "iteration 9200 training loss 0.8961494 lr 0.00050\n",
      "iteration 9300 training loss 0.9589573 lr 0.00050\n",
      "iteration 9400 training loss 0.7773356 lr 0.00050\n",
      "iteration 9500 training loss 0.9970461 lr 0.00050\n",
      "iteration 9600 training loss 0.98030984 lr 0.00050\n",
      "iteration 9700 training loss 0.925865 lr 0.00050\n",
      "iteration 9800 training loss 0.9563239 lr 0.00050\n",
      "iteration 9900 training loss 0.9303617 lr 0.00050\n",
      "iteration 10000 training loss 1.0668923 lr 0.00050\n",
      "iteration 10100 training loss 1.245193 lr 0.00050\n",
      "iteration 10200 training loss 1.030076 lr 0.00050\n",
      "iteration 10300 training loss 0.97301847 lr 0.00050\n",
      "iteration 10400 training loss 0.8271918 lr 0.00050\n",
      "iteration 10500 training loss 1.1520358 lr 0.00050\n",
      "iteration 10600 training loss 1.069766 lr 0.00050\n",
      "iteration 10700 training loss 0.9426476 lr 0.00050\n",
      "iteration 10800 training loss 1.0901514 lr 0.00050\n",
      "iteration 10900 training loss 1.0048968 lr 0.00050\n",
      "iteration 11000 training loss 1.056512 lr 0.00050\n",
      "iteration 11100 training loss 0.86225677 lr 0.00050\n",
      "iteration 11200 training loss 0.8414012 lr 0.00050\n",
      "iteration 11300 training loss 1.0625451 lr 0.00050\n",
      "iteration 11400 training loss 0.9063192 lr 0.00050\n",
      "iteration 11500 training loss 0.89986354 lr 0.00050\n",
      "iteration 11600 training loss 0.8162565 lr 0.00050\n",
      "iteration 11700 training loss 1.1053816 lr 0.00050\n",
      "iteration 11800 training loss 0.9852084 lr 0.00050\n",
      "iteration 11900 training loss 0.8022917 lr 0.00050\n",
      "iteration 12000 training loss 0.84316146 lr 0.00050\n",
      "iteration 12100 training loss 1.0159651 lr 0.00050\n",
      "iteration 12200 training loss 0.75586605 lr 0.00050\n",
      "iteration 12300 training loss 0.85449624 lr 0.00050\n",
      "iteration 12400 training loss 1.0265185 lr 0.00050\n",
      "iteration 12500 training loss 1.0023293 lr 0.00050\n",
      "iteration 12600 training loss 1.0043892 lr 0.00050\n",
      "iteration 12700 training loss 0.8819384 lr 0.00050\n",
      "iteration 12800 training loss 0.82289684 lr 0.00050\n",
      "iteration 12900 training loss 0.8594148 lr 0.00050\n",
      "iteration 13000 training loss 0.7832954 lr 0.00050\n",
      "iteration 13100 training loss 0.94780517 lr 0.00050\n",
      "iteration 13200 training loss 0.812074 lr 0.00050\n",
      "iteration 13300 training loss 0.82247037 lr 0.00050\n",
      "iteration 13400 training loss 0.90882844 lr 0.00050\n",
      "iteration 13500 training loss 0.905781 lr 0.00050\n",
      "iteration 13600 training loss 0.80162096 lr 0.00050\n",
      "iteration 13700 training loss 0.7198906 lr 0.00050\n",
      "iteration 13800 training loss 0.7753522 lr 0.00050\n",
      "iteration 13900 training loss 0.9227571 lr 0.00050\n",
      "iteration 14000 training loss 0.85242164 lr 0.00050\n",
      "iteration 14100 training loss 0.77310026 lr 0.00050\n",
      "iteration 14200 training loss 0.76087373 lr 0.00050\n",
      "iteration 14300 training loss 0.7342277 lr 0.00050\n",
      "iteration 14400 training loss 0.745492 lr 0.00050\n",
      "iteration 14500 training loss 0.7916266 lr 0.00050\n",
      "iteration 14600 training loss 0.7239213 lr 0.00050\n",
      "iteration 14700 training loss 0.9169178 lr 0.00050\n",
      "iteration 14800 training loss 1.0754277 lr 0.00050\n",
      "iteration 14900 training loss 0.8628082 lr 0.00050\n",
      "iteration 15000 training loss 0.89612293 lr 0.00050\n",
      "iteration 15100 training loss 0.8282825 lr 0.00050\n",
      "iteration 15200 training loss 0.7599004 lr 0.00050\n",
      "iteration 15300 training loss 0.8727037 lr 0.00050\n",
      "iteration 15400 training loss 0.85147744 lr 0.00050\n",
      "iteration 15500 training loss 0.9664803 lr 0.00050\n",
      "iteration 15600 training loss 0.9104269 lr 0.00050\n",
      "iteration 15700 training loss 0.8907352 lr 0.00050\n",
      "iteration 15800 training loss 0.87249535 lr 0.00050\n",
      "iteration 15900 training loss 1.0051596 lr 0.00050\n",
      "iteration 16000 training loss 1.1746645 lr 0.00050\n",
      "iteration 16100 training loss 1.0666819 lr 0.00050\n",
      "iteration 16200 training loss 1.0611506 lr 0.00050\n",
      "iteration 16300 training loss 0.9939582 lr 0.00050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16400 training loss 0.96695536 lr 0.00050\n",
      "iteration 16500 training loss 1.1433299 lr 0.00050\n",
      "iteration 16600 training loss 1.0104624 lr 0.00050\n",
      "iteration 16700 training loss 0.929353 lr 0.00050\n",
      "iteration 16800 training loss 0.7874842 lr 0.00050\n",
      "iteration 16900 training loss 0.80443764 lr 0.00050\n",
      "iteration 17000 training loss 0.9229065 lr 0.00050\n",
      "iteration 17100 training loss 0.9037707 lr 0.00050\n",
      "iteration 17200 training loss 0.934561 lr 0.00050\n",
      "iteration 17300 training loss 0.99747014 lr 0.00050\n",
      "iteration 17400 training loss 0.8768823 lr 0.00050\n",
      "iteration 17500 training loss 0.7806146 lr 0.00050\n",
      "iteration 17600 training loss 0.79353046 lr 0.00050\n",
      "iteration 17700 training loss 1.0839518 lr 0.00050\n",
      "iteration 17800 training loss 0.75374347 lr 0.00050\n",
      "iteration 17900 training loss 1.0253435 lr 0.00050\n",
      "iteration 18000 training loss 0.8886516 lr 0.00050\n",
      "iteration 18100 training loss 1.0223576 lr 0.00050\n",
      "iteration 18200 training loss 0.825696 lr 0.00050\n",
      "iteration 18300 training loss 1.157877 lr 0.00050\n",
      "iteration 18400 training loss 0.7523574 lr 0.00050\n",
      "iteration 18500 training loss 0.9240717 lr 0.00050\n",
      "iteration 18600 training loss 0.71847296 lr 0.00050\n",
      "iteration 18700 training loss 0.8368382 lr 0.00050\n",
      "iteration 18800 training loss 0.9548119 lr 0.00050\n",
      "iteration 18900 training loss 0.87333935 lr 0.00050\n",
      "iteration 19000 training loss 0.7931482 lr 0.00050\n",
      "iteration 19100 training loss 0.9225921 lr 0.00050\n",
      "iteration 19200 training loss 0.9808877 lr 0.00050\n",
      "iteration 19300 training loss 0.8745063 lr 0.00050\n",
      "iteration 19400 training loss 0.7659757 lr 0.00050\n",
      "iteration 19500 training loss 0.86510223 lr 0.00050\n",
      "iteration 19600 training loss 0.8906068 lr 0.00050\n",
      "iteration 19700 training loss 0.92671686 lr 0.00050\n",
      "iteration 19800 training loss 1.0676051 lr 0.00050\n",
      "iteration 19900 training loss 0.88447624 lr 0.00050\n",
      "iteration 20000 training loss 0.9804096 lr 0.00050\n",
      "layout:nlp:random 0.7278144985926581\n",
      "layout:nlp:default 0.36699525905158475\n",
      "layout:xla:random 0.47451845931378805\n",
      "layout:xla:default 0.1519735932149562\n",
      "epoch 0, it 20000 validation loss -0.430\n",
      "iteration 20100 training loss 0.8453458 lr 0.00045\n",
      "iteration 20200 training loss 0.8926448 lr 0.00045\n",
      "iteration 20300 training loss 0.8733071 lr 0.00045\n",
      "iteration 20400 training loss 0.8424858 lr 0.00045\n",
      "iteration 20500 training loss 0.7601963 lr 0.00045\n",
      "iteration 20600 training loss 0.8398056 lr 0.00045\n",
      "iteration 20700 training loss 0.9626288 lr 0.00045\n",
      "iteration 20800 training loss 1.069738 lr 0.00045\n",
      "iteration 20900 training loss 0.90464234 lr 0.00045\n",
      "iteration 21000 training loss 0.9454347 lr 0.00045\n",
      "iteration 21100 training loss 0.9823656 lr 0.00045\n",
      "iteration 21200 training loss 0.82197094 lr 0.00045\n",
      "iteration 21300 training loss 0.86743677 lr 0.00045\n",
      "iteration 21400 training loss 0.8983594 lr 0.00045\n",
      "iteration 21500 training loss 0.846407 lr 0.00045\n",
      "iteration 21600 training loss 0.9829376 lr 0.00045\n",
      "iteration 21700 training loss 1.1078811 lr 0.00045\n",
      "iteration 21800 training loss 0.81356835 lr 0.00045\n",
      "iteration 21900 training loss 0.8754772 lr 0.00045\n",
      "iteration 22000 training loss 0.83344233 lr 0.00045\n",
      "iteration 22100 training loss 0.89029306 lr 0.00045\n",
      "iteration 22200 training loss 0.98033154 lr 0.00045\n",
      "iteration 22300 training loss 0.85571975 lr 0.00045\n",
      "iteration 22400 training loss 1.0699157 lr 0.00045\n",
      "iteration 22500 training loss 0.9120869 lr 0.00045\n",
      "iteration 22600 training loss 0.94147986 lr 0.00045\n",
      "iteration 22700 training loss 0.7996092 lr 0.00045\n",
      "iteration 22800 training loss 0.9098073 lr 0.00045\n",
      "iteration 22900 training loss 0.8756178 lr 0.00045\n",
      "iteration 23000 training loss 0.9016719 lr 0.00045\n",
      "iteration 23100 training loss 0.7840821 lr 0.00045\n",
      "iteration 23200 training loss 0.91892755 lr 0.00045\n",
      "iteration 23300 training loss 0.9427417 lr 0.00045\n",
      "iteration 23400 training loss 0.8426484 lr 0.00045\n",
      "iteration 23500 training loss 0.97269505 lr 0.00045\n",
      "iteration 23600 training loss 0.7918798 lr 0.00045\n",
      "iteration 23700 training loss 0.8959814 lr 0.00045\n",
      "iteration 23800 training loss 0.85061336 lr 0.00045\n",
      "iteration 23900 training loss 0.78856736 lr 0.00045\n",
      "iteration 24000 training loss 0.6043961 lr 0.00045\n",
      "iteration 24100 training loss 0.8035239 lr 0.00045\n",
      "iteration 24200 training loss 0.8235603 lr 0.00045\n",
      "iteration 24300 training loss 1.0577024 lr 0.00045\n",
      "iteration 24400 training loss 0.95371693 lr 0.00045\n",
      "iteration 24500 training loss 0.88066816 lr 0.00045\n",
      "iteration 24600 training loss 0.8259466 lr 0.00045\n",
      "iteration 24700 training loss 0.98703456 lr 0.00045\n",
      "iteration 24800 training loss 0.85459167 lr 0.00045\n",
      "iteration 24900 training loss 0.8133044 lr 0.00045\n",
      "iteration 25000 training loss 0.69264424 lr 0.00045\n",
      "iteration 25100 training loss 1.0648942 lr 0.00045\n",
      "iteration 25200 training loss 0.88815755 lr 0.00045\n",
      "iteration 25300 training loss 0.818136 lr 0.00045\n",
      "iteration 25400 training loss 0.7325084 lr 0.00045\n",
      "iteration 25500 training loss 0.94555485 lr 0.00045\n",
      "iteration 25600 training loss 0.96481806 lr 0.00045\n",
      "iteration 25700 training loss 0.85886276 lr 0.00045\n",
      "iteration 25800 training loss 0.79849756 lr 0.00045\n",
      "iteration 25900 training loss 0.81244874 lr 0.00045\n",
      "iteration 26000 training loss 0.75556654 lr 0.00045\n",
      "iteration 26100 training loss 0.81889564 lr 0.00045\n",
      "iteration 26200 training loss 0.8165434 lr 0.00045\n",
      "iteration 26300 training loss 0.9247706 lr 0.00045\n",
      "iteration 26400 training loss 0.86784726 lr 0.00045\n",
      "iteration 26500 training loss 0.74214315 lr 0.00045\n",
      "iteration 26600 training loss 0.7935152 lr 0.00045\n",
      "iteration 26700 training loss 0.6593748 lr 0.00045\n",
      "iteration 26800 training loss 0.84805316 lr 0.00045\n",
      "iteration 26900 training loss 0.5903335 lr 0.00045\n",
      "iteration 27000 training loss 0.8867134 lr 0.00045\n",
      "iteration 27100 training loss 0.9416015 lr 0.00045\n",
      "iteration 27200 training loss 0.94556594 lr 0.00045\n",
      "iteration 27300 training loss 0.7601523 lr 0.00045\n",
      "iteration 27400 training loss 0.8542956 lr 0.00045\n",
      "iteration 27500 training loss 0.7298936 lr 0.00045\n",
      "iteration 27600 training loss 0.7285594 lr 0.00045\n",
      "iteration 27700 training loss 0.68338907 lr 0.00045\n",
      "iteration 27800 training loss 0.6562813 lr 0.00045\n",
      "iteration 27900 training loss 0.7652777 lr 0.00045\n",
      "iteration 28000 training loss 0.768385 lr 0.00045\n",
      "iteration 28100 training loss 0.87880385 lr 0.00045\n",
      "iteration 28200 training loss 0.86058986 lr 0.00045\n",
      "iteration 28300 training loss 0.6941073 lr 0.00045\n",
      "iteration 28400 training loss 0.68717366 lr 0.00045\n",
      "iteration 28500 training loss 0.6784894 lr 0.00045\n",
      "iteration 28600 training loss 0.71382236 lr 0.00045\n",
      "iteration 28700 training loss 0.876337 lr 0.00045\n",
      "iteration 28800 training loss 0.73383576 lr 0.00045\n",
      "iteration 28900 training loss 0.7221046 lr 0.00045\n",
      "iteration 29000 training loss 0.8829135 lr 0.00045\n",
      "iteration 29100 training loss 0.89786357 lr 0.00045\n",
      "iteration 29200 training loss 0.90442777 lr 0.00045\n",
      "iteration 29300 training loss 0.6023808 lr 0.00045\n",
      "iteration 29400 training loss 0.76751924 lr 0.00045\n",
      "iteration 29500 training loss 0.8633905 lr 0.00045\n",
      "iteration 29600 training loss 0.6437106 lr 0.00045\n",
      "iteration 29700 training loss 0.7372176 lr 0.00045\n",
      "iteration 29800 training loss 0.9432314 lr 0.00045\n",
      "iteration 29900 training loss 0.893888 lr 0.00045\n",
      "iteration 30000 training loss 0.69957185 lr 0.00045\n",
      "iteration 30100 training loss 0.76270133 lr 0.00045\n",
      "iteration 30200 training loss 0.7836674 lr 0.00045\n",
      "iteration 30300 training loss 0.7015722 lr 0.00045\n",
      "iteration 30400 training loss 0.8448419 lr 0.00045\n",
      "iteration 30500 training loss 0.707931 lr 0.00045\n",
      "iteration 30600 training loss 0.666744 lr 0.00045\n",
      "iteration 30700 training loss 1.0149043 lr 0.00045\n",
      "iteration 30800 training loss 0.9359526 lr 0.00045\n",
      "iteration 30900 training loss 0.8590903 lr 0.00045\n",
      "iteration 31000 training loss 0.8997282 lr 0.00045\n",
      "iteration 31100 training loss 0.7818442 lr 0.00045\n",
      "iteration 31200 training loss 0.7859028 lr 0.00045\n",
      "iteration 31300 training loss 1.0696999 lr 0.00045\n",
      "iteration 31400 training loss 0.72173744 lr 0.00045\n",
      "iteration 31500 training loss 0.765822 lr 0.00045\n",
      "iteration 31600 training loss 0.70870215 lr 0.00045\n",
      "iteration 31700 training loss 0.8587243 lr 0.00045\n",
      "iteration 31800 training loss 0.7727261 lr 0.00045\n",
      "iteration 31900 training loss 0.95308614 lr 0.00045\n",
      "iteration 32000 training loss 0.9746082 lr 0.00045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 32100 training loss 1.0308603 lr 0.00045\n",
      "iteration 32200 training loss 0.80950063 lr 0.00045\n",
      "iteration 32300 training loss 0.9416916 lr 0.00045\n",
      "iteration 32400 training loss 1.0476305 lr 0.00045\n",
      "iteration 32500 training loss 0.8483855 lr 0.00045\n",
      "iteration 32600 training loss 0.85070837 lr 0.00045\n",
      "iteration 32700 training loss 0.9059016 lr 0.00045\n",
      "iteration 32800 training loss 0.8930475 lr 0.00045\n",
      "iteration 32900 training loss 0.9226707 lr 0.00045\n",
      "iteration 33000 training loss 0.95581293 lr 0.00045\n",
      "iteration 33100 training loss 0.81659967 lr 0.00045\n",
      "iteration 33200 training loss 0.7975944 lr 0.00045\n",
      "iteration 33300 training loss 0.8379138 lr 0.00045\n",
      "iteration 33400 training loss 0.93964773 lr 0.00045\n",
      "iteration 33500 training loss 0.8524838 lr 0.00045\n",
      "iteration 33600 training loss 0.7694763 lr 0.00045\n",
      "iteration 33700 training loss 0.8920278 lr 0.00045\n",
      "iteration 33800 training loss 1.0733457 lr 0.00045\n",
      "iteration 33900 training loss 0.8472372 lr 0.00045\n",
      "iteration 34000 training loss 0.7828468 lr 0.00045\n",
      "iteration 34100 training loss 0.72397804 lr 0.00045\n",
      "iteration 34200 training loss 0.829152 lr 0.00045\n",
      "iteration 34300 training loss 0.7411039 lr 0.00045\n",
      "iteration 34400 training loss 1.0319152 lr 0.00045\n",
      "iteration 34500 training loss 0.7952138 lr 0.00045\n",
      "iteration 34600 training loss 0.7475128 lr 0.00045\n",
      "iteration 34700 training loss 1.0062213 lr 0.00045\n",
      "iteration 34800 training loss 0.82869774 lr 0.00045\n",
      "iteration 34900 training loss 0.8482192 lr 0.00045\n",
      "iteration 35000 training loss 0.79264164 lr 0.00045\n",
      "iteration 35100 training loss 0.9728494 lr 0.00045\n",
      "iteration 35200 training loss 0.78061026 lr 0.00045\n",
      "iteration 35300 training loss 1.0415074 lr 0.00045\n",
      "iteration 35400 training loss 0.75944924 lr 0.00045\n",
      "iteration 35500 training loss 0.79533166 lr 0.00045\n",
      "iteration 35600 training loss 0.8141487 lr 0.00045\n",
      "iteration 35700 training loss 0.82142556 lr 0.00045\n",
      "iteration 35800 training loss 0.85897285 lr 0.00045\n",
      "iteration 35900 training loss 0.6099486 lr 0.00045\n",
      "iteration 36000 training loss 0.9443903 lr 0.00045\n",
      "iteration 36100 training loss 1.0757889 lr 0.00045\n",
      "iteration 36200 training loss 0.9005018 lr 0.00045\n",
      "iteration 36300 training loss 1.0445734 lr 0.00045\n",
      "iteration 36400 training loss 0.98465055 lr 0.00045\n",
      "iteration 36500 training loss 0.838196 lr 0.00045\n",
      "iteration 36600 training loss 0.58978575 lr 0.00045\n",
      "iteration 36700 training loss 0.84129107 lr 0.00045\n",
      "iteration 36800 training loss 0.94187194 lr 0.00045\n",
      "iteration 36900 training loss 0.9382328 lr 0.00045\n",
      "iteration 37000 training loss 0.9338258 lr 0.00045\n",
      "iteration 37100 training loss 0.6243425 lr 0.00045\n",
      "iteration 37200 training loss 0.9069242 lr 0.00045\n",
      "iteration 37300 training loss 0.92204016 lr 0.00045\n",
      "iteration 37400 training loss 0.84173304 lr 0.00045\n",
      "iteration 37500 training loss 0.68556523 lr 0.00045\n",
      "iteration 37600 training loss 0.8950466 lr 0.00045\n",
      "iteration 37700 training loss 0.82479143 lr 0.00045\n",
      "iteration 37800 training loss 0.72309715 lr 0.00045\n",
      "iteration 37900 training loss 0.7947922 lr 0.00045\n",
      "iteration 38000 training loss 0.77882135 lr 0.00045\n",
      "iteration 38100 training loss 0.83324 lr 0.00045\n",
      "iteration 38200 training loss 0.86881804 lr 0.00045\n",
      "iteration 38300 training loss 0.8339225 lr 0.00045\n",
      "iteration 38400 training loss 0.8067197 lr 0.00045\n",
      "iteration 38500 training loss 1.0030179 lr 0.00045\n",
      "iteration 38600 training loss 0.76411057 lr 0.00045\n",
      "iteration 38700 training loss 0.8398223 lr 0.00045\n",
      "iteration 38800 training loss 0.75071454 lr 0.00045\n",
      "iteration 38900 training loss 0.8700058 lr 0.00045\n",
      "iteration 39000 training loss 0.9082338 lr 0.00045\n",
      "iteration 39100 training loss 0.80357516 lr 0.00045\n",
      "iteration 39200 training loss 0.8760893 lr 0.00045\n",
      "iteration 39300 training loss 0.9599461 lr 0.00045\n",
      "iteration 39400 training loss 0.8089991 lr 0.00045\n",
      "iteration 39500 training loss 0.9432286 lr 0.00045\n",
      "iteration 39600 training loss 0.89556396 lr 0.00045\n",
      "iteration 39700 training loss 0.8746049 lr 0.00045\n",
      "iteration 39800 training loss 0.7217187 lr 0.00045\n",
      "iteration 39900 training loss 0.80714977 lr 0.00045\n",
      "iteration 40000 training loss 0.89066297 lr 0.00045\n",
      "layout:nlp:random 0.7486819088210466\n",
      "layout:nlp:default 0.3818758443222424\n",
      "layout:xla:random 0.46233809333015546\n",
      "layout:xla:default 0.14629584462365783\n",
      "epoch 0, it 40000 validation loss -0.435\n",
      "iteration 40100 training loss 0.91818404 lr 0.00040\n",
      "iteration 40200 training loss 0.92209995 lr 0.00040\n",
      "iteration 40300 training loss 0.7807898 lr 0.00040\n",
      "iteration 40400 training loss 0.83715194 lr 0.00040\n",
      "iteration 40500 training loss 0.71903735 lr 0.00040\n",
      "iteration 40600 training loss 0.787031 lr 0.00040\n",
      "iteration 40700 training loss 0.8877297 lr 0.00040\n",
      "iteration 40800 training loss 1.0645164 lr 0.00040\n",
      "iteration 40900 training loss 0.71061116 lr 0.00040\n",
      "iteration 41000 training loss 0.80299413 lr 0.00040\n",
      "iteration 41100 training loss 0.7073376 lr 0.00040\n",
      "iteration 41200 training loss 0.86745125 lr 0.00040\n",
      "iteration 41300 training loss 0.7216137 lr 0.00040\n",
      "iteration 41400 training loss 0.796354 lr 0.00040\n",
      "iteration 41500 training loss 0.7161956 lr 0.00040\n",
      "iteration 41600 training loss 0.7817099 lr 0.00040\n",
      "iteration 41700 training loss 0.6210978 lr 0.00040\n",
      "iteration 41800 training loss 0.9178617 lr 0.00040\n",
      "iteration 41900 training loss 0.6581347 lr 0.00040\n",
      "iteration 42000 training loss 0.90538675 lr 0.00040\n",
      "iteration 42100 training loss 0.76425 lr 0.00040\n",
      "iteration 42200 training loss 0.689766 lr 0.00040\n",
      "iteration 42300 training loss 0.81163514 lr 0.00040\n",
      "iteration 42400 training loss 0.74386984 lr 0.00040\n",
      "iteration 42500 training loss 0.7602814 lr 0.00040\n",
      "iteration 42600 training loss 0.7390087 lr 0.00040\n",
      "iteration 42700 training loss 0.7783489 lr 0.00040\n",
      "iteration 42800 training loss 0.8595707 lr 0.00040\n",
      "iteration 42900 training loss 0.7294986 lr 0.00040\n",
      "iteration 43000 training loss 0.6743179 lr 0.00040\n",
      "iteration 43100 training loss 0.7278007 lr 0.00040\n",
      "iteration 43200 training loss 0.8992512 lr 0.00040\n",
      "iteration 43300 training loss 0.7920018 lr 0.00040\n",
      "iteration 43400 training loss 0.55740005 lr 0.00040\n",
      "iteration 43500 training loss 0.8358939 lr 0.00040\n",
      "iteration 43600 training loss 0.9039773 lr 0.00040\n",
      "iteration 43700 training loss 0.74249625 lr 0.00040\n",
      "iteration 43800 training loss 0.81496286 lr 0.00040\n",
      "iteration 43900 training loss 0.7168811 lr 0.00040\n",
      "iteration 44000 training loss 0.9846564 lr 0.00040\n",
      "iteration 44100 training loss 0.66406953 lr 0.00040\n",
      "iteration 44200 training loss 0.7467425 lr 0.00040\n",
      "iteration 44300 training loss 0.83761054 lr 0.00040\n",
      "iteration 44400 training loss 0.8466911 lr 0.00040\n",
      "iteration 44500 training loss 0.9105162 lr 0.00040\n",
      "iteration 44600 training loss 0.728487 lr 0.00040\n",
      "iteration 44700 training loss 0.8506014 lr 0.00040\n",
      "iteration 44800 training loss 1.0389029 lr 0.00040\n",
      "iteration 44900 training loss 0.7501175 lr 0.00040\n",
      "iteration 45000 training loss 0.9299998 lr 0.00040\n",
      "iteration 45100 training loss 0.7873503 lr 0.00040\n",
      "iteration 45200 training loss 0.80998105 lr 0.00040\n",
      "iteration 45300 training loss 0.938951 lr 0.00040\n",
      "iteration 45400 training loss 0.6701438 lr 0.00040\n",
      "iteration 45500 training loss 0.9248494 lr 0.00040\n",
      "iteration 45600 training loss 0.8136105 lr 0.00040\n",
      "iteration 45700 training loss 0.96956474 lr 0.00040\n",
      "iteration 45800 training loss 0.9780865 lr 0.00040\n",
      "iteration 45900 training loss 0.66370934 lr 0.00040\n",
      "iteration 46000 training loss 1.0023496 lr 0.00040\n",
      "iteration 46100 training loss 0.7986769 lr 0.00040\n",
      "iteration 46200 training loss 0.7837689 lr 0.00040\n",
      "iteration 46300 training loss 0.7751017 lr 0.00040\n",
      "iteration 46400 training loss 0.6697331 lr 0.00040\n",
      "iteration 46500 training loss 0.84899265 lr 0.00040\n",
      "iteration 46600 training loss 0.78187114 lr 0.00040\n",
      "iteration 46700 training loss 0.62548685 lr 0.00040\n",
      "iteration 46800 training loss 0.93915784 lr 0.00040\n",
      "iteration 46900 training loss 0.9757365 lr 0.00040\n",
      "iteration 47000 training loss 0.8995119 lr 0.00040\n",
      "iteration 47100 training loss 0.9457984 lr 0.00040\n",
      "iteration 47200 training loss 0.7781989 lr 0.00040\n",
      "iteration 47300 training loss 0.7729008 lr 0.00040\n",
      "iteration 47400 training loss 0.8337544 lr 0.00040\n",
      "iteration 47500 training loss 0.7792641 lr 0.00040\n",
      "iteration 47600 training loss 1.1104572 lr 0.00040\n",
      "iteration 47700 training loss 0.7911186 lr 0.00040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 47800 training loss 0.8188848 lr 0.00040\n",
      "iteration 47900 training loss 0.9231073 lr 0.00040\n",
      "iteration 48000 training loss 0.86118287 lr 0.00040\n",
      "iteration 48100 training loss 0.8155749 lr 0.00040\n",
      "iteration 48200 training loss 0.71771425 lr 0.00040\n",
      "iteration 48300 training loss 0.86801106 lr 0.00040\n",
      "iteration 48400 training loss 0.7927539 lr 0.00040\n",
      "iteration 48500 training loss 0.941806 lr 0.00040\n",
      "iteration 48600 training loss 0.85758334 lr 0.00040\n",
      "iteration 48700 training loss 0.8669979 lr 0.00040\n",
      "iteration 48800 training loss 0.87585914 lr 0.00040\n",
      "iteration 48900 training loss 0.92879045 lr 0.00040\n",
      "iteration 49000 training loss 0.9286512 lr 0.00040\n",
      "iteration 49100 training loss 0.9065155 lr 0.00040\n",
      "iteration 49200 training loss 0.707858 lr 0.00040\n",
      "iteration 49300 training loss 0.87216675 lr 0.00040\n",
      "iteration 49400 training loss 0.8802354 lr 0.00040\n",
      "iteration 49500 training loss 0.81945217 lr 0.00040\n",
      "iteration 49600 training loss 0.9741753 lr 0.00040\n",
      "iteration 49700 training loss 0.9476144 lr 0.00040\n",
      "iteration 49800 training loss 0.7177212 lr 0.00040\n",
      "iteration 49900 training loss 0.7610577 lr 0.00040\n",
      "iteration 50000 training loss 0.7037407 lr 0.00040\n",
      "iteration 50100 training loss 0.66247344 lr 0.00040\n",
      "iteration 50200 training loss 0.75928915 lr 0.00040\n",
      "iteration 50300 training loss 0.82824963 lr 0.00040\n",
      "iteration 50400 training loss 0.77804446 lr 0.00040\n",
      "iteration 50500 training loss 0.8493984 lr 0.00040\n",
      "iteration 50600 training loss 0.8932223 lr 0.00040\n",
      "iteration 50700 training loss 0.84843266 lr 0.00040\n",
      "iteration 50800 training loss 0.8070171 lr 0.00040\n",
      "iteration 50900 training loss 0.7113597 lr 0.00040\n",
      "iteration 51000 training loss 0.6913531 lr 0.00040\n",
      "iteration 51100 training loss 0.7931764 lr 0.00040\n",
      "iteration 51200 training loss 0.7335876 lr 0.00040\n",
      "iteration 51300 training loss 0.85804576 lr 0.00040\n",
      "iteration 51400 training loss 0.7902436 lr 0.00040\n",
      "iteration 51500 training loss 0.8611559 lr 0.00040\n",
      "iteration 51600 training loss 0.7828028 lr 0.00040\n",
      "iteration 51700 training loss 1.0033243 lr 0.00040\n",
      "iteration 51800 training loss 0.78167623 lr 0.00040\n",
      "iteration 51900 training loss 0.7076981 lr 0.00040\n",
      "iteration 52000 training loss 0.79543275 lr 0.00040\n",
      "iteration 52100 training loss 0.67424834 lr 0.00040\n",
      "iteration 52200 training loss 0.8138338 lr 0.00040\n",
      "iteration 52300 training loss 0.84937185 lr 0.00040\n",
      "iteration 52400 training loss 0.873545 lr 0.00040\n",
      "iteration 52500 training loss 0.6563829 lr 0.00040\n",
      "iteration 52600 training loss 0.88739693 lr 0.00040\n",
      "iteration 52700 training loss 0.80644125 lr 0.00040\n",
      "iteration 52800 training loss 0.78943896 lr 0.00040\n",
      "iteration 52900 training loss 0.7239251 lr 0.00040\n",
      "iteration 53000 training loss 0.711638 lr 0.00040\n",
      "iteration 53100 training loss 0.89109176 lr 0.00040\n",
      "iteration 53200 training loss 0.74001217 lr 0.00040\n",
      "iteration 53300 training loss 0.7882308 lr 0.00040\n",
      "iteration 53400 training loss 0.77678835 lr 0.00040\n",
      "iteration 53500 training loss 0.895895 lr 0.00040\n",
      "iteration 53600 training loss 0.8322877 lr 0.00040\n",
      "iteration 53700 training loss 0.74498546 lr 0.00040\n",
      "iteration 53800 training loss 0.6580164 lr 0.00040\n",
      "iteration 53900 training loss 0.8964415 lr 0.00040\n",
      "iteration 54000 training loss 0.6344049 lr 0.00040\n",
      "iteration 54100 training loss 1.1137583 lr 0.00040\n",
      "iteration 54200 training loss 0.5008359 lr 0.00040\n",
      "iteration 54300 training loss 0.86217755 lr 0.00040\n",
      "iteration 54400 training loss 0.950171 lr 0.00040\n",
      "iteration 54500 training loss 0.77625954 lr 0.00040\n",
      "iteration 54600 training loss 0.8110328 lr 0.00040\n",
      "iteration 54700 training loss 0.7189522 lr 0.00040\n",
      "iteration 54800 training loss 0.7024062 lr 0.00040\n",
      "iteration 54900 training loss 0.90402967 lr 0.00040\n",
      "iteration 55000 training loss 0.91497326 lr 0.00040\n",
      "iteration 55100 training loss 0.627828 lr 0.00040\n",
      "iteration 55200 training loss 0.77395236 lr 0.00040\n",
      "iteration 55300 training loss 0.947545 lr 0.00040\n",
      "iteration 55400 training loss 0.8606476 lr 0.00040\n",
      "iteration 55500 training loss 0.8703484 lr 0.00040\n",
      "iteration 55600 training loss 0.82174194 lr 0.00040\n",
      "iteration 55700 training loss 0.6521979 lr 0.00040\n",
      "iteration 55800 training loss 0.7711686 lr 0.00040\n",
      "iteration 55900 training loss 0.67594904 lr 0.00040\n",
      "iteration 56000 training loss 0.86792105 lr 0.00040\n",
      "iteration 56100 training loss 0.6100574 lr 0.00040\n",
      "iteration 56200 training loss 0.68264383 lr 0.00040\n",
      "iteration 56300 training loss 0.73232645 lr 0.00040\n",
      "iteration 56400 training loss 0.67031777 lr 0.00040\n",
      "iteration 56500 training loss 0.69467825 lr 0.00040\n",
      "iteration 56600 training loss 0.79722655 lr 0.00040\n",
      "iteration 56700 training loss 0.8228488 lr 0.00040\n",
      "iteration 56800 training loss 0.670873 lr 0.00040\n",
      "iteration 56900 training loss 0.8128536 lr 0.00040\n",
      "iteration 57000 training loss 0.771496 lr 0.00040\n",
      "iteration 57100 training loss 0.59020114 lr 0.00040\n",
      "iteration 57200 training loss 0.7610339 lr 0.00040\n",
      "iteration 57300 training loss 0.7210521 lr 0.00040\n",
      "iteration 57400 training loss 0.70844245 lr 0.00040\n",
      "iteration 57500 training loss 0.8327044 lr 0.00040\n",
      "iteration 57600 training loss 0.9095726 lr 0.00040\n",
      "iteration 57700 training loss 0.71795243 lr 0.00040\n",
      "iteration 57800 training loss 0.7610104 lr 0.00040\n",
      "iteration 57900 training loss 0.7014422 lr 0.00040\n",
      "iteration 58000 training loss 0.83398664 lr 0.00040\n",
      "iteration 58100 training loss 0.84676725 lr 0.00040\n",
      "iteration 58200 training loss 0.6628573 lr 0.00040\n",
      "iteration 58300 training loss 0.89393497 lr 0.00040\n",
      "iteration 58400 training loss 0.78716975 lr 0.00040\n",
      "iteration 58500 training loss 0.76763153 lr 0.00040\n",
      "iteration 58600 training loss 0.75442755 lr 0.00040\n",
      "iteration 58700 training loss 0.7383906 lr 0.00040\n",
      "iteration 58800 training loss 0.66863286 lr 0.00040\n",
      "iteration 58900 training loss 0.8795213 lr 0.00040\n",
      "iteration 59000 training loss 0.83192754 lr 0.00040\n",
      "iteration 59100 training loss 0.89595926 lr 0.00040\n",
      "iteration 59200 training loss 0.69393885 lr 0.00040\n",
      "iteration 59300 training loss 0.80308974 lr 0.00040\n",
      "iteration 59400 training loss 0.94190127 lr 0.00040\n",
      "iteration 59500 training loss 0.8829249 lr 0.00040\n",
      "iteration 59600 training loss 0.77100414 lr 0.00040\n",
      "iteration 59700 training loss 0.83905405 lr 0.00040\n",
      "iteration 59800 training loss 0.86600983 lr 0.00040\n",
      "iteration 59900 training loss 0.8491814 lr 0.00040\n",
      "iteration 60000 training loss 0.9980092 lr 0.00040\n",
      "layout:nlp:random 0.757428693721862\n",
      "layout:nlp:default 0.3928982284046403\n",
      "layout:xla:random 0.47051722738379226\n",
      "layout:xla:default 0.1876227685656288\n",
      "epoch 0, it 60000 validation loss -0.452\n",
      "iteration 60100 training loss 0.86172885 lr 0.00036\n",
      "iteration 60200 training loss 0.9850058 lr 0.00036\n",
      "iteration 60300 training loss 0.7980124 lr 0.00036\n",
      "iteration 60400 training loss 0.68823934 lr 0.00036\n",
      "iteration 60500 training loss 0.9140424 lr 0.00036\n",
      "iteration 60600 training loss 0.59867674 lr 0.00036\n",
      "iteration 60700 training loss 0.9150079 lr 0.00036\n",
      "iteration 60800 training loss 0.87049884 lr 0.00036\n",
      "iteration 60900 training loss 0.91968864 lr 0.00036\n",
      "iteration 61000 training loss 0.771503 lr 0.00036\n",
      "iteration 61100 training loss 0.9790214 lr 0.00036\n",
      "iteration 61200 training loss 0.86053795 lr 0.00036\n",
      "iteration 61300 training loss 0.7372219 lr 0.00036\n",
      "iteration 61400 training loss 0.75142646 lr 0.00036\n",
      "iteration 61500 training loss 1.013192 lr 0.00036\n",
      "iteration 61600 training loss 0.91929877 lr 0.00036\n",
      "iteration 61700 training loss 0.81237775 lr 0.00036\n",
      "iteration 61800 training loss 0.73751205 lr 0.00036\n",
      "iteration 61900 training loss 0.74862486 lr 0.00036\n",
      "iteration 62000 training loss 0.9482281 lr 0.00036\n",
      "iteration 62100 training loss 0.79525894 lr 0.00036\n",
      "iteration 62200 training loss 0.791318 lr 0.00036\n",
      "iteration 62300 training loss 0.80343866 lr 0.00036\n",
      "iteration 62400 training loss 0.9701825 lr 0.00036\n",
      "iteration 62500 training loss 0.8069037 lr 0.00036\n",
      "iteration 62600 training loss 0.82719845 lr 0.00036\n",
      "iteration 62700 training loss 0.8442482 lr 0.00036\n",
      "iteration 62800 training loss 0.84690464 lr 0.00036\n",
      "iteration 62900 training loss 0.77537733 lr 0.00036\n",
      "iteration 63000 training loss 0.79740536 lr 0.00036\n",
      "iteration 63100 training loss 0.7913025 lr 0.00036\n",
      "iteration 63200 training loss 0.8019581 lr 0.00036\n",
      "iteration 63300 training loss 0.9347894 lr 0.00036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 63400 training loss 0.6827587 lr 0.00036\n",
      "iteration 63500 training loss 0.7919746 lr 0.00036\n",
      "iteration 63600 training loss 0.7677223 lr 0.00036\n",
      "iteration 63700 training loss 0.82590336 lr 0.00036\n",
      "iteration 63800 training loss 0.62756836 lr 0.00036\n",
      "iteration 63900 training loss 0.6537417 lr 0.00036\n",
      "iteration 64000 training loss 0.9245458 lr 0.00036\n",
      "iteration 64100 training loss 0.8611059 lr 0.00036\n",
      "iteration 64200 training loss 0.9193798 lr 0.00036\n",
      "iteration 64300 training loss 0.85018593 lr 0.00036\n",
      "iteration 64400 training loss 0.91275704 lr 0.00036\n",
      "iteration 64500 training loss 0.73935306 lr 0.00036\n",
      "iteration 64600 training loss 1.0529335 lr 0.00036\n",
      "iteration 64700 training loss 0.81776696 lr 0.00036\n",
      "iteration 64800 training loss 1.0991868 lr 0.00036\n",
      "iteration 64900 training loss 0.85230947 lr 0.00036\n",
      "iteration 65000 training loss 0.91884375 lr 0.00036\n",
      "iteration 65100 training loss 0.72627634 lr 0.00036\n",
      "iteration 65200 training loss 1.0928249 lr 0.00036\n",
      "iteration 65300 training loss 0.82072484 lr 0.00036\n",
      "iteration 65400 training loss 0.8896065 lr 0.00036\n",
      "iteration 65500 training loss 0.77194494 lr 0.00036\n",
      "iteration 65600 training loss 0.6869276 lr 0.00036\n",
      "iteration 65700 training loss 0.69722503 lr 0.00036\n",
      "iteration 65800 training loss 0.92354935 lr 0.00036\n",
      "iteration 65900 training loss 0.8109091 lr 0.00036\n",
      "iteration 66000 training loss 0.64798766 lr 0.00036\n",
      "iteration 66100 training loss 0.7288644 lr 0.00036\n",
      "iteration 66200 training loss 0.9659162 lr 0.00036\n",
      "iteration 66300 training loss 0.72236305 lr 0.00036\n",
      "iteration 66400 training loss 0.875553 lr 0.00036\n",
      "iteration 66500 training loss 0.6872739 lr 0.00036\n",
      "iteration 66600 training loss 0.75194824 lr 0.00036\n",
      "iteration 66700 training loss 0.7155407 lr 0.00036\n",
      "iteration 66800 training loss 0.70701647 lr 0.00036\n",
      "iteration 66900 training loss 0.8102432 lr 0.00036\n",
      "iteration 67000 training loss 0.81735843 lr 0.00036\n",
      "iteration 67100 training loss 0.805909 lr 0.00036\n",
      "iteration 67200 training loss 0.70299405 lr 0.00036\n",
      "iteration 67300 training loss 0.6749777 lr 0.00036\n",
      "iteration 67400 training loss 0.82885796 lr 0.00036\n",
      "iteration 67500 training loss 0.8217708 lr 0.00036\n",
      "iteration 67600 training loss 0.5790634 lr 0.00036\n",
      "iteration 67700 training loss 0.6232432 lr 0.00036\n",
      "iteration 67800 training loss 0.823011 lr 0.00036\n",
      "iteration 67900 training loss 0.8213538 lr 0.00036\n",
      "iteration 68000 training loss 0.72949576 lr 0.00036\n",
      "iteration 68100 training loss 0.7838432 lr 0.00036\n",
      "iteration 68200 training loss 0.9049724 lr 0.00036\n",
      "iteration 68300 training loss 0.9077965 lr 0.00036\n",
      "iteration 68400 training loss 0.81616443 lr 0.00036\n",
      "iteration 68500 training loss 0.7374834 lr 0.00036\n",
      "iteration 68600 training loss 0.62292653 lr 0.00036\n",
      "iteration 68700 training loss 0.73266226 lr 0.00036\n",
      "iteration 68800 training loss 0.653269 lr 0.00036\n",
      "iteration 68900 training loss 0.9766309 lr 0.00036\n",
      "iteration 69000 training loss 0.7441917 lr 0.00036\n",
      "iteration 69100 training loss 0.7636147 lr 0.00036\n",
      "iteration 69200 training loss 0.66692996 lr 0.00036\n",
      "iteration 69300 training loss 0.8109662 lr 0.00036\n",
      "iteration 69400 training loss 0.84755516 lr 0.00036\n",
      "iteration 69500 training loss 0.5978802 lr 0.00036\n",
      "iteration 69600 training loss 0.88197887 lr 0.00036\n",
      "iteration 69700 training loss 0.73999625 lr 0.00036\n",
      "iteration 69800 training loss 0.7860421 lr 0.00036\n",
      "iteration 69900 training loss 0.75676733 lr 0.00036\n",
      "iteration 70000 training loss 0.7751751 lr 0.00036\n",
      "iteration 70100 training loss 0.72479117 lr 0.00036\n",
      "iteration 70200 training loss 0.724476 lr 0.00036\n",
      "iteration 70300 training loss 0.618274 lr 0.00036\n",
      "iteration 70400 training loss 0.74262726 lr 0.00036\n",
      "iteration 70500 training loss 0.8112952 lr 0.00036\n",
      "iteration 70600 training loss 0.66443837 lr 0.00036\n",
      "iteration 70700 training loss 0.6126187 lr 0.00036\n",
      "iteration 70800 training loss 0.865033 lr 0.00036\n",
      "iteration 70900 training loss 0.73144054 lr 0.00036\n",
      "iteration 71000 training loss 0.77559143 lr 0.00036\n",
      "iteration 71100 training loss 0.59425056 lr 0.00036\n",
      "iteration 71200 training loss 0.60089964 lr 0.00036\n",
      "iteration 71300 training loss 0.7472327 lr 0.00036\n",
      "iteration 71400 training loss 0.6651727 lr 0.00036\n",
      "iteration 71500 training loss 0.7455948 lr 0.00036\n",
      "iteration 71600 training loss 0.70872384 lr 0.00036\n",
      "iteration 71700 training loss 0.8015691 lr 0.00036\n",
      "iteration 71800 training loss 0.77180177 lr 0.00036\n",
      "iteration 71900 training loss 0.7818448 lr 0.00036\n",
      "iteration 72000 training loss 0.7165343 lr 0.00036\n",
      "iteration 72100 training loss 0.8697321 lr 0.00036\n",
      "iteration 72200 training loss 0.77393353 lr 0.00036\n",
      "iteration 72300 training loss 0.7512519 lr 0.00036\n",
      "iteration 72400 training loss 0.55589646 lr 0.00036\n",
      "iteration 72500 training loss 0.86259526 lr 0.00036\n",
      "iteration 72600 training loss 0.74147415 lr 0.00036\n",
      "iteration 72700 training loss 0.7805618 lr 0.00036\n",
      "iteration 72800 training loss 0.8772889 lr 0.00036\n",
      "iteration 72900 training loss 0.7033693 lr 0.00036\n",
      "iteration 73000 training loss 0.740617 lr 0.00036\n",
      "iteration 73100 training loss 0.65318596 lr 0.00036\n",
      "iteration 73200 training loss 0.6329319 lr 0.00036\n",
      "iteration 73300 training loss 0.69420093 lr 0.00036\n",
      "iteration 73400 training loss 0.66704243 lr 0.00036\n",
      "iteration 73500 training loss 0.7763051 lr 0.00036\n",
      "iteration 73600 training loss 0.79585165 lr 0.00036\n",
      "iteration 73700 training loss 0.6924552 lr 0.00036\n",
      "iteration 73800 training loss 0.6828209 lr 0.00036\n",
      "iteration 73900 training loss 0.6966991 lr 0.00036\n",
      "iteration 74000 training loss 0.72339636 lr 0.00036\n",
      "iteration 74100 training loss 0.7114363 lr 0.00036\n",
      "iteration 74200 training loss 0.7514827 lr 0.00036\n",
      "iteration 74300 training loss 1.021467 lr 0.00036\n",
      "iteration 74400 training loss 0.8022986 lr 0.00036\n",
      "iteration 74500 training loss 0.8929475 lr 0.00036\n",
      "iteration 74600 training loss 1.0423625 lr 0.00036\n",
      "iteration 74700 training loss 0.92305315 lr 0.00036\n",
      "iteration 74800 training loss 0.9499278 lr 0.00036\n",
      "iteration 74900 training loss 0.8738006 lr 0.00036\n",
      "iteration 75000 training loss 0.6769282 lr 0.00036\n",
      "iteration 75100 training loss 0.71843064 lr 0.00036\n",
      "iteration 75200 training loss 0.85679424 lr 0.00036\n",
      "iteration 75300 training loss 0.87698585 lr 0.00036\n",
      "iteration 75400 training loss 0.9182818 lr 0.00036\n",
      "iteration 75500 training loss 0.9246308 lr 0.00036\n",
      "iteration 75600 training loss 0.84028125 lr 0.00036\n",
      "iteration 75700 training loss 0.99078006 lr 0.00036\n",
      "iteration 75800 training loss 0.8182917 lr 0.00036\n",
      "iteration 75900 training loss 0.720985 lr 0.00036\n",
      "iteration 76000 training loss 0.93129236 lr 0.00036\n",
      "iteration 76100 training loss 0.90851957 lr 0.00036\n",
      "iteration 76200 training loss 0.9684423 lr 0.00036\n",
      "iteration 76300 training loss 0.81265396 lr 0.00036\n",
      "iteration 76400 training loss 0.6387718 lr 0.00036\n",
      "iteration 76500 training loss 0.86635095 lr 0.00036\n",
      "iteration 76600 training loss 0.78273755 lr 0.00036\n",
      "iteration 76700 training loss 0.9213085 lr 0.00036\n",
      "iteration 76800 training loss 0.9462917 lr 0.00036\n",
      "iteration 76900 training loss 0.68592155 lr 0.00036\n",
      "iteration 77000 training loss 0.7583119 lr 0.00036\n",
      "iteration 77100 training loss 0.8585357 lr 0.00036\n",
      "iteration 77200 training loss 0.87608296 lr 0.00036\n",
      "iteration 77300 training loss 0.7696577 lr 0.00036\n",
      "iteration 77400 training loss 0.7771357 lr 0.00036\n",
      "iteration 77500 training loss 0.8363138 lr 0.00036\n",
      "iteration 77600 training loss 0.8766277 lr 0.00036\n",
      "iteration 77700 training loss 1.0276833 lr 0.00036\n",
      "iteration 77800 training loss 0.8493497 lr 0.00036\n",
      "iteration 77900 training loss 0.81336373 lr 0.00036\n",
      "iteration 78000 training loss 0.8532672 lr 0.00036\n",
      "iteration 78100 training loss 0.7377009 lr 0.00036\n",
      "iteration 78200 training loss 0.96754116 lr 0.00036\n",
      "iteration 78300 training loss 0.6985418 lr 0.00036\n",
      "iteration 78400 training loss 0.8421343 lr 0.00036\n",
      "iteration 78500 training loss 0.72312 lr 0.00036\n",
      "iteration 78600 training loss 0.8645586 lr 0.00036\n",
      "iteration 78700 training loss 0.742822 lr 0.00036\n",
      "iteration 78800 training loss 0.71053857 lr 0.00036\n",
      "iteration 78900 training loss 0.69893414 lr 0.00036\n",
      "iteration 79000 training loss 0.88723725 lr 0.00036\n",
      "iteration 79100 training loss 0.7745287 lr 0.00036\n",
      "iteration 79200 training loss 0.7287461 lr 0.00036\n",
      "iteration 79300 training loss 0.7316452 lr 0.00036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 79400 training loss 0.8728372 lr 0.00036\n",
      "iteration 79500 training loss 0.8523296 lr 0.00036\n",
      "iteration 79600 training loss 0.6844663 lr 0.00036\n",
      "iteration 79700 training loss 0.82244265 lr 0.00036\n",
      "iteration 79800 training loss 0.8755283 lr 0.00036\n",
      "iteration 79900 training loss 0.8389093 lr 0.00036\n",
      "iteration 80000 training loss 0.99056464 lr 0.00036\n",
      "layout:nlp:random 0.7566149938362464\n",
      "layout:nlp:default 0.3909815353830071\n",
      "layout:xla:random 0.463475402628455\n",
      "layout:xla:default 0.16501829873555904\n",
      "epoch 0, it 80000 validation loss -0.444\n",
      "iteration 80100 training loss 1.040007 lr 0.00033\n",
      "iteration 80200 training loss 0.6510828 lr 0.00033\n",
      "iteration 80300 training loss 0.8395504 lr 0.00033\n",
      "iteration 80400 training loss 0.6982411 lr 0.00033\n",
      "iteration 80500 training loss 0.78682125 lr 0.00033\n",
      "iteration 80600 training loss 0.8924037 lr 0.00033\n",
      "iteration 80700 training loss 0.76094586 lr 0.00033\n",
      "iteration 80800 training loss 0.79122895 lr 0.00033\n",
      "iteration 80900 training loss 0.7200631 lr 0.00033\n",
      "iteration 81000 training loss 0.88528156 lr 0.00033\n",
      "iteration 81100 training loss 0.7355571 lr 0.00033\n",
      "iteration 81200 training loss 0.7941982 lr 0.00033\n",
      "iteration 81300 training loss 0.80482846 lr 0.00033\n",
      "iteration 81400 training loss 0.9647791 lr 0.00033\n",
      "iteration 81500 training loss 0.7168487 lr 0.00033\n",
      "iteration 81600 training loss 0.8853616 lr 0.00033\n",
      "iteration 81700 training loss 0.89315563 lr 0.00033\n",
      "iteration 81800 training loss 0.63505155 lr 0.00033\n",
      "iteration 81900 training loss 0.86014736 lr 0.00033\n",
      "iteration 82000 training loss 0.85833424 lr 0.00033\n",
      "iteration 82100 training loss 0.7154657 lr 0.00033\n",
      "iteration 82200 training loss 0.79747516 lr 0.00033\n",
      "iteration 82300 training loss 0.6728956 lr 0.00033\n",
      "iteration 82400 training loss 0.9017833 lr 0.00033\n",
      "iteration 82500 training loss 0.9255641 lr 0.00033\n",
      "iteration 82600 training loss 0.60661703 lr 0.00033\n",
      "iteration 82700 training loss 0.85700715 lr 0.00033\n",
      "iteration 82800 training loss 0.72024184 lr 0.00033\n",
      "iteration 82900 training loss 0.7372912 lr 0.00033\n",
      "iteration 83000 training loss 0.85260427 lr 0.00033\n",
      "iteration 83100 training loss 0.88655674 lr 0.00033\n",
      "iteration 83200 training loss 0.89831275 lr 0.00033\n",
      "iteration 83300 training loss 0.6739017 lr 0.00033\n",
      "iteration 83400 training loss 0.7335722 lr 0.00033\n",
      "iteration 83500 training loss 0.83753645 lr 0.00033\n",
      "iteration 83600 training loss 0.5810032 lr 0.00033\n",
      "iteration 83700 training loss 0.5276677 lr 0.00033\n",
      "iteration 83800 training loss 0.79404455 lr 0.00033\n",
      "iteration 83900 training loss 0.51849216 lr 0.00033\n",
      "iteration 84000 training loss 0.71211505 lr 0.00033\n",
      "iteration 84100 training loss 0.75673604 lr 0.00033\n",
      "iteration 84200 training loss 0.75589865 lr 0.00033\n",
      "iteration 84300 training loss 0.85724413 lr 0.00033\n",
      "iteration 84400 training loss 0.7574894 lr 0.00033\n",
      "iteration 84500 training loss 0.75147444 lr 0.00033\n",
      "iteration 84600 training loss 0.83983976 lr 0.00033\n",
      "iteration 84700 training loss 0.8238588 lr 0.00033\n",
      "iteration 84800 training loss 0.69609195 lr 0.00033\n",
      "iteration 84900 training loss 0.8053922 lr 0.00033\n",
      "iteration 85000 training loss 0.9841272 lr 0.00033\n",
      "iteration 85100 training loss 0.8536661 lr 0.00033\n",
      "iteration 85200 training loss 0.64367586 lr 0.00033\n",
      "iteration 85300 training loss 0.79920405 lr 0.00033\n",
      "iteration 85400 training loss 0.64439696 lr 0.00033\n",
      "iteration 85500 training loss 0.6830345 lr 0.00033\n",
      "iteration 85600 training loss 0.6209835 lr 0.00033\n",
      "iteration 85700 training loss 0.774284 lr 0.00033\n",
      "iteration 85800 training loss 0.8217871 lr 0.00033\n",
      "iteration 85900 training loss 0.64810693 lr 0.00033\n",
      "iteration 86000 training loss 0.6059779 lr 0.00033\n",
      "iteration 86100 training loss 0.6342073 lr 0.00033\n",
      "iteration 86200 training loss 0.6578939 lr 0.00033\n",
      "iteration 86300 training loss 0.78741705 lr 0.00033\n",
      "iteration 86400 training loss 0.779872 lr 0.00033\n",
      "iteration 86500 training loss 0.46349463 lr 0.00033\n",
      "iteration 86600 training loss 0.864386 lr 0.00033\n",
      "iteration 86700 training loss 0.74661183 lr 0.00033\n",
      "iteration 86800 training loss 0.7012365 lr 0.00033\n",
      "iteration 86900 training loss 0.8508238 lr 0.00033\n",
      "iteration 87000 training loss 0.8218018 lr 0.00033\n",
      "iteration 87100 training loss 0.76882905 lr 0.00033\n",
      "iteration 87200 training loss 0.7257258 lr 0.00033\n",
      "iteration 87300 training loss 0.90668464 lr 0.00033\n",
      "iteration 87400 training loss 0.64440286 lr 0.00033\n",
      "iteration 87500 training loss 0.7166445 lr 0.00033\n",
      "iteration 87600 training loss 0.88843554 lr 0.00033\n",
      "iteration 87700 training loss 0.6667304 lr 0.00033\n",
      "iteration 87800 training loss 0.8826439 lr 0.00033\n",
      "iteration 87900 training loss 0.7348778 lr 0.00033\n",
      "iteration 88000 training loss 0.6948411 lr 0.00033\n",
      "iteration 88100 training loss 0.77635366 lr 0.00033\n",
      "iteration 88200 training loss 1.0469432 lr 0.00033\n",
      "iteration 88300 training loss 0.67044985 lr 0.00033\n",
      "iteration 88400 training loss 0.6504383 lr 0.00033\n",
      "iteration 88500 training loss 0.8904863 lr 0.00033\n",
      "iteration 88600 training loss 0.8324229 lr 0.00033\n",
      "iteration 88700 training loss 1.0729293 lr 0.00033\n",
      "iteration 88800 training loss 0.9614185 lr 0.00033\n",
      "iteration 88900 training loss 0.7724849 lr 0.00033\n",
      "iteration 89000 training loss 0.7965435 lr 0.00033\n",
      "iteration 89100 training loss 0.7661058 lr 0.00033\n",
      "iteration 89200 training loss 0.7982296 lr 0.00033\n",
      "iteration 89300 training loss 0.8407659 lr 0.00033\n",
      "iteration 89400 training loss 0.7725768 lr 0.00033\n",
      "iteration 89500 training loss 0.76508284 lr 0.00033\n",
      "iteration 89600 training loss 0.7164463 lr 0.00033\n",
      "iteration 89700 training loss 0.7934565 lr 0.00033\n",
      "iteration 89800 training loss 0.9818041 lr 0.00033\n",
      "iteration 89900 training loss 0.8741801 lr 0.00033\n",
      "iteration 90000 training loss 0.8169344 lr 0.00033\n",
      "iteration 90100 training loss 0.97944266 lr 0.00033\n",
      "iteration 90200 training loss 0.8122781 lr 0.00033\n",
      "iteration 90300 training loss 0.79404986 lr 0.00033\n",
      "iteration 90400 training loss 0.86332184 lr 0.00033\n",
      "iteration 90500 training loss 0.72424114 lr 0.00033\n",
      "iteration 90600 training loss 0.90596366 lr 0.00033\n",
      "iteration 90700 training loss 0.7498838 lr 0.00033\n",
      "iteration 90800 training loss 0.9075445 lr 0.00033\n",
      "iteration 90900 training loss 0.79345113 lr 0.00033\n",
      "iteration 91000 training loss 0.9287173 lr 0.00033\n",
      "iteration 91100 training loss 0.82184535 lr 0.00033\n",
      "iteration 91200 training loss 1.0224682 lr 0.00033\n",
      "iteration 91300 training loss 0.6893339 lr 0.00033\n",
      "iteration 91400 training loss 0.77867514 lr 0.00033\n",
      "iteration 91500 training loss 0.8629425 lr 0.00033\n",
      "iteration 91600 training loss 0.83604956 lr 0.00033\n",
      "iteration 91700 training loss 0.87029743 lr 0.00033\n",
      "iteration 91800 training loss 0.6597867 lr 0.00033\n",
      "iteration 91900 training loss 0.7381389 lr 0.00033\n",
      "iteration 92000 training loss 0.7547064 lr 0.00033\n",
      "iteration 92100 training loss 0.917536 lr 0.00033\n",
      "iteration 92200 training loss 0.8841603 lr 0.00033\n",
      "iteration 92300 training loss 0.8030416 lr 0.00033\n",
      "iteration 92400 training loss 0.7579846 lr 0.00033\n",
      "iteration 92500 training loss 0.75668657 lr 0.00033\n",
      "iteration 92600 training loss 0.7622456 lr 0.00033\n",
      "iteration 92700 training loss 0.6381956 lr 0.00033\n",
      "iteration 92800 training loss 0.89814216 lr 0.00033\n",
      "iteration 92900 training loss 0.7753737 lr 0.00033\n",
      "iteration 93000 training loss 0.8738849 lr 0.00033\n",
      "iteration 93100 training loss 0.9016335 lr 0.00033\n",
      "iteration 93200 training loss 0.63856834 lr 0.00033\n",
      "iteration 93300 training loss 0.68557376 lr 0.00033\n",
      "iteration 93400 training loss 0.85865015 lr 0.00033\n",
      "iteration 93500 training loss 0.8689718 lr 0.00033\n",
      "iteration 93600 training loss 0.67858326 lr 0.00033\n",
      "iteration 93700 training loss 0.6467352 lr 0.00033\n",
      "iteration 93800 training loss 0.74630153 lr 0.00033\n",
      "iteration 93900 training loss 0.7542718 lr 0.00033\n",
      "iteration 94000 training loss 0.6389616 lr 0.00033\n",
      "iteration 94100 training loss 0.7274064 lr 0.00033\n",
      "iteration 94200 training loss 0.83392644 lr 0.00033\n",
      "iteration 94300 training loss 0.706489 lr 0.00033\n",
      "iteration 94400 training loss 0.9927934 lr 0.00033\n",
      "iteration 94500 training loss 0.7819584 lr 0.00033\n",
      "iteration 94600 training loss 0.7946144 lr 0.00033\n",
      "iteration 94700 training loss 0.7696757 lr 0.00033\n",
      "iteration 94800 training loss 0.82729465 lr 0.00033\n",
      "iteration 94900 training loss 0.7112334 lr 0.00033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 95000 training loss 0.74917644 lr 0.00033\n",
      "iteration 95100 training loss 0.793849 lr 0.00033\n",
      "iteration 95200 training loss 0.7791266 lr 0.00033\n",
      "iteration 95300 training loss 0.6504948 lr 0.00033\n",
      "iteration 95400 training loss 0.65666 lr 0.00033\n",
      "iteration 95500 training loss 0.7323087 lr 0.00033\n",
      "iteration 95600 training loss 0.59826535 lr 0.00033\n",
      "iteration 95700 training loss 0.6004999 lr 0.00033\n",
      "iteration 95800 training loss 0.78085065 lr 0.00033\n",
      "iteration 95900 training loss 0.81290805 lr 0.00033\n",
      "iteration 96000 training loss 0.7009763 lr 0.00033\n",
      "iteration 96100 training loss 0.6480862 lr 0.00033\n",
      "iteration 96200 training loss 0.7303793 lr 0.00033\n",
      "iteration 96300 training loss 0.86652505 lr 0.00033\n",
      "iteration 96400 training loss 0.7994003 lr 0.00033\n",
      "iteration 96500 training loss 0.86149263 lr 0.00033\n",
      "iteration 96600 training loss 0.6780917 lr 0.00033\n",
      "iteration 96700 training loss 0.9354434 lr 0.00033\n",
      "iteration 96800 training loss 0.94310796 lr 0.00033\n",
      "iteration 96900 training loss 0.74660665 lr 0.00033\n",
      "iteration 97000 training loss 0.7364425 lr 0.00033\n",
      "iteration 97100 training loss 0.75674856 lr 0.00033\n",
      "iteration 97200 training loss 0.79475754 lr 0.00033\n",
      "iteration 97300 training loss 0.96153873 lr 0.00033\n",
      "iteration 97400 training loss 0.86780643 lr 0.00033\n",
      "iteration 97500 training loss 0.8463137 lr 0.00033\n",
      "iteration 97600 training loss 0.8871616 lr 0.00033\n",
      "iteration 97700 training loss 0.6902855 lr 0.00033\n",
      "iteration 97800 training loss 0.7809689 lr 0.00033\n",
      "iteration 97900 training loss 0.79990155 lr 0.00033\n",
      "iteration 98000 training loss 0.8179464 lr 0.00033\n",
      "iteration 98100 training loss 0.6543796 lr 0.00033\n",
      "iteration 98200 training loss 0.7271098 lr 0.00033\n",
      "iteration 98300 training loss 0.62043357 lr 0.00033\n",
      "iteration 98400 training loss 0.8184659 lr 0.00033\n",
      "iteration 98500 training loss 0.7635322 lr 0.00033\n",
      "iteration 98600 training loss 0.6159656 lr 0.00033\n",
      "iteration 98700 training loss 0.74387485 lr 0.00033\n",
      "iteration 98800 training loss 0.78668725 lr 0.00033\n",
      "iteration 98900 training loss 0.7708395 lr 0.00033\n",
      "iteration 99000 training loss 0.5309529 lr 0.00033\n",
      "iteration 99100 training loss 0.71421784 lr 0.00033\n",
      "iteration 99200 training loss 0.6942154 lr 0.00033\n",
      "iteration 99300 training loss 0.88018227 lr 0.00033\n",
      "iteration 99400 training loss 0.74434507 lr 0.00033\n",
      "iteration 99500 training loss 0.66945493 lr 0.00033\n",
      "iteration 99600 training loss 0.7270508 lr 0.00033\n",
      "iteration 99700 training loss 0.8257452 lr 0.00033\n",
      "iteration 99800 training loss 0.62461996 lr 0.00033\n",
      "iteration 99900 training loss 0.7178369 lr 0.00033\n",
      "iteration 100000 training loss 0.7497693 lr 0.00033\n",
      "layout:nlp:random 0.7758162399084501\n",
      "layout:nlp:default 0.4009519919902639\n",
      "layout:xla:random 0.4313619792218028\n",
      "layout:xla:default 0.20932178323141418\n",
      "epoch 0, it 100000 validation loss -0.454\n",
      "iteration 100100 training loss 0.6458472 lr 0.00030\n",
      "iteration 100200 training loss 0.7729677 lr 0.00030\n",
      "iteration 100300 training loss 0.7741154 lr 0.00030\n",
      "iteration 100400 training loss 0.78929067 lr 0.00030\n",
      "iteration 100500 training loss 0.71539897 lr 0.00030\n",
      "iteration 100600 training loss 0.8082234 lr 0.00030\n",
      "iteration 100700 training loss 0.62081003 lr 0.00030\n",
      "iteration 100800 training loss 0.6210317 lr 0.00030\n",
      "iteration 100900 training loss 0.6069723 lr 0.00030\n",
      "iteration 101000 training loss 0.8056622 lr 0.00030\n",
      "iteration 101100 training loss 0.8327843 lr 0.00030\n",
      "iteration 101200 training loss 0.58320206 lr 0.00030\n",
      "iteration 101300 training loss 0.9870037 lr 0.00030\n",
      "iteration 101400 training loss 0.73391956 lr 0.00030\n",
      "iteration 101500 training loss 0.66961235 lr 0.00030\n",
      "iteration 101600 training loss 0.591686 lr 0.00030\n",
      "iteration 101700 training loss 0.9118217 lr 0.00030\n",
      "iteration 101800 training loss 0.4970782 lr 0.00030\n",
      "iteration 101900 training loss 0.6868108 lr 0.00030\n",
      "iteration 102000 training loss 0.9000018 lr 0.00030\n",
      "iteration 102100 training loss 0.67482626 lr 0.00030\n",
      "iteration 102200 training loss 1.043556 lr 0.00030\n",
      "iteration 102300 training loss 0.9177808 lr 0.00030\n",
      "iteration 102400 training loss 0.8504125 lr 0.00030\n",
      "iteration 102500 training loss 0.87717587 lr 0.00030\n",
      "iteration 102600 training loss 0.69192094 lr 0.00030\n",
      "iteration 102700 training loss 0.85349494 lr 0.00030\n",
      "iteration 102800 training loss 0.82245725 lr 0.00030\n",
      "iteration 102900 training loss 0.894944 lr 0.00030\n",
      "iteration 103000 training loss 0.8033918 lr 0.00030\n",
      "iteration 103100 training loss 0.9631184 lr 0.00030\n",
      "iteration 103200 training loss 0.7952831 lr 0.00030\n",
      "iteration 103300 training loss 0.91733366 lr 0.00030\n",
      "iteration 103400 training loss 0.9404291 lr 0.00030\n",
      "iteration 103500 training loss 0.64093167 lr 0.00030\n",
      "iteration 103600 training loss 0.88765514 lr 0.00030\n",
      "iteration 103700 training loss 0.72897655 lr 0.00030\n",
      "iteration 103800 training loss 0.728526 lr 0.00030\n",
      "iteration 103900 training loss 0.7795645 lr 0.00030\n",
      "iteration 104000 training loss 0.56655777 lr 0.00030\n",
      "iteration 104100 training loss 0.7560534 lr 0.00030\n",
      "iteration 104200 training loss 0.8656312 lr 0.00030\n",
      "iteration 104300 training loss 0.79047865 lr 0.00030\n",
      "iteration 104400 training loss 0.8270507 lr 0.00030\n",
      "iteration 104500 training loss 0.7898773 lr 0.00030\n",
      "iteration 104600 training loss 0.89600027 lr 0.00030\n",
      "iteration 104700 training loss 0.9978512 lr 0.00030\n",
      "iteration 104800 training loss 0.6297965 lr 0.00030\n",
      "iteration 104900 training loss 0.7876102 lr 0.00030\n",
      "iteration 105000 training loss 0.7740641 lr 0.00030\n",
      "iteration 105100 training loss 0.68949914 lr 0.00030\n",
      "iteration 105200 training loss 0.6534269 lr 0.00030\n",
      "iteration 105300 training loss 0.7879541 lr 0.00030\n",
      "iteration 105400 training loss 0.77312094 lr 0.00030\n",
      "iteration 105500 training loss 0.7881257 lr 0.00030\n",
      "iteration 105600 training loss 0.86027294 lr 0.00030\n",
      "iteration 105700 training loss 0.8258961 lr 0.00030\n",
      "iteration 105800 training loss 0.87477636 lr 0.00030\n",
      "iteration 105900 training loss 0.8678471 lr 0.00030\n",
      "iteration 106000 training loss 0.7239392 lr 0.00030\n",
      "iteration 106100 training loss 0.60719794 lr 0.00030\n",
      "iteration 106200 training loss 0.86716497 lr 0.00030\n",
      "iteration 106300 training loss 0.8480042 lr 0.00030\n",
      "iteration 106400 training loss 0.75124174 lr 0.00030\n",
      "iteration 106500 training loss 0.7588929 lr 0.00030\n",
      "iteration 106600 training loss 0.5962225 lr 0.00030\n",
      "iteration 106700 training loss 0.7845877 lr 0.00030\n",
      "iteration 106800 training loss 1.0180836 lr 0.00030\n",
      "iteration 106900 training loss 0.7002437 lr 0.00030\n",
      "iteration 107000 training loss 0.9398053 lr 0.00030\n",
      "iteration 107100 training loss 0.8999498 lr 0.00030\n",
      "iteration 107200 training loss 0.62508774 lr 0.00030\n",
      "iteration 107300 training loss 0.69302934 lr 0.00030\n",
      "iteration 107400 training loss 0.75709194 lr 0.00030\n",
      "iteration 107500 training loss 0.72113115 lr 0.00030\n",
      "iteration 107600 training loss 0.7401316 lr 0.00030\n",
      "iteration 107700 training loss 0.8314849 lr 0.00030\n",
      "iteration 107800 training loss 0.88520074 lr 0.00030\n",
      "iteration 107900 training loss 0.8007587 lr 0.00030\n",
      "iteration 108000 training loss 0.8816974 lr 0.00030\n",
      "iteration 108100 training loss 0.68705076 lr 0.00030\n",
      "iteration 108200 training loss 0.7087492 lr 0.00030\n",
      "iteration 108300 training loss 0.895299 lr 0.00030\n",
      "iteration 108400 training loss 0.8161349 lr 0.00030\n",
      "iteration 108500 training loss 0.8149916 lr 0.00030\n",
      "iteration 108600 training loss 0.6994389 lr 0.00030\n",
      "iteration 108700 training loss 0.7952114 lr 0.00030\n",
      "iteration 108800 training loss 0.79058534 lr 0.00030\n",
      "iteration 108900 training loss 0.81758016 lr 0.00030\n",
      "iteration 109000 training loss 0.7927763 lr 0.00030\n",
      "iteration 109100 training loss 0.86230165 lr 0.00030\n",
      "iteration 109200 training loss 1.0542108 lr 0.00030\n",
      "iteration 109300 training loss 0.72201943 lr 0.00030\n",
      "iteration 109400 training loss 0.82170993 lr 0.00030\n",
      "iteration 109500 training loss 0.7927688 lr 0.00030\n",
      "iteration 109600 training loss 0.77502686 lr 0.00030\n",
      "iteration 109700 training loss 0.80129844 lr 0.00030\n",
      "iteration 109800 training loss 0.73244494 lr 0.00030\n",
      "iteration 109900 training loss 0.60470194 lr 0.00030\n",
      "iteration 110000 training loss 0.7636787 lr 0.00030\n",
      "iteration 110100 training loss 0.6535423 lr 0.00030\n",
      "iteration 110200 training loss 0.7070628 lr 0.00030\n",
      "iteration 110300 training loss 0.71092975 lr 0.00030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 110400 training loss 0.70259297 lr 0.00030\n",
      "iteration 110500 training loss 0.85963523 lr 0.00030\n",
      "iteration 110600 training loss 0.768442 lr 0.00030\n",
      "iteration 110700 training loss 0.8665566 lr 0.00030\n",
      "iteration 110800 training loss 0.8579092 lr 0.00030\n",
      "iteration 110900 training loss 0.9058286 lr 0.00030\n",
      "iteration 111000 training loss 0.89910656 lr 0.00030\n",
      "iteration 111100 training loss 0.70246166 lr 0.00030\n",
      "iteration 111200 training loss 0.74773103 lr 0.00030\n",
      "iteration 111300 training loss 0.65951276 lr 0.00030\n",
      "iteration 111400 training loss 0.702297 lr 0.00030\n",
      "iteration 111500 training loss 0.718611 lr 0.00030\n",
      "iteration 111600 training loss 0.7935173 lr 0.00030\n",
      "iteration 111700 training loss 0.6623615 lr 0.00030\n",
      "iteration 111800 training loss 0.68761986 lr 0.00030\n",
      "iteration 111900 training loss 0.7269557 lr 0.00030\n",
      "iteration 112000 training loss 0.74067175 lr 0.00030\n",
      "iteration 112100 training loss 1.0323284 lr 0.00030\n",
      "iteration 112200 training loss 0.7988894 lr 0.00030\n",
      "iteration 112300 training loss 0.79020214 lr 0.00030\n",
      "iteration 112400 training loss 0.7954488 lr 0.00030\n",
      "iteration 112500 training loss 0.872688 lr 0.00030\n",
      "iteration 112600 training loss 1.024512 lr 0.00030\n",
      "iteration 112700 training loss 0.61958766 lr 0.00030\n",
      "iteration 112800 training loss 0.7129695 lr 0.00030\n",
      "iteration 112900 training loss 0.69483876 lr 0.00030\n",
      "iteration 113000 training loss 0.77718526 lr 0.00030\n",
      "iteration 113100 training loss 0.82297295 lr 0.00030\n",
      "iteration 113200 training loss 0.70549935 lr 0.00030\n",
      "iteration 113300 training loss 0.66306406 lr 0.00030\n",
      "iteration 113400 training loss 0.6811827 lr 0.00030\n",
      "iteration 113500 training loss 0.5446 lr 0.00030\n",
      "iteration 113600 training loss 0.8202731 lr 0.00030\n",
      "iteration 113700 training loss 0.90792567 lr 0.00030\n",
      "iteration 113800 training loss 0.80838144 lr 0.00030\n",
      "iteration 113900 training loss 0.9741718 lr 0.00030\n",
      "iteration 114000 training loss 0.64225686 lr 0.00030\n",
      "iteration 114100 training loss 0.6334305 lr 0.00030\n",
      "iteration 114200 training loss 0.6765767 lr 0.00030\n",
      "iteration 114300 training loss 0.73766476 lr 0.00030\n",
      "iteration 114400 training loss 0.5068304 lr 0.00030\n",
      "iteration 114500 training loss 0.6976701 lr 0.00030\n",
      "iteration 114600 training loss 0.8359343 lr 0.00030\n",
      "iteration 114700 training loss 0.7094676 lr 0.00030\n",
      "iteration 114800 training loss 0.771745 lr 0.00030\n",
      "iteration 114900 training loss 0.67124134 lr 0.00030\n",
      "iteration 115000 training loss 0.8008242 lr 0.00030\n",
      "iteration 115100 training loss 0.846829 lr 0.00030\n",
      "iteration 115200 training loss 0.665721 lr 0.00030\n",
      "iteration 115300 training loss 0.71528834 lr 0.00030\n",
      "iteration 115400 training loss 0.6201455 lr 0.00030\n",
      "iteration 115500 training loss 0.75582975 lr 0.00030\n",
      "iteration 115600 training loss 0.80469257 lr 0.00030\n",
      "iteration 115700 training loss 0.767981 lr 0.00030\n",
      "iteration 115800 training loss 0.73396426 lr 0.00030\n",
      "iteration 115900 training loss 0.65403986 lr 0.00030\n",
      "iteration 116000 training loss 0.8673425 lr 0.00030\n",
      "iteration 116100 training loss 0.80973476 lr 0.00030\n",
      "iteration 116200 training loss 0.663815 lr 0.00030\n",
      "iteration 116300 training loss 0.7112453 lr 0.00030\n",
      "iteration 116400 training loss 0.70743424 lr 0.00030\n",
      "iteration 116500 training loss 0.77716404 lr 0.00030\n",
      "iteration 116600 training loss 0.83444524 lr 0.00030\n",
      "iteration 116700 training loss 0.8424095 lr 0.00030\n",
      "iteration 116800 training loss 0.52210593 lr 0.00030\n",
      "iteration 116900 training loss 0.7977453 lr 0.00030\n",
      "iteration 117000 training loss 0.7427775 lr 0.00030\n",
      "iteration 117100 training loss 0.6060104 lr 0.00030\n",
      "iteration 117200 training loss 0.8367128 lr 0.00030\n",
      "iteration 117300 training loss 0.7442691 lr 0.00030\n",
      "iteration 117400 training loss 0.8526387 lr 0.00030\n",
      "iteration 117500 training loss 0.7273384 lr 0.00030\n",
      "iteration 117600 training loss 0.7539587 lr 0.00030\n",
      "iteration 117700 training loss 0.74958915 lr 0.00030\n",
      "iteration 117800 training loss 0.58432376 lr 0.00030\n",
      "iteration 117900 training loss 0.93048304 lr 0.00030\n",
      "iteration 118000 training loss 0.61984015 lr 0.00030\n",
      "iteration 118100 training loss 0.94962573 lr 0.00030\n",
      "iteration 118200 training loss 0.61234367 lr 0.00030\n",
      "iteration 118300 training loss 0.75597066 lr 0.00030\n",
      "iteration 118400 training loss 0.84373754 lr 0.00030\n",
      "iteration 118500 training loss 0.82898784 lr 0.00030\n",
      "iteration 118600 training loss 0.7786356 lr 0.00030\n",
      "iteration 118700 training loss 0.6806764 lr 0.00030\n",
      "iteration 118800 training loss 0.8400809 lr 0.00030\n",
      "iteration 118900 training loss 0.815402 lr 0.00030\n",
      "iteration 119000 training loss 0.70663166 lr 0.00030\n",
      "iteration 119100 training loss 0.9151883 lr 0.00030\n",
      "iteration 119200 training loss 0.7505513 lr 0.00030\n",
      "iteration 119300 training loss 0.82212836 lr 0.00030\n",
      "iteration 119400 training loss 0.7101766 lr 0.00030\n",
      "iteration 119500 training loss 0.84558934 lr 0.00030\n",
      "iteration 119600 training loss 0.8700806 lr 0.00030\n",
      "iteration 119700 training loss 0.783294 lr 0.00030\n",
      "iteration 119800 training loss 0.86750156 lr 0.00030\n",
      "iteration 119900 training loss 0.7468956 lr 0.00030\n",
      "iteration 120000 training loss 0.8510185 lr 0.00030\n",
      "layout:nlp:random 0.7907237705292534\n",
      "layout:nlp:default 0.39554114819665404\n",
      "layout:xla:random 0.3973525349925073\n",
      "layout:xla:default 0.14551514772460297\n",
      "epoch 0, it 120000 validation loss -0.432\n",
      "iteration 120100 training loss 0.7435809 lr 0.00027\n",
      "iteration 120200 training loss 0.81969744 lr 0.00027\n",
      "iteration 120300 training loss 0.6909796 lr 0.00027\n",
      "iteration 120400 training loss 0.71348983 lr 0.00027\n",
      "iteration 120500 training loss 0.79416025 lr 0.00027\n",
      "iteration 120600 training loss 0.8356741 lr 0.00027\n",
      "iteration 120700 training loss 0.5988449 lr 0.00027\n",
      "iteration 120800 training loss 0.78297794 lr 0.00027\n",
      "iteration 120900 training loss 0.70739496 lr 0.00027\n",
      "iteration 121000 training loss 0.6698929 lr 0.00027\n",
      "iteration 121100 training loss 0.80709124 lr 0.00027\n",
      "iteration 121200 training loss 0.7827092 lr 0.00027\n",
      "iteration 121300 training loss 0.8440611 lr 0.00027\n",
      "iteration 121400 training loss 0.74826133 lr 0.00027\n",
      "iteration 121500 training loss 0.79983854 lr 0.00027\n",
      "iteration 121600 training loss 0.97308064 lr 0.00027\n",
      "iteration 121700 training loss 0.9544306 lr 0.00027\n",
      "iteration 121800 training loss 0.7382193 lr 0.00027\n",
      "iteration 121900 training loss 0.86370677 lr 0.00027\n",
      "iteration 122000 training loss 0.69262654 lr 0.00027\n",
      "iteration 122100 training loss 0.69890887 lr 0.00027\n",
      "iteration 122200 training loss 0.96297413 lr 0.00027\n",
      "iteration 122300 training loss 0.8640686 lr 0.00027\n",
      "iteration 122400 training loss 0.7583353 lr 0.00027\n",
      "iteration 122500 training loss 0.7302059 lr 0.00027\n",
      "iteration 122600 training loss 0.72946393 lr 0.00027\n",
      "iteration 122700 training loss 0.8517582 lr 0.00027\n",
      "iteration 122800 training loss 0.9524441 lr 0.00027\n",
      "iteration 122900 training loss 0.7736755 lr 0.00027\n",
      "iteration 123000 training loss 0.8276828 lr 0.00027\n",
      "iteration 123100 training loss 0.8084037 lr 0.00027\n",
      "iteration 123200 training loss 0.7102946 lr 0.00027\n",
      "iteration 123300 training loss 0.9296486 lr 0.00027\n",
      "iteration 123400 training loss 0.778764 lr 0.00027\n",
      "iteration 123500 training loss 0.6135973 lr 0.00027\n",
      "iteration 123600 training loss 0.621522 lr 0.00027\n",
      "iteration 123700 training loss 0.7375847 lr 0.00027\n",
      "iteration 123800 training loss 0.8061849 lr 0.00027\n",
      "iteration 123900 training loss 0.93525785 lr 0.00027\n",
      "iteration 124000 training loss 0.8183116 lr 0.00027\n",
      "iteration 124100 training loss 0.58082515 lr 0.00027\n",
      "iteration 124200 training loss 0.84923327 lr 0.00027\n",
      "iteration 124300 training loss 0.88870627 lr 0.00027\n",
      "iteration 124400 training loss 0.8228629 lr 0.00027\n",
      "iteration 124500 training loss 0.57553786 lr 0.00027\n",
      "iteration 124600 training loss 0.8341625 lr 0.00027\n",
      "iteration 124700 training loss 0.8774923 lr 0.00027\n",
      "iteration 124800 training loss 0.89522165 lr 0.00027\n",
      "iteration 124900 training loss 0.75674284 lr 0.00027\n",
      "iteration 125000 training loss 0.87521315 lr 0.00027\n",
      "iteration 125100 training loss 0.651486 lr 0.00027\n",
      "iteration 125200 training loss 0.83813596 lr 0.00027\n",
      "iteration 125300 training loss 0.8223447 lr 0.00027\n",
      "iteration 125400 training loss 0.74535483 lr 0.00027\n",
      "iteration 125500 training loss 0.78998345 lr 0.00027\n",
      "iteration 125600 training loss 0.6663706 lr 0.00027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 125700 training loss 0.92953557 lr 0.00027\n",
      "iteration 125800 training loss 0.66600895 lr 0.00027\n",
      "iteration 125900 training loss 0.7508439 lr 0.00027\n",
      "iteration 126000 training loss 0.93010443 lr 0.00027\n",
      "iteration 126100 training loss 0.6281618 lr 0.00027\n",
      "iteration 126200 training loss 0.8364783 lr 0.00027\n",
      "iteration 126300 training loss 0.7217508 lr 0.00027\n",
      "iteration 126400 training loss 0.7142872 lr 0.00027\n",
      "iteration 126500 training loss 0.7098856 lr 0.00027\n",
      "iteration 126600 training loss 0.60771704 lr 0.00027\n",
      "iteration 126700 training loss 0.6426492 lr 0.00027\n",
      "iteration 126800 training loss 0.69864196 lr 0.00027\n",
      "iteration 126900 training loss 0.8170129 lr 0.00027\n",
      "iteration 127000 training loss 0.6541492 lr 0.00027\n",
      "iteration 127100 training loss 0.84495825 lr 0.00027\n",
      "iteration 127200 training loss 0.8324816 lr 0.00027\n",
      "iteration 127300 training loss 0.6716848 lr 0.00027\n",
      "iteration 127400 training loss 0.7071216 lr 0.00027\n",
      "iteration 127500 training loss 0.63765323 lr 0.00027\n",
      "iteration 127600 training loss 0.6285523 lr 0.00027\n",
      "iteration 127700 training loss 0.9183015 lr 0.00027\n",
      "iteration 127800 training loss 0.58415693 lr 0.00027\n",
      "iteration 127900 training loss 0.62783587 lr 0.00027\n",
      "iteration 128000 training loss 0.6569437 lr 0.00027\n",
      "iteration 128100 training loss 0.6326413 lr 0.00027\n",
      "iteration 128200 training loss 0.7003447 lr 0.00027\n",
      "iteration 128300 training loss 0.6470025 lr 0.00027\n",
      "iteration 128400 training loss 0.7205193 lr 0.00027\n",
      "iteration 128500 training loss 0.74780464 lr 0.00027\n",
      "iteration 128600 training loss 0.9488828 lr 0.00027\n",
      "iteration 128700 training loss 0.8470903 lr 0.00027\n",
      "iteration 128800 training loss 0.47202474 lr 0.00027\n",
      "iteration 128900 training loss 0.8018383 lr 0.00027\n",
      "iteration 129000 training loss 0.7069221 lr 0.00027\n",
      "iteration 129100 training loss 0.7567111 lr 0.00027\n",
      "iteration 129200 training loss 0.48239774 lr 0.00027\n",
      "iteration 129300 training loss 0.6630557 lr 0.00027\n",
      "iteration 129400 training loss 0.644329 lr 0.00027\n",
      "iteration 129500 training loss 0.6751156 lr 0.00027\n",
      "iteration 129600 training loss 0.72442263 lr 0.00027\n",
      "iteration 129700 training loss 0.67099494 lr 0.00027\n",
      "iteration 129800 training loss 0.68493193 lr 0.00027\n",
      "iteration 129900 training loss 0.67206895 lr 0.00027\n",
      "iteration 130000 training loss 0.5696106 lr 0.00027\n",
      "iteration 130100 training loss 0.6862549 lr 0.00027\n",
      "iteration 130200 training loss 0.71723694 lr 0.00027\n",
      "iteration 130300 training loss 0.85773253 lr 0.00027\n",
      "iteration 130400 training loss 0.92139566 lr 0.00027\n",
      "iteration 130500 training loss 0.7168689 lr 0.00027\n",
      "iteration 130600 training loss 0.9146957 lr 0.00027\n",
      "iteration 130700 training loss 0.58388835 lr 0.00027\n",
      "iteration 130800 training loss 0.7017606 lr 0.00027\n",
      "iteration 130900 training loss 0.8969549 lr 0.00027\n",
      "iteration 131000 training loss 0.6960908 lr 0.00027\n",
      "iteration 131100 training loss 0.69169724 lr 0.00027\n",
      "iteration 131200 training loss 0.6244662 lr 0.00027\n",
      "iteration 131300 training loss 0.75276697 lr 0.00027\n",
      "iteration 131400 training loss 0.7430694 lr 0.00027\n",
      "iteration 131500 training loss 0.9943208 lr 0.00027\n",
      "iteration 131600 training loss 0.6239679 lr 0.00027\n",
      "iteration 131700 training loss 0.63976586 lr 0.00027\n",
      "iteration 131800 training loss 0.717428 lr 0.00027\n",
      "iteration 131900 training loss 0.8899475 lr 0.00027\n",
      "iteration 132000 training loss 0.62250954 lr 0.00027\n",
      "iteration 132100 training loss 0.86913955 lr 0.00027\n",
      "iteration 132200 training loss 0.58799076 lr 0.00027\n",
      "iteration 132300 training loss 0.8312554 lr 0.00027\n",
      "iteration 132400 training loss 0.79260993 lr 0.00027\n",
      "iteration 132500 training loss 0.8746862 lr 0.00027\n",
      "iteration 132600 training loss 0.9360953 lr 0.00027\n",
      "iteration 132700 training loss 0.6553927 lr 0.00027\n",
      "iteration 132800 training loss 0.84985083 lr 0.00027\n",
      "iteration 132900 training loss 0.7175187 lr 0.00027\n",
      "iteration 133000 training loss 0.60800177 lr 0.00027\n",
      "iteration 133100 training loss 0.7511462 lr 0.00027\n",
      "iteration 133200 training loss 0.6486002 lr 0.00027\n",
      "iteration 133300 training loss 0.6150559 lr 0.00027\n",
      "iteration 133400 training loss 0.8400283 lr 0.00027\n",
      "iteration 133500 training loss 0.78032726 lr 0.00027\n",
      "iteration 133600 training loss 0.82828104 lr 0.00027\n",
      "iteration 133700 training loss 0.808634 lr 0.00027\n",
      "iteration 133800 training loss 0.9689646 lr 0.00027\n",
      "iteration 133900 training loss 0.6706516 lr 0.00027\n",
      "iteration 134000 training loss 0.7184477 lr 0.00027\n",
      "iteration 134100 training loss 0.75146914 lr 0.00027\n",
      "iteration 134200 training loss 0.59545565 lr 0.00027\n",
      "iteration 134300 training loss 0.8407757 lr 0.00027\n",
      "iteration 134400 training loss 0.7592397 lr 0.00027\n",
      "iteration 134500 training loss 0.8724559 lr 0.00027\n",
      "iteration 134600 training loss 0.7241512 lr 0.00027\n",
      "iteration 134700 training loss 0.6916161 lr 0.00027\n",
      "iteration 134800 training loss 0.6014126 lr 0.00027\n",
      "iteration 134900 training loss 0.84807694 lr 0.00027\n",
      "iteration 135000 training loss 0.7975584 lr 0.00027\n",
      "iteration 135100 training loss 0.72774327 lr 0.00027\n",
      "iteration 135200 training loss 0.8589765 lr 0.00027\n",
      "iteration 135300 training loss 0.7330995 lr 0.00027\n",
      "iteration 135400 training loss 0.8578386 lr 0.00027\n",
      "iteration 135500 training loss 0.8385402 lr 0.00027\n",
      "iteration 135600 training loss 0.6534397 lr 0.00027\n",
      "iteration 135700 training loss 0.6525888 lr 0.00027\n",
      "iteration 135800 training loss 0.80523753 lr 0.00027\n",
      "iteration 135900 training loss 0.64282995 lr 0.00027\n",
      "iteration 136000 training loss 0.9727357 lr 0.00027\n",
      "iteration 136100 training loss 0.8276646 lr 0.00027\n",
      "iteration 136200 training loss 0.8372718 lr 0.00027\n",
      "iteration 136300 training loss 0.86178744 lr 0.00027\n",
      "iteration 136400 training loss 0.87357265 lr 0.00027\n",
      "iteration 136500 training loss 0.7617801 lr 0.00027\n",
      "iteration 136600 training loss 0.83886147 lr 0.00027\n",
      "iteration 136700 training loss 0.77613586 lr 0.00027\n",
      "iteration 136800 training loss 0.73865134 lr 0.00027\n",
      "iteration 136900 training loss 0.82955515 lr 0.00027\n",
      "iteration 137000 training loss 0.8683636 lr 0.00027\n",
      "iteration 137100 training loss 0.72757477 lr 0.00027\n",
      "iteration 137200 training loss 0.84122056 lr 0.00027\n",
      "iteration 137300 training loss 0.9287598 lr 0.00027\n",
      "iteration 137400 training loss 0.7079713 lr 0.00027\n",
      "iteration 137500 training loss 0.68100387 lr 0.00027\n",
      "iteration 137600 training loss 0.64871013 lr 0.00027\n",
      "iteration 137700 training loss 0.654482 lr 0.00027\n",
      "iteration 137800 training loss 0.7903366 lr 0.00027\n",
      "iteration 137900 training loss 0.90319085 lr 0.00027\n",
      "iteration 138000 training loss 0.7900429 lr 0.00027\n",
      "iteration 138100 training loss 0.6792341 lr 0.00027\n",
      "iteration 138200 training loss 0.7517316 lr 0.00027\n",
      "iteration 138300 training loss 0.72749215 lr 0.00027\n",
      "iteration 138400 training loss 0.68140537 lr 0.00027\n",
      "iteration 138500 training loss 0.8081126 lr 0.00027\n",
      "iteration 138600 training loss 0.7315765 lr 0.00027\n",
      "iteration 138700 training loss 0.7248297 lr 0.00027\n",
      "iteration 138800 training loss 0.93027383 lr 0.00027\n",
      "iteration 138900 training loss 0.7817598 lr 0.00027\n",
      "iteration 139000 training loss 0.68700993 lr 0.00027\n",
      "iteration 139100 training loss 0.93958014 lr 0.00027\n",
      "iteration 139200 training loss 0.9359651 lr 0.00027\n",
      "iteration 139300 training loss 0.86067104 lr 0.00027\n",
      "iteration 139400 training loss 0.70525104 lr 0.00027\n",
      "iteration 139500 training loss 0.85735637 lr 0.00027\n",
      "iteration 139600 training loss 0.8133432 lr 0.00027\n",
      "iteration 139700 training loss 0.837858 lr 0.00027\n",
      "iteration 139800 training loss 0.8042344 lr 0.00027\n",
      "iteration 139900 training loss 0.8030371 lr 0.00027\n",
      "iteration 140000 training loss 0.7209936 lr 0.00027\n",
      "layout:nlp:random 0.7662273867699634\n",
      "layout:nlp:default 0.4048138156366142\n",
      "layout:xla:random 0.40565144038636786\n",
      "layout:xla:default 0.16376953764822996\n",
      "epoch 0, it 140000 validation loss -0.435\n",
      "iteration 140100 training loss 0.900874 lr 0.00024\n",
      "iteration 140200 training loss 0.81573874 lr 0.00024\n",
      "iteration 140300 training loss 0.73629826 lr 0.00024\n",
      "iteration 140400 training loss 0.6390483 lr 0.00024\n",
      "iteration 140500 training loss 0.8853637 lr 0.00024\n",
      "iteration 140600 training loss 0.88830644 lr 0.00024\n",
      "iteration 140700 training loss 0.7883431 lr 0.00024\n",
      "iteration 140800 training loss 0.70569265 lr 0.00024\n",
      "iteration 140900 training loss 0.75168717 lr 0.00024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 141000 training loss 0.7068497 lr 0.00024\n",
      "iteration 141100 training loss 0.719588 lr 0.00024\n",
      "iteration 141200 training loss 0.7551401 lr 0.00024\n",
      "iteration 141300 training loss 0.6685441 lr 0.00024\n",
      "iteration 141400 training loss 0.793195 lr 0.00024\n",
      "iteration 141500 training loss 0.5926173 lr 0.00024\n",
      "iteration 141600 training loss 0.8782683 lr 0.00024\n",
      "iteration 141700 training loss 0.64135945 lr 0.00024\n",
      "iteration 141800 training loss 0.73302174 lr 0.00024\n",
      "iteration 141900 training loss 0.6182794 lr 0.00024\n",
      "iteration 142000 training loss 0.735648 lr 0.00024\n",
      "iteration 142100 training loss 0.8068293 lr 0.00024\n",
      "iteration 142200 training loss 0.7095306 lr 0.00024\n",
      "iteration 142300 training loss 0.57718956 lr 0.00024\n",
      "iteration 142400 training loss 0.5834407 lr 0.00024\n",
      "iteration 142500 training loss 0.5402183 lr 0.00024\n",
      "iteration 142600 training loss 0.6893199 lr 0.00024\n",
      "iteration 142700 training loss 0.5868073 lr 0.00024\n",
      "iteration 142800 training loss 0.77963585 lr 0.00024\n",
      "iteration 142900 training loss 0.5677145 lr 0.00024\n",
      "iteration 143000 training loss 0.60690236 lr 0.00024\n",
      "iteration 143100 training loss 0.585686 lr 0.00024\n",
      "iteration 143200 training loss 0.5553848 lr 0.00024\n",
      "iteration 143300 training loss 0.81997734 lr 0.00024\n",
      "iteration 143400 training loss 0.5463764 lr 0.00024\n",
      "iteration 143500 training loss 0.60069793 lr 0.00024\n",
      "iteration 143600 training loss 0.57882583 lr 0.00024\n",
      "iteration 143700 training loss 0.4970173 lr 0.00024\n",
      "iteration 143800 training loss 0.7989472 lr 0.00024\n",
      "iteration 143900 training loss 0.6286305 lr 0.00024\n",
      "iteration 144000 training loss 0.65768224 lr 0.00024\n",
      "iteration 144100 training loss 0.59504735 lr 0.00024\n",
      "iteration 144200 training loss 0.72948855 lr 0.00024\n",
      "iteration 144300 training loss 1.0010812 lr 0.00024\n",
      "iteration 144400 training loss 0.52445495 lr 0.00024\n",
      "iteration 144500 training loss 0.69664735 lr 0.00024\n",
      "iteration 144600 training loss 0.5121844 lr 0.00024\n",
      "iteration 144700 training loss 0.8256647 lr 0.00024\n",
      "iteration 144800 training loss 0.91487074 lr 0.00024\n",
      "iteration 144900 training loss 0.7069238 lr 0.00024\n",
      "iteration 145000 training loss 0.7983815 lr 0.00024\n",
      "iteration 145100 training loss 0.6868616 lr 0.00024\n",
      "iteration 145200 training loss 0.6971892 lr 0.00024\n",
      "iteration 145300 training loss 0.800964 lr 0.00024\n",
      "iteration 145400 training loss 0.6589347 lr 0.00024\n",
      "iteration 145500 training loss 0.7220288 lr 0.00024\n",
      "iteration 145600 training loss 0.8376724 lr 0.00024\n",
      "iteration 145700 training loss 0.7509753 lr 0.00024\n",
      "iteration 145800 training loss 0.7611545 lr 0.00024\n",
      "iteration 145900 training loss 0.742408 lr 0.00024\n",
      "iteration 146000 training loss 0.8149211 lr 0.00024\n",
      "iteration 146100 training loss 0.72901237 lr 0.00024\n",
      "iteration 146200 training loss 0.795351 lr 0.00024\n",
      "iteration 146300 training loss 0.6714256 lr 0.00024\n",
      "iteration 146400 training loss 0.7486329 lr 0.00024\n",
      "iteration 146500 training loss 0.7109502 lr 0.00024\n",
      "iteration 146600 training loss 0.6849973 lr 0.00024\n",
      "iteration 146700 training loss 0.7432768 lr 0.00024\n",
      "iteration 146800 training loss 0.64709204 lr 0.00024\n",
      "iteration 146900 training loss 0.8197078 lr 0.00024\n",
      "iteration 147000 training loss 0.8443459 lr 0.00024\n",
      "iteration 147100 training loss 0.7212962 lr 0.00024\n",
      "iteration 147200 training loss 0.6728264 lr 0.00024\n",
      "iteration 147300 training loss 0.88773936 lr 0.00024\n",
      "iteration 147400 training loss 0.90824133 lr 0.00024\n",
      "iteration 147500 training loss 0.7465288 lr 0.00024\n",
      "iteration 147600 training loss 0.7160735 lr 0.00024\n",
      "iteration 147700 training loss 0.68308955 lr 0.00024\n",
      "iteration 147800 training loss 0.85876614 lr 0.00024\n",
      "iteration 147900 training loss 0.66818494 lr 0.00024\n",
      "iteration 148000 training loss 0.77598536 lr 0.00024\n",
      "iteration 148100 training loss 0.6967875 lr 0.00024\n",
      "iteration 148200 training loss 0.7499101 lr 0.00024\n",
      "iteration 148300 training loss 0.7608211 lr 0.00024\n",
      "iteration 148400 training loss 0.91136146 lr 0.00024\n",
      "iteration 148500 training loss 0.78463405 lr 0.00024\n",
      "iteration 148600 training loss 0.62776953 lr 0.00024\n",
      "iteration 148700 training loss 0.8059423 lr 0.00024\n",
      "iteration 148800 training loss 0.94402933 lr 0.00024\n",
      "iteration 148900 training loss 0.77783054 lr 0.00024\n",
      "iteration 149000 training loss 0.68883 lr 0.00024\n",
      "iteration 149100 training loss 0.678005 lr 0.00024\n",
      "iteration 149200 training loss 0.7792443 lr 0.00024\n",
      "iteration 149300 training loss 0.78322434 lr 0.00024\n",
      "iteration 149400 training loss 0.8117176 lr 0.00024\n",
      "iteration 149500 training loss 0.69868886 lr 0.00024\n",
      "iteration 149600 training loss 0.8277075 lr 0.00024\n",
      "iteration 149700 training loss 0.64773333 lr 0.00024\n",
      "iteration 149800 training loss 0.77071774 lr 0.00024\n",
      "iteration 149900 training loss 0.8955779 lr 0.00024\n",
      "iteration 150000 training loss 0.83609146 lr 0.00024\n",
      "iteration 150100 training loss 0.6885382 lr 0.00024\n",
      "iteration 150200 training loss 0.6299943 lr 0.00024\n",
      "iteration 150300 training loss 0.7546639 lr 0.00024\n",
      "iteration 150400 training loss 0.8143572 lr 0.00024\n",
      "iteration 150500 training loss 0.7823658 lr 0.00024\n",
      "iteration 150600 training loss 0.7459271 lr 0.00024\n",
      "iteration 150700 training loss 0.7575235 lr 0.00024\n",
      "iteration 150800 training loss 0.83708924 lr 0.00024\n",
      "iteration 150900 training loss 0.7663194 lr 0.00024\n",
      "iteration 151000 training loss 0.93207127 lr 0.00024\n",
      "iteration 151100 training loss 0.7511112 lr 0.00024\n",
      "iteration 151200 training loss 0.89302325 lr 0.00024\n",
      "iteration 151300 training loss 0.7826589 lr 0.00024\n",
      "iteration 151400 training loss 1.0223532 lr 0.00024\n",
      "iteration 151500 training loss 0.7140429 lr 0.00024\n",
      "iteration 151600 training loss 0.69347966 lr 0.00024\n",
      "iteration 151700 training loss 0.8395139 lr 0.00024\n",
      "iteration 151800 training loss 0.7522876 lr 0.00024\n",
      "iteration 151900 training loss 0.80541784 lr 0.00024\n",
      "iteration 152000 training loss 0.80907804 lr 0.00024\n",
      "iteration 152100 training loss 0.63344115 lr 0.00024\n",
      "iteration 152200 training loss 0.7598207 lr 0.00024\n",
      "iteration 152300 training loss 0.7020255 lr 0.00024\n",
      "iteration 152400 training loss 0.8322224 lr 0.00024\n",
      "iteration 152500 training loss 0.7069709 lr 0.00024\n",
      "iteration 152600 training loss 0.7248346 lr 0.00024\n",
      "iteration 152700 training loss 0.88405097 lr 0.00024\n",
      "iteration 152800 training loss 0.65515697 lr 0.00024\n",
      "iteration 152900 training loss 0.6879455 lr 0.00024\n",
      "iteration 153000 training loss 0.73363423 lr 0.00024\n",
      "iteration 153100 training loss 0.731372 lr 0.00024\n",
      "iteration 153200 training loss 0.73680425 lr 0.00024\n",
      "iteration 153300 training loss 0.7436237 lr 0.00024\n",
      "iteration 153400 training loss 0.5929908 lr 0.00024\n",
      "iteration 153500 training loss 0.6636957 lr 0.00024\n",
      "iteration 153600 training loss 0.76454204 lr 0.00024\n",
      "iteration 153700 training loss 0.7269565 lr 0.00024\n",
      "iteration 153800 training loss 0.81054145 lr 0.00024\n",
      "iteration 153900 training loss 0.78043145 lr 0.00024\n",
      "iteration 154000 training loss 0.8394185 lr 0.00024\n",
      "iteration 154100 training loss 0.75127834 lr 0.00024\n",
      "iteration 154200 training loss 0.8213676 lr 0.00024\n",
      "iteration 154300 training loss 0.78047234 lr 0.00024\n",
      "iteration 154400 training loss 0.6591314 lr 0.00024\n",
      "iteration 154500 training loss 0.7546672 lr 0.00024\n",
      "iteration 154600 training loss 0.8311066 lr 0.00024\n",
      "iteration 154700 training loss 0.75114155 lr 0.00024\n",
      "iteration 154800 training loss 0.876589 lr 0.00024\n",
      "iteration 154900 training loss 0.7121784 lr 0.00024\n",
      "iteration 155000 training loss 0.72070056 lr 0.00024\n",
      "iteration 155100 training loss 0.5887275 lr 0.00024\n",
      "iteration 155200 training loss 0.684304 lr 0.00024\n",
      "iteration 155300 training loss 0.6912897 lr 0.00024\n",
      "iteration 155400 training loss 0.6767212 lr 0.00024\n",
      "iteration 155500 training loss 0.8820991 lr 0.00024\n",
      "iteration 155600 training loss 0.6944552 lr 0.00024\n",
      "iteration 155700 training loss 0.93912613 lr 0.00024\n",
      "iteration 155800 training loss 0.75637364 lr 0.00024\n",
      "iteration 155900 training loss 0.6556108 lr 0.00024\n",
      "iteration 156000 training loss 0.7786896 lr 0.00024\n",
      "iteration 156100 training loss 0.5632344 lr 0.00024\n",
      "iteration 156200 training loss 0.74593383 lr 0.00024\n",
      "iteration 156300 training loss 0.789441 lr 0.00024\n",
      "iteration 156400 training loss 0.7067977 lr 0.00024\n",
      "iteration 156500 training loss 0.6061471 lr 0.00024\n",
      "iteration 156600 training loss 0.89448524 lr 0.00024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 156700 training loss 0.7542779 lr 0.00024\n",
      "iteration 156800 training loss 0.69784784 lr 0.00024\n",
      "iteration 156900 training loss 0.6436925 lr 0.00024\n",
      "iteration 157000 training loss 0.8072392 lr 0.00024\n",
      "iteration 157100 training loss 0.74878496 lr 0.00024\n",
      "iteration 157200 training loss 0.7448007 lr 0.00024\n",
      "iteration 157300 training loss 0.6629867 lr 0.00024\n",
      "iteration 157400 training loss 0.73895955 lr 0.00024\n",
      "iteration 157500 training loss 0.7951522 lr 0.00024\n",
      "iteration 157600 training loss 0.88491046 lr 0.00024\n",
      "iteration 157700 training loss 0.58187515 lr 0.00024\n",
      "iteration 157800 training loss 0.86281496 lr 0.00024\n",
      "iteration 157900 training loss 0.6180664 lr 0.00024\n",
      "iteration 158000 training loss 0.61202824 lr 0.00024\n",
      "iteration 158100 training loss 0.671061 lr 0.00024\n",
      "iteration 158200 training loss 0.71801984 lr 0.00024\n",
      "iteration 158300 training loss 0.67019784 lr 0.00024\n",
      "iteration 158400 training loss 0.5885588 lr 0.00024\n",
      "iteration 158500 training loss 0.5969853 lr 0.00024\n",
      "iteration 158600 training loss 0.58507496 lr 0.00024\n",
      "iteration 158700 training loss 0.5881081 lr 0.00024\n",
      "iteration 158800 training loss 0.69703996 lr 0.00024\n",
      "iteration 158900 training loss 0.6149879 lr 0.00024\n",
      "iteration 159000 training loss 0.5068757 lr 0.00024\n",
      "iteration 159100 training loss 0.58851206 lr 0.00024\n",
      "iteration 159200 training loss 0.5530989 lr 0.00024\n",
      "iteration 159300 training loss 0.68251157 lr 0.00024\n",
      "iteration 159400 training loss 0.8232568 lr 0.00024\n",
      "iteration 159500 training loss 0.66240615 lr 0.00024\n",
      "iteration 159600 training loss 0.7230456 lr 0.00024\n",
      "iteration 159700 training loss 0.58104247 lr 0.00024\n",
      "iteration 159800 training loss 0.71788645 lr 0.00024\n",
      "iteration 159900 training loss 0.6690881 lr 0.00024\n",
      "iteration 160000 training loss 0.7283708 lr 0.00024\n",
      "layout:nlp:random 0.7998287509089803\n",
      "layout:nlp:default 0.41820985846457814\n",
      "layout:xla:random 0.46662359694890504\n",
      "layout:xla:default 0.2013671008370401\n",
      "epoch 0, it 160000 validation loss -0.472\n",
      "iteration 160100 training loss 0.73434323 lr 0.00022\n",
      "iteration 160200 training loss 0.63606036 lr 0.00022\n",
      "iteration 160300 training loss 0.7492855 lr 0.00022\n",
      "iteration 160400 training loss 0.7117573 lr 0.00022\n",
      "iteration 160500 training loss 0.8661271 lr 0.00022\n",
      "iteration 160600 training loss 0.8014537 lr 0.00022\n",
      "iteration 160700 training loss 0.7596127 lr 0.00022\n",
      "iteration 160800 training loss 0.62679243 lr 0.00022\n",
      "iteration 160900 training loss 0.78221285 lr 0.00022\n",
      "iteration 161000 training loss 0.76024544 lr 0.00022\n",
      "iteration 161100 training loss 0.66348046 lr 0.00022\n",
      "iteration 161200 training loss 0.77216804 lr 0.00022\n",
      "iteration 161300 training loss 0.8758352 lr 0.00022\n",
      "iteration 161400 training loss 0.7975982 lr 0.00022\n",
      "iteration 161500 training loss 0.74527615 lr 0.00022\n",
      "iteration 161600 training loss 0.7501114 lr 0.00022\n",
      "iteration 161700 training loss 0.7319145 lr 0.00022\n",
      "iteration 161800 training loss 0.84231126 lr 0.00022\n",
      "iteration 161900 training loss 0.8267966 lr 0.00022\n",
      "iteration 162000 training loss 0.73542416 lr 0.00022\n",
      "iteration 162100 training loss 0.7770706 lr 0.00022\n",
      "iteration 162200 training loss 0.9409616 lr 0.00022\n",
      "iteration 162300 training loss 0.702981 lr 0.00022\n",
      "iteration 162400 training loss 0.7799911 lr 0.00022\n",
      "iteration 162500 training loss 0.6290093 lr 0.00022\n",
      "iteration 162600 training loss 0.8167644 lr 0.00022\n",
      "iteration 162700 training loss 0.6523622 lr 0.00022\n",
      "iteration 162800 training loss 0.7739997 lr 0.00022\n",
      "iteration 162900 training loss 0.72062665 lr 0.00022\n",
      "iteration 163000 training loss 0.7291865 lr 0.00022\n",
      "iteration 163100 training loss 0.8745339 lr 0.00022\n",
      "iteration 163200 training loss 0.8498529 lr 0.00022\n",
      "iteration 163300 training loss 0.91702956 lr 0.00022\n",
      "iteration 163400 training loss 0.5703651 lr 0.00022\n",
      "iteration 163500 training loss 0.9071723 lr 0.00022\n",
      "iteration 163600 training loss 0.72523195 lr 0.00022\n",
      "iteration 163700 training loss 0.65628564 lr 0.00022\n",
      "iteration 163800 training loss 0.8329111 lr 0.00022\n",
      "iteration 163900 training loss 0.65853906 lr 0.00022\n",
      "iteration 164000 training loss 0.77789515 lr 0.00022\n",
      "iteration 164100 training loss 0.66678065 lr 0.00022\n",
      "iteration 164200 training loss 0.5456399 lr 0.00022\n",
      "iteration 164300 training loss 0.59914446 lr 0.00022\n",
      "iteration 164400 training loss 0.8623903 lr 0.00022\n",
      "iteration 164500 training loss 0.80652434 lr 0.00022\n",
      "iteration 164600 training loss 0.73697597 lr 0.00022\n",
      "iteration 164700 training loss 0.86808276 lr 0.00022\n",
      "iteration 164800 training loss 0.96235234 lr 0.00022\n",
      "iteration 164900 training loss 0.89962417 lr 0.00022\n",
      "iteration 165000 training loss 0.63891673 lr 0.00022\n",
      "iteration 165100 training loss 0.7515741 lr 0.00022\n",
      "iteration 165200 training loss 0.769666 lr 0.00022\n",
      "iteration 165300 training loss 0.65375805 lr 0.00022\n",
      "iteration 165400 training loss 0.7431071 lr 0.00022\n",
      "iteration 165500 training loss 0.82342154 lr 0.00022\n",
      "iteration 165600 training loss 0.77898985 lr 0.00022\n",
      "iteration 165700 training loss 0.76066494 lr 0.00022\n",
      "iteration 165800 training loss 0.9012491 lr 0.00022\n",
      "iteration 165900 training loss 0.8403553 lr 0.00022\n",
      "iteration 166000 training loss 0.6983846 lr 0.00022\n",
      "iteration 166100 training loss 0.7500127 lr 0.00022\n",
      "iteration 166200 training loss 0.64279 lr 0.00022\n",
      "iteration 166300 training loss 0.95363396 lr 0.00022\n",
      "iteration 166400 training loss 0.83405375 lr 0.00022\n",
      "iteration 166500 training loss 0.84466356 lr 0.00022\n",
      "iteration 166600 training loss 0.8377939 lr 0.00022\n",
      "iteration 166700 training loss 0.8461223 lr 0.00022\n",
      "iteration 166800 training loss 0.6462078 lr 0.00022\n",
      "iteration 166900 training loss 0.76782763 lr 0.00022\n",
      "iteration 167000 training loss 0.6373061 lr 0.00022\n",
      "iteration 167100 training loss 0.86103034 lr 0.00022\n",
      "iteration 167200 training loss 0.8259495 lr 0.00022\n",
      "iteration 167300 training loss 0.90311223 lr 0.00022\n",
      "iteration 167400 training loss 0.83643246 lr 0.00022\n",
      "iteration 167500 training loss 0.77101773 lr 0.00022\n",
      "iteration 167600 training loss 0.829217 lr 0.00022\n",
      "iteration 167700 training loss 0.6494546 lr 0.00022\n",
      "iteration 167800 training loss 0.7843986 lr 0.00022\n",
      "iteration 167900 training loss 0.84875995 lr 0.00022\n",
      "iteration 168000 training loss 0.8608036 lr 0.00022\n",
      "iteration 168100 training loss 0.70323116 lr 0.00022\n",
      "iteration 168200 training loss 0.7740686 lr 0.00022\n",
      "iteration 168300 training loss 0.59610903 lr 0.00022\n",
      "iteration 168400 training loss 0.87684035 lr 0.00022\n",
      "iteration 168500 training loss 0.8066338 lr 0.00022\n",
      "iteration 168600 training loss 0.8576633 lr 0.00022\n",
      "iteration 168700 training loss 0.7533002 lr 0.00022\n",
      "iteration 168800 training loss 0.59623706 lr 0.00022\n",
      "iteration 168900 training loss 0.9373896 lr 0.00022\n",
      "iteration 169000 training loss 0.74596345 lr 0.00022\n",
      "iteration 169100 training loss 0.7939319 lr 0.00022\n",
      "iteration 169200 training loss 0.7795466 lr 0.00022\n",
      "iteration 169300 training loss 0.94481236 lr 0.00022\n",
      "iteration 169400 training loss 0.7004116 lr 0.00022\n",
      "iteration 169500 training loss 0.70129013 lr 0.00022\n",
      "iteration 169600 training loss 0.8155609 lr 0.00022\n",
      "iteration 169700 training loss 0.84177876 lr 0.00022\n",
      "iteration 169800 training loss 0.7275075 lr 0.00022\n",
      "iteration 169900 training loss 0.82072026 lr 0.00022\n",
      "iteration 170000 training loss 0.8187916 lr 0.00022\n",
      "iteration 170100 training loss 0.7143871 lr 0.00022\n",
      "iteration 170200 training loss 0.79694027 lr 0.00022\n",
      "iteration 170300 training loss 0.66071546 lr 0.00022\n",
      "iteration 170400 training loss 0.6261901 lr 0.00022\n",
      "iteration 170500 training loss 0.7448853 lr 0.00022\n",
      "iteration 170600 training loss 0.7204854 lr 0.00022\n",
      "iteration 170700 training loss 0.65480316 lr 0.00022\n",
      "iteration 170800 training loss 0.6318106 lr 0.00022\n",
      "iteration 170900 training loss 0.69118756 lr 0.00022\n",
      "iteration 171000 training loss 0.7368256 lr 0.00022\n",
      "iteration 171100 training loss 0.8917109 lr 0.00022\n",
      "iteration 171200 training loss 0.8186608 lr 0.00022\n",
      "iteration 171300 training loss 0.81703824 lr 0.00022\n",
      "iteration 171400 training loss 0.74534935 lr 0.00022\n",
      "iteration 171500 training loss 0.76296645 lr 0.00022\n",
      "iteration 171600 training loss 0.76602376 lr 0.00022\n",
      "iteration 171700 training loss 0.7224068 lr 0.00022\n",
      "iteration 171800 training loss 0.912421 lr 0.00022\n",
      "iteration 171900 training loss 0.8224262 lr 0.00022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 172000 training loss 0.7148115 lr 0.00022\n",
      "iteration 172100 training loss 0.62420464 lr 0.00022\n",
      "iteration 172200 training loss 0.7571539 lr 0.00022\n",
      "iteration 172300 training loss 0.5957254 lr 0.00022\n",
      "iteration 172400 training loss 0.57165974 lr 0.00022\n",
      "iteration 172500 training loss 0.68335545 lr 0.00022\n",
      "iteration 172600 training loss 0.7159439 lr 0.00022\n",
      "iteration 172700 training loss 0.6162562 lr 0.00022\n",
      "iteration 172800 training loss 0.5184195 lr 0.00022\n",
      "iteration 172900 training loss 0.61332834 lr 0.00022\n",
      "iteration 173000 training loss 0.72659314 lr 0.00022\n",
      "iteration 173100 training loss 0.7728555 lr 0.00022\n",
      "iteration 173200 training loss 0.85452944 lr 0.00022\n",
      "iteration 173300 training loss 0.95761544 lr 0.00022\n",
      "iteration 173400 training loss 0.8581638 lr 0.00022\n",
      "iteration 173500 training loss 0.72667 lr 0.00022\n",
      "iteration 173600 training loss 0.61457086 lr 0.00022\n",
      "iteration 173700 training loss 0.5651269 lr 0.00022\n",
      "iteration 173800 training loss 0.68405473 lr 0.00022\n",
      "iteration 173900 training loss 0.6771863 lr 0.00022\n",
      "iteration 174000 training loss 0.655836 lr 0.00022\n",
      "iteration 174100 training loss 0.5964545 lr 0.00022\n",
      "iteration 174200 training loss 0.691518 lr 0.00022\n",
      "iteration 174300 training loss 0.6673265 lr 0.00022\n",
      "iteration 174400 training loss 0.8397041 lr 0.00022\n",
      "iteration 174500 training loss 0.72719944 lr 0.00022\n",
      "iteration 174600 training loss 0.66726834 lr 0.00022\n",
      "iteration 174700 training loss 0.69224685 lr 0.00022\n",
      "iteration 174800 training loss 0.7137546 lr 0.00022\n",
      "iteration 174900 training loss 0.70246565 lr 0.00022\n",
      "iteration 175000 training loss 0.8310851 lr 0.00022\n",
      "iteration 175100 training loss 0.7456832 lr 0.00022\n",
      "iteration 175200 training loss 0.6677714 lr 0.00022\n",
      "iteration 175300 training loss 0.76513636 lr 0.00022\n",
      "iteration 175400 training loss 0.83756214 lr 0.00022\n",
      "iteration 175500 training loss 0.49965015 lr 0.00022\n",
      "iteration 175600 training loss 0.7236848 lr 0.00022\n",
      "iteration 175700 training loss 0.7718107 lr 0.00022\n",
      "iteration 175800 training loss 0.7084367 lr 0.00022\n",
      "iteration 175900 training loss 0.6652551 lr 0.00022\n",
      "iteration 176000 training loss 0.67470205 lr 0.00022\n",
      "iteration 176100 training loss 0.65435123 lr 0.00022\n",
      "iteration 176200 training loss 0.73597485 lr 0.00022\n",
      "iteration 176300 training loss 0.7669015 lr 0.00022\n",
      "iteration 176400 training loss 0.8312845 lr 0.00022\n",
      "iteration 176500 training loss 0.9453266 lr 0.00022\n",
      "iteration 176600 training loss 0.79131424 lr 0.00022\n",
      "iteration 176700 training loss 0.6822831 lr 0.00022\n",
      "iteration 176800 training loss 0.8191378 lr 0.00022\n",
      "iteration 176900 training loss 0.8227055 lr 0.00022\n",
      "iteration 177000 training loss 0.72134525 lr 0.00022\n",
      "iteration 177100 training loss 0.8205617 lr 0.00022\n",
      "iteration 177200 training loss 0.74926937 lr 0.00022\n",
      "iteration 177300 training loss 0.84686345 lr 0.00022\n",
      "iteration 177400 training loss 0.93006575 lr 0.00022\n",
      "iteration 177500 training loss 0.7969991 lr 0.00022\n",
      "iteration 177600 training loss 0.89403564 lr 0.00022\n",
      "iteration 177700 training loss 0.6864322 lr 0.00022\n",
      "iteration 177800 training loss 0.9883755 lr 0.00022\n",
      "iteration 177900 training loss 0.82355714 lr 0.00022\n",
      "iteration 178000 training loss 0.83814484 lr 0.00022\n",
      "iteration 178100 training loss 0.61599344 lr 0.00022\n",
      "iteration 178200 training loss 0.7947623 lr 0.00022\n",
      "iteration 178300 training loss 0.72686356 lr 0.00022\n",
      "iteration 178400 training loss 0.67783165 lr 0.00022\n",
      "iteration 178500 training loss 0.72123784 lr 0.00022\n",
      "iteration 178600 training loss 0.81256163 lr 0.00022\n",
      "iteration 178700 training loss 0.95671344 lr 0.00022\n",
      "iteration 178800 training loss 0.7994574 lr 0.00022\n",
      "iteration 178900 training loss 0.63979894 lr 0.00022\n",
      "iteration 179000 training loss 0.8033344 lr 0.00022\n",
      "iteration 179100 training loss 0.6355627 lr 0.00022\n",
      "iteration 179200 training loss 0.70945007 lr 0.00022\n",
      "iteration 179300 training loss 0.8824017 lr 0.00022\n",
      "iteration 179400 training loss 0.5982079 lr 0.00022\n",
      "iteration 179500 training loss 0.6716025 lr 0.00022\n",
      "iteration 179600 training loss 0.53816783 lr 0.00022\n",
      "iteration 179700 training loss 0.5173752 lr 0.00022\n",
      "iteration 179800 training loss 0.6768937 lr 0.00022\n",
      "iteration 179900 training loss 0.8098597 lr 0.00022\n",
      "iteration 180000 training loss 0.6707089 lr 0.00022\n",
      "layout:nlp:random 0.7829589132108565\n",
      "layout:nlp:default 0.4012580855736167\n",
      "layout:xla:random 0.5165550418430298\n",
      "layout:xla:default 0.2124260856517354\n",
      "epoch 0, it 180000 validation loss -0.478\n",
      "iteration 180100 training loss 0.77361566 lr 0.00019\n",
      "iteration 180200 training loss 0.80228204 lr 0.00019\n",
      "iteration 180300 training loss 0.82662505 lr 0.00019\n",
      "iteration 180400 training loss 0.7793528 lr 0.00019\n",
      "iteration 180500 training loss 0.74607015 lr 0.00019\n",
      "iteration 180600 training loss 0.68123484 lr 0.00019\n",
      "iteration 180700 training loss 0.673638 lr 0.00019\n",
      "iteration 180800 training loss 0.74241483 lr 0.00019\n",
      "iteration 180900 training loss 0.6807263 lr 0.00019\n",
      "iteration 181000 training loss 0.68714094 lr 0.00019\n",
      "iteration 181100 training loss 0.8583367 lr 0.00019\n",
      "iteration 181200 training loss 0.81586516 lr 0.00019\n",
      "iteration 181300 training loss 0.7017898 lr 0.00019\n",
      "iteration 181400 training loss 0.6595606 lr 0.00019\n",
      "iteration 181500 training loss 0.7682552 lr 0.00019\n",
      "iteration 181600 training loss 0.80398744 lr 0.00019\n",
      "iteration 181700 training loss 0.784513 lr 0.00019\n",
      "iteration 181800 training loss 0.6566914 lr 0.00019\n",
      "iteration 181900 training loss 0.87856084 lr 0.00019\n",
      "iteration 182000 training loss 0.85253274 lr 0.00019\n",
      "iteration 182100 training loss 0.7525847 lr 0.00019\n",
      "iteration 182200 training loss 0.63390464 lr 0.00019\n",
      "iteration 182300 training loss 0.6235917 lr 0.00019\n",
      "iteration 182400 training loss 0.7691093 lr 0.00019\n",
      "iteration 182500 training loss 0.73854214 lr 0.00019\n",
      "iteration 182600 training loss 0.773483 lr 0.00019\n",
      "iteration 182700 training loss 0.80146366 lr 0.00019\n",
      "iteration 182800 training loss 0.74722886 lr 0.00019\n",
      "iteration 182900 training loss 0.74594796 lr 0.00019\n",
      "iteration 183000 training loss 0.92931867 lr 0.00019\n",
      "iteration 183100 training loss 0.6699947 lr 0.00019\n",
      "iteration 183200 training loss 0.65633893 lr 0.00019\n",
      "iteration 183300 training loss 0.69554085 lr 0.00019\n",
      "iteration 183400 training loss 0.76631534 lr 0.00019\n",
      "iteration 183500 training loss 0.7956871 lr 0.00019\n",
      "iteration 183600 training loss 0.61145085 lr 0.00019\n",
      "iteration 183700 training loss 0.7822735 lr 0.00019\n",
      "iteration 183800 training loss 0.6111135 lr 0.00019\n",
      "iteration 183900 training loss 0.68121487 lr 0.00019\n",
      "iteration 184000 training loss 0.9025278 lr 0.00019\n",
      "iteration 184100 training loss 0.83875906 lr 0.00019\n",
      "iteration 184200 training loss 0.6547462 lr 0.00019\n",
      "iteration 184300 training loss 0.6749559 lr 0.00019\n",
      "iteration 184400 training loss 0.77482635 lr 0.00019\n",
      "iteration 184500 training loss 0.7991261 lr 0.00019\n",
      "iteration 184600 training loss 0.8190433 lr 0.00019\n",
      "iteration 184700 training loss 0.8385362 lr 0.00019\n",
      "iteration 184800 training loss 0.89592946 lr 0.00019\n",
      "iteration 184900 training loss 0.69038177 lr 0.00019\n",
      "iteration 185000 training loss 0.5360937 lr 0.00019\n",
      "iteration 185100 training loss 0.7616215 lr 0.00019\n",
      "iteration 185200 training loss 0.72834224 lr 0.00019\n",
      "iteration 185300 training loss 0.6754574 lr 0.00019\n",
      "iteration 185400 training loss 0.88715273 lr 0.00019\n",
      "iteration 185500 training loss 0.70721745 lr 0.00019\n",
      "iteration 185600 training loss 0.9151948 lr 0.00019\n",
      "iteration 185700 training loss 0.668446 lr 0.00019\n",
      "iteration 185800 training loss 0.770405 lr 0.00019\n",
      "iteration 185900 training loss 0.616413 lr 0.00019\n",
      "iteration 186000 training loss 0.69971216 lr 0.00019\n",
      "iteration 186100 training loss 0.8310807 lr 0.00019\n",
      "iteration 186200 training loss 0.62423503 lr 0.00019\n",
      "iteration 186300 training loss 0.7499731 lr 0.00019\n",
      "iteration 186400 training loss 0.63980913 lr 0.00019\n",
      "iteration 186500 training loss 0.6213309 lr 0.00019\n",
      "iteration 186600 training loss 0.82790166 lr 0.00019\n",
      "iteration 186700 training loss 0.74450517 lr 0.00019\n",
      "iteration 186800 training loss 0.87294 lr 0.00019\n",
      "iteration 186900 training loss 0.6570805 lr 0.00019\n",
      "iteration 187000 training loss 0.65106297 lr 0.00019\n",
      "iteration 187100 training loss 0.80760807 lr 0.00019\n",
      "iteration 187200 training loss 0.80886316 lr 0.00019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 187300 training loss 0.6589648 lr 0.00019\n",
      "iteration 187400 training loss 0.5730493 lr 0.00019\n",
      "iteration 187500 training loss 0.6885205 lr 0.00019\n",
      "iteration 187600 training loss 0.62772846 lr 0.00019\n",
      "iteration 187700 training loss 0.71795267 lr 0.00019\n",
      "iteration 187800 training loss 0.53813 lr 0.00019\n",
      "iteration 187900 training loss 0.74252254 lr 0.00019\n",
      "iteration 188000 training loss 0.5128956 lr 0.00019\n",
      "iteration 188100 training loss 0.72311723 lr 0.00019\n",
      "iteration 188200 training loss 0.63284767 lr 0.00019\n",
      "iteration 188300 training loss 0.86406374 lr 0.00019\n",
      "iteration 188400 training loss 0.84983927 lr 0.00019\n",
      "iteration 188500 training loss 0.83003265 lr 0.00019\n",
      "iteration 188600 training loss 0.7603096 lr 0.00019\n",
      "iteration 188700 training loss 0.7085623 lr 0.00019\n",
      "iteration 188800 training loss 0.8641589 lr 0.00019\n",
      "iteration 188900 training loss 0.65089333 lr 0.00019\n",
      "iteration 189000 training loss 0.6861465 lr 0.00019\n",
      "iteration 189100 training loss 0.8074794 lr 0.00019\n",
      "iteration 189200 training loss 0.6823859 lr 0.00019\n",
      "iteration 189300 training loss 0.7654274 lr 0.00019\n",
      "iteration 189400 training loss 0.5287811 lr 0.00019\n",
      "iteration 189500 training loss 0.831485 lr 0.00019\n",
      "iteration 189600 training loss 0.846887 lr 0.00019\n",
      "iteration 189700 training loss 0.8165636 lr 0.00019\n",
      "iteration 189800 training loss 0.7930786 lr 0.00019\n",
      "iteration 189900 training loss 0.57126266 lr 0.00019\n",
      "iteration 190000 training loss 0.77113706 lr 0.00019\n",
      "iteration 190100 training loss 0.6540119 lr 0.00019\n",
      "iteration 190200 training loss 0.80435354 lr 0.00019\n",
      "iteration 190300 training loss 0.6314244 lr 0.00019\n",
      "iteration 190400 training loss 0.836493 lr 0.00019\n",
      "iteration 190500 training loss 0.78291047 lr 0.00019\n",
      "iteration 190600 training loss 0.53111017 lr 0.00019\n",
      "iteration 190700 training loss 0.9435532 lr 0.00019\n",
      "iteration 190800 training loss 0.74004537 lr 0.00019\n",
      "iteration 190900 training loss 0.6434735 lr 0.00019\n",
      "iteration 191000 training loss 0.8727224 lr 0.00019\n",
      "iteration 191100 training loss 0.6557833 lr 0.00019\n",
      "iteration 191200 training loss 0.6764344 lr 0.00019\n",
      "iteration 191300 training loss 0.6640967 lr 0.00019\n",
      "iteration 191400 training loss 0.6138059 lr 0.00019\n",
      "iteration 191500 training loss 0.6150179 lr 0.00019\n",
      "iteration 191600 training loss 0.8145729 lr 0.00019\n",
      "iteration 191700 training loss 0.8037295 lr 0.00019\n",
      "iteration 191800 training loss 0.73947823 lr 0.00019\n",
      "iteration 191900 training loss 0.99559313 lr 0.00019\n",
      "iteration 192000 training loss 0.86034834 lr 0.00019\n",
      "iteration 192100 training loss 0.81046665 lr 0.00019\n",
      "iteration 192200 training loss 0.7573183 lr 0.00019\n",
      "iteration 192300 training loss 0.7612322 lr 0.00019\n",
      "iteration 192400 training loss 0.7984983 lr 0.00019\n",
      "iteration 192500 training loss 0.6219158 lr 0.00019\n",
      "iteration 192600 training loss 0.80666876 lr 0.00019\n",
      "iteration 192700 training loss 0.6413857 lr 0.00019\n",
      "iteration 192800 training loss 0.79327303 lr 0.00019\n",
      "iteration 192900 training loss 0.6527162 lr 0.00019\n",
      "iteration 193000 training loss 0.8152245 lr 0.00019\n",
      "iteration 193100 training loss 0.77603483 lr 0.00019\n",
      "iteration 193200 training loss 0.7463806 lr 0.00019\n",
      "iteration 193300 training loss 0.74582595 lr 0.00019\n",
      "iteration 193400 training loss 0.6480972 lr 0.00019\n",
      "iteration 193500 training loss 0.7711179 lr 0.00019\n",
      "iteration 193600 training loss 0.72333694 lr 0.00019\n",
      "iteration 193700 training loss 0.6665268 lr 0.00019\n",
      "iteration 193800 training loss 0.8509212 lr 0.00019\n",
      "iteration 193900 training loss 0.69189364 lr 0.00019\n",
      "iteration 194000 training loss 0.7304674 lr 0.00019\n",
      "iteration 194100 training loss 1.1014314 lr 0.00019\n",
      "iteration 194200 training loss 0.8125529 lr 0.00019\n",
      "iteration 194300 training loss 0.672276 lr 0.00019\n",
      "iteration 194400 training loss 0.6115303 lr 0.00019\n",
      "iteration 194500 training loss 0.677312 lr 0.00019\n",
      "iteration 194600 training loss 0.71748334 lr 0.00019\n",
      "iteration 194700 training loss 0.8764918 lr 0.00019\n",
      "iteration 194800 training loss 0.7397723 lr 0.00019\n",
      "iteration 194900 training loss 0.6873315 lr 0.00019\n",
      "iteration 195000 training loss 0.7905605 lr 0.00019\n",
      "iteration 195100 training loss 0.8157842 lr 0.00019\n",
      "iteration 195200 training loss 0.67216766 lr 0.00019\n",
      "iteration 195300 training loss 0.86978793 lr 0.00019\n",
      "iteration 195400 training loss 0.7004723 lr 0.00019\n",
      "iteration 195500 training loss 0.82589024 lr 0.00019\n",
      "iteration 195600 training loss 0.8224135 lr 0.00019\n",
      "iteration 195700 training loss 0.6997096 lr 0.00019\n",
      "iteration 195800 training loss 0.6514676 lr 0.00019\n",
      "iteration 195900 training loss 0.7825948 lr 0.00019\n",
      "iteration 196000 training loss 0.663419 lr 0.00019\n",
      "iteration 196100 training loss 0.61652213 lr 0.00019\n",
      "iteration 196200 training loss 0.7632721 lr 0.00019\n",
      "iteration 196300 training loss 0.8058557 lr 0.00019\n",
      "iteration 196400 training loss 0.83580685 lr 0.00019\n",
      "iteration 196500 training loss 0.63812274 lr 0.00019\n",
      "iteration 196600 training loss 0.7795961 lr 0.00019\n",
      "iteration 196700 training loss 0.7566498 lr 0.00019\n",
      "iteration 196800 training loss 0.76036036 lr 0.00019\n",
      "iteration 196900 training loss 0.813019 lr 0.00019\n",
      "iteration 197000 training loss 0.7886292 lr 0.00019\n",
      "iteration 197100 training loss 0.8220118 lr 0.00019\n",
      "iteration 197200 training loss 0.62074363 lr 0.00019\n",
      "iteration 197300 training loss 0.62202626 lr 0.00019\n",
      "iteration 197400 training loss 0.9560027 lr 0.00019\n",
      "iteration 197500 training loss 0.7594031 lr 0.00019\n",
      "iteration 197600 training loss 0.7262504 lr 0.00019\n",
      "iteration 197700 training loss 0.6250762 lr 0.00019\n",
      "iteration 197800 training loss 0.6796669 lr 0.00019\n",
      "iteration 197900 training loss 0.81612396 lr 0.00019\n",
      "iteration 198000 training loss 0.7319729 lr 0.00019\n",
      "iteration 198100 training loss 0.7856639 lr 0.00019\n",
      "iteration 198200 training loss 0.72769785 lr 0.00019\n",
      "iteration 198300 training loss 0.75882274 lr 0.00019\n",
      "iteration 198400 training loss 0.58772635 lr 0.00019\n",
      "iteration 198500 training loss 0.64837027 lr 0.00019\n",
      "iteration 198600 training loss 0.8398869 lr 0.00019\n",
      "iteration 198700 training loss 0.7133526 lr 0.00019\n",
      "iteration 198800 training loss 0.72079134 lr 0.00019\n",
      "iteration 198900 training loss 0.7885372 lr 0.00019\n",
      "iteration 199000 training loss 0.74629885 lr 0.00019\n",
      "iteration 199100 training loss 0.82589924 lr 0.00019\n",
      "iteration 199200 training loss 0.7202345 lr 0.00019\n",
      "iteration 199300 training loss 0.93842345 lr 0.00019\n",
      "iteration 199400 training loss 0.66448617 lr 0.00019\n",
      "iteration 199500 training loss 0.824116 lr 0.00019\n",
      "iteration 199600 training loss 0.54718924 lr 0.00019\n",
      "iteration 199700 training loss 0.6149154 lr 0.00019\n",
      "iteration 199800 training loss 0.76335186 lr 0.00019\n",
      "iteration 199900 training loss 0.75247544 lr 0.00019\n",
      "iteration 200000 training loss 0.810802 lr 0.00019\n",
      "layout:nlp:random 0.7794278239570054\n",
      "layout:nlp:default 0.40879497059352043\n",
      "layout:xla:random 0.44039401697886754\n",
      "layout:xla:default 0.22198673070406502\n",
      "epoch 0, it 200000 validation loss -0.463\n",
      "iteration 200100 training loss 0.6617525 lr 0.00017\n",
      "iteration 200200 training loss 0.6448222 lr 0.00017\n",
      "iteration 200300 training loss 0.7187983 lr 0.00017\n",
      "iteration 200400 training loss 0.691258 lr 0.00017\n",
      "iteration 200500 training loss 0.83090127 lr 0.00017\n",
      "iteration 200600 training loss 0.6066886 lr 0.00017\n",
      "iteration 200700 training loss 0.5508728 lr 0.00017\n",
      "iteration 200800 training loss 0.61770195 lr 0.00017\n",
      "iteration 200900 training loss 0.6354749 lr 0.00017\n",
      "iteration 201000 training loss 0.58720005 lr 0.00017\n",
      "iteration 201100 training loss 0.55475265 lr 0.00017\n",
      "iteration 201200 training loss 0.68238413 lr 0.00017\n",
      "iteration 201300 training loss 0.43225336 lr 0.00017\n",
      "iteration 201400 training loss 0.6557016 lr 0.00017\n",
      "iteration 201500 training loss 0.7283287 lr 0.00017\n",
      "iteration 201600 training loss 0.64076877 lr 0.00017\n",
      "iteration 201700 training loss 0.63791656 lr 0.00017\n",
      "iteration 201800 training loss 0.5785013 lr 0.00017\n",
      "iteration 201900 training loss 0.74433887 lr 0.00017\n",
      "iteration 202000 training loss 0.7056511 lr 0.00017\n",
      "iteration 202100 training loss 0.6334583 lr 0.00017\n",
      "iteration 202200 training loss 0.75786734 lr 0.00017\n",
      "iteration 202300 training loss 0.50638807 lr 0.00017\n",
      "iteration 202400 training loss 0.58665174 lr 0.00017\n",
      "iteration 202500 training loss 0.63286316 lr 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 202600 training loss 0.55297285 lr 0.00017\n",
      "iteration 202700 training loss 0.5677525 lr 0.00017\n",
      "iteration 202800 training loss 0.629296 lr 0.00017\n",
      "iteration 202900 training loss 0.7760787 lr 0.00017\n",
      "iteration 203000 training loss 0.7477061 lr 0.00017\n",
      "iteration 203100 training loss 0.60570556 lr 0.00017\n",
      "iteration 203200 training loss 0.728659 lr 0.00017\n",
      "iteration 203300 training loss 0.66279405 lr 0.00017\n",
      "iteration 203400 training loss 1.0373855 lr 0.00017\n",
      "iteration 203500 training loss 0.73457605 lr 0.00017\n",
      "iteration 203600 training loss 0.7305053 lr 0.00017\n",
      "iteration 203700 training loss 0.81076574 lr 0.00017\n",
      "iteration 203800 training loss 0.80654883 lr 0.00017\n",
      "iteration 203900 training loss 0.9636515 lr 0.00017\n",
      "iteration 204000 training loss 0.7389885 lr 0.00017\n",
      "iteration 204100 training loss 0.89773846 lr 0.00017\n",
      "iteration 204200 training loss 0.65888286 lr 0.00017\n",
      "iteration 204300 training loss 0.8568594 lr 0.00017\n",
      "iteration 204400 training loss 0.765292 lr 0.00017\n",
      "iteration 204500 training loss 0.8465657 lr 0.00017\n",
      "iteration 204600 training loss 0.7291163 lr 0.00017\n",
      "iteration 204700 training loss 0.7335982 lr 0.00017\n",
      "iteration 204800 training loss 0.63923705 lr 0.00017\n",
      "iteration 204900 training loss 0.79245603 lr 0.00017\n",
      "iteration 205000 training loss 0.80995363 lr 0.00017\n",
      "iteration 205100 training loss 0.8315339 lr 0.00017\n",
      "iteration 205200 training loss 0.71916246 lr 0.00017\n",
      "iteration 205300 training loss 0.7040346 lr 0.00017\n",
      "iteration 205400 training loss 0.68455654 lr 0.00017\n",
      "iteration 205500 training loss 0.645667 lr 0.00017\n",
      "iteration 205600 training loss 0.6488584 lr 0.00017\n",
      "iteration 205700 training loss 0.926924 lr 0.00017\n",
      "iteration 205800 training loss 0.5307992 lr 0.00017\n",
      "iteration 205900 training loss 0.7340287 lr 0.00017\n",
      "iteration 206000 training loss 0.8039151 lr 0.00017\n",
      "iteration 206100 training loss 0.73098975 lr 0.00017\n",
      "iteration 206200 training loss 0.6918754 lr 0.00017\n",
      "iteration 206300 training loss 0.7358482 lr 0.00017\n",
      "iteration 206400 training loss 0.6583948 lr 0.00017\n",
      "iteration 206500 training loss 0.61141664 lr 0.00017\n",
      "iteration 206600 training loss 0.8431751 lr 0.00017\n",
      "iteration 206700 training loss 0.86161387 lr 0.00017\n",
      "iteration 206800 training loss 0.58531773 lr 0.00017\n",
      "iteration 206900 training loss 0.7062582 lr 0.00017\n",
      "iteration 207000 training loss 0.7558719 lr 0.00017\n",
      "iteration 207100 training loss 0.65149176 lr 0.00017\n",
      "iteration 207200 training loss 0.6135857 lr 0.00017\n",
      "iteration 207300 training loss 0.5651476 lr 0.00017\n",
      "iteration 207400 training loss 0.6986177 lr 0.00017\n",
      "iteration 207500 training loss 0.7586606 lr 0.00017\n",
      "iteration 207600 training loss 0.5184347 lr 0.00017\n",
      "iteration 207700 training loss 0.76196104 lr 0.00017\n",
      "iteration 207800 training loss 0.7100525 lr 0.00017\n",
      "iteration 207900 training loss 0.6229596 lr 0.00017\n",
      "iteration 208000 training loss 0.74175555 lr 0.00017\n",
      "iteration 208100 training loss 0.7872998 lr 0.00017\n",
      "iteration 208200 training loss 0.7258106 lr 0.00017\n",
      "iteration 208300 training loss 0.7869382 lr 0.00017\n",
      "iteration 208400 training loss 0.7189408 lr 0.00017\n",
      "iteration 208500 training loss 0.70690954 lr 0.00017\n",
      "iteration 208600 training loss 0.7408419 lr 0.00017\n",
      "iteration 208700 training loss 0.744034 lr 0.00017\n",
      "iteration 208800 training loss 0.6431222 lr 0.00017\n",
      "iteration 208900 training loss 0.7519815 lr 0.00017\n",
      "iteration 209000 training loss 1.0255374 lr 0.00017\n",
      "iteration 209100 training loss 0.7118 lr 0.00017\n",
      "iteration 209200 training loss 0.8278903 lr 0.00017\n",
      "iteration 209300 training loss 0.7351929 lr 0.00017\n",
      "iteration 209400 training loss 0.7203249 lr 0.00017\n",
      "iteration 209500 training loss 0.8226504 lr 0.00017\n",
      "iteration 209600 training loss 0.651153 lr 0.00017\n",
      "iteration 209700 training loss 0.7294018 lr 0.00017\n",
      "iteration 209800 training loss 0.6244485 lr 0.00017\n",
      "iteration 209900 training loss 0.60527176 lr 0.00017\n",
      "iteration 210000 training loss 0.78473556 lr 0.00017\n",
      "iteration 210100 training loss 0.7769553 lr 0.00017\n",
      "iteration 210200 training loss 0.8436926 lr 0.00017\n",
      "iteration 210300 training loss 0.71944505 lr 0.00017\n",
      "iteration 210400 training loss 0.79372066 lr 0.00017\n",
      "iteration 210500 training loss 0.69173753 lr 0.00017\n",
      "iteration 210600 training loss 0.78300714 lr 0.00017\n",
      "iteration 210700 training loss 0.8180075 lr 0.00017\n",
      "iteration 210800 training loss 0.79604346 lr 0.00017\n",
      "iteration 210900 training loss 0.5785391 lr 0.00017\n",
      "iteration 211000 training loss 0.7934111 lr 0.00017\n",
      "iteration 211100 training loss 0.97196025 lr 0.00017\n",
      "iteration 211200 training loss 0.7592785 lr 0.00017\n",
      "iteration 211300 training loss 0.77599114 lr 0.00017\n",
      "iteration 211400 training loss 0.56762046 lr 0.00017\n",
      "iteration 211500 training loss 0.7662002 lr 0.00017\n",
      "iteration 211600 training loss 0.85286367 lr 0.00017\n",
      "iteration 211700 training loss 0.74901754 lr 0.00017\n",
      "iteration 211800 training loss 0.66815585 lr 0.00017\n",
      "iteration 211900 training loss 0.70565504 lr 0.00017\n",
      "iteration 212000 training loss 0.80275613 lr 0.00017\n",
      "iteration 212100 training loss 0.798658 lr 0.00017\n",
      "iteration 212200 training loss 0.68341446 lr 0.00017\n",
      "iteration 212300 training loss 0.7900785 lr 0.00017\n",
      "iteration 212400 training loss 0.75529194 lr 0.00017\n",
      "iteration 212500 training loss 0.7515076 lr 0.00017\n",
      "iteration 212600 training loss 0.57503533 lr 0.00017\n",
      "iteration 212700 training loss 0.70535403 lr 0.00017\n",
      "iteration 212800 training loss 0.7236868 lr 0.00017\n",
      "iteration 212900 training loss 0.8666383 lr 0.00017\n",
      "iteration 213000 training loss 0.8723212 lr 0.00017\n",
      "iteration 213100 training loss 0.868996 lr 0.00017\n",
      "iteration 213200 training loss 0.68894047 lr 0.00017\n",
      "iteration 213300 training loss 0.69764453 lr 0.00017\n",
      "iteration 213400 training loss 0.7782199 lr 0.00017\n",
      "iteration 213500 training loss 0.6124375 lr 0.00017\n",
      "iteration 213600 training loss 0.80464673 lr 0.00017\n",
      "iteration 213700 training loss 0.8255096 lr 0.00017\n",
      "iteration 213800 training loss 0.8642023 lr 0.00017\n",
      "iteration 213900 training loss 0.79748946 lr 0.00017\n",
      "iteration 214000 training loss 0.85800195 lr 0.00017\n",
      "iteration 214100 training loss 0.6209938 lr 0.00017\n",
      "iteration 214200 training loss 0.81912917 lr 0.00017\n",
      "iteration 214300 training loss 0.7340851 lr 0.00017\n",
      "iteration 214400 training loss 0.6351427 lr 0.00017\n",
      "iteration 214500 training loss 0.89484733 lr 0.00017\n",
      "iteration 214600 training loss 0.718928 lr 0.00017\n",
      "iteration 214700 training loss 0.73330635 lr 0.00017\n",
      "iteration 214800 training loss 0.7341119 lr 0.00017\n",
      "iteration 214900 training loss 0.68731934 lr 0.00017\n",
      "iteration 215000 training loss 0.7269963 lr 0.00017\n",
      "iteration 215100 training loss 0.7779105 lr 0.00017\n",
      "iteration 215200 training loss 0.5803646 lr 0.00017\n",
      "iteration 215300 training loss 0.659726 lr 0.00017\n",
      "iteration 215400 training loss 0.51411843 lr 0.00017\n",
      "iteration 215500 training loss 0.55263805 lr 0.00017\n",
      "iteration 215600 training loss 0.5937044 lr 0.00017\n",
      "iteration 215700 training loss 0.68432987 lr 0.00017\n",
      "iteration 215800 training loss 0.53697777 lr 0.00017\n",
      "iteration 215900 training loss 0.5904815 lr 0.00017\n",
      "iteration 216000 training loss 0.6831296 lr 0.00017\n",
      "iteration 216100 training loss 0.7112017 lr 0.00017\n",
      "iteration 216200 training loss 0.6997964 lr 0.00017\n",
      "iteration 216300 training loss 0.59644735 lr 0.00017\n",
      "iteration 216400 training loss 0.5741113 lr 0.00017\n",
      "iteration 216500 training loss 0.7837704 lr 0.00017\n",
      "iteration 216600 training loss 0.5989173 lr 0.00017\n",
      "iteration 216700 training loss 0.6550826 lr 0.00017\n",
      "iteration 216800 training loss 0.58455926 lr 0.00017\n",
      "iteration 216900 training loss 0.6034729 lr 0.00017\n",
      "iteration 217000 training loss 0.47181723 lr 0.00017\n",
      "iteration 217100 training loss 0.60107875 lr 0.00017\n",
      "iteration 217200 training loss 0.6807333 lr 0.00017\n",
      "iteration 217300 training loss 0.8917848 lr 0.00017\n",
      "iteration 217400 training loss 0.6977697 lr 0.00017\n",
      "iteration 217500 training loss 0.6552093 lr 0.00017\n",
      "iteration 217600 training loss 0.8891598 lr 0.00017\n",
      "iteration 217700 training loss 0.7036584 lr 0.00017\n",
      "iteration 217800 training loss 0.8215465 lr 0.00017\n",
      "iteration 217900 training loss 0.66171086 lr 0.00017\n",
      "iteration 218000 training loss 0.65121996 lr 0.00017\n",
      "iteration 218100 training loss 0.79176104 lr 0.00017\n",
      "iteration 218200 training loss 0.7093683 lr 0.00017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 218300 training loss 0.8089887 lr 0.00017\n",
      "iteration 218400 training loss 0.6662826 lr 0.00017\n",
      "iteration 218500 training loss 0.63059735 lr 0.00017\n",
      "iteration 218600 training loss 0.913736 lr 0.00017\n",
      "iteration 218700 training loss 0.7592323 lr 0.00017\n",
      "iteration 218800 training loss 0.62728 lr 0.00017\n",
      "iteration 218900 training loss 0.6626117 lr 0.00017\n",
      "iteration 219000 training loss 0.7785107 lr 0.00017\n",
      "iteration 219100 training loss 0.9122048 lr 0.00017\n",
      "iteration 219200 training loss 0.5701523 lr 0.00017\n",
      "iteration 219300 training loss 0.79780865 lr 0.00017\n",
      "iteration 219400 training loss 0.9227277 lr 0.00017\n",
      "iteration 219500 training loss 0.8820451 lr 0.00017\n",
      "iteration 219600 training loss 0.7440319 lr 0.00017\n",
      "iteration 219700 training loss 0.6716883 lr 0.00017\n",
      "iteration 219800 training loss 0.7936947 lr 0.00017\n",
      "iteration 219900 training loss 0.8431261 lr 0.00017\n",
      "iteration 220000 training loss 0.7089483 lr 0.00017\n",
      "layout:nlp:random 0.8057465279785969\n",
      "layout:nlp:default 0.4220349442445038\n",
      "layout:xla:random 0.46847042832944136\n",
      "layout:xla:default 0.20503319198902492\n",
      "epoch 0, it 220000 validation loss -0.475\n",
      "iteration 220100 training loss 0.75449187 lr 0.00016\n",
      "iteration 220200 training loss 0.48569945 lr 0.00016\n",
      "iteration 220300 training loss 1.0329393 lr 0.00016\n",
      "iteration 220400 training loss 0.90211105 lr 0.00016\n",
      "iteration 220500 training loss 0.6139645 lr 0.00016\n",
      "iteration 220600 training loss 0.7975598 lr 0.00016\n",
      "iteration 220700 training loss 0.93064034 lr 0.00016\n",
      "iteration 220800 training loss 0.7353258 lr 0.00016\n",
      "iteration 220900 training loss 0.73677546 lr 0.00016\n",
      "iteration 221000 training loss 0.91307557 lr 0.00016\n",
      "iteration 221100 training loss 0.726373 lr 0.00016\n",
      "iteration 221200 training loss 0.58680797 lr 0.00016\n",
      "iteration 221300 training loss 0.66136825 lr 0.00016\n",
      "iteration 221400 training loss 0.70851135 lr 0.00016\n",
      "iteration 221500 training loss 0.74947333 lr 0.00016\n",
      "iteration 221600 training loss 0.7648452 lr 0.00016\n",
      "iteration 221700 training loss 0.7084312 lr 0.00016\n",
      "iteration 221800 training loss 0.8831038 lr 0.00016\n",
      "iteration 221900 training loss 0.6604127 lr 0.00016\n",
      "iteration 222000 training loss 0.6215554 lr 0.00016\n",
      "iteration 222100 training loss 0.63652986 lr 0.00016\n",
      "iteration 222200 training loss 0.7116508 lr 0.00016\n",
      "iteration 222300 training loss 0.80774057 lr 0.00016\n",
      "iteration 222400 training loss 0.5986599 lr 0.00016\n",
      "iteration 222500 training loss 0.8006941 lr 0.00016\n",
      "iteration 222600 training loss 0.62930256 lr 0.00016\n",
      "iteration 222700 training loss 0.7489646 lr 0.00016\n",
      "iteration 222800 training loss 0.65680027 lr 0.00016\n",
      "iteration 222900 training loss 0.74689215 lr 0.00016\n",
      "iteration 223000 training loss 0.53916496 lr 0.00016\n",
      "iteration 223100 training loss 0.60997796 lr 0.00016\n",
      "iteration 223200 training loss 0.6513344 lr 0.00016\n",
      "iteration 223300 training loss 0.7924325 lr 0.00016\n",
      "iteration 223400 training loss 0.85193735 lr 0.00016\n",
      "iteration 223500 training loss 0.6053279 lr 0.00016\n",
      "iteration 223600 training loss 0.7217554 lr 0.00016\n",
      "iteration 223700 training loss 0.6803438 lr 0.00016\n",
      "iteration 223800 training loss 0.6588541 lr 0.00016\n",
      "iteration 223900 training loss 0.7019665 lr 0.00016\n",
      "iteration 224000 training loss 0.65808344 lr 0.00016\n",
      "iteration 224100 training loss 0.5642426 lr 0.00016\n",
      "iteration 224200 training loss 0.72541183 lr 0.00016\n",
      "iteration 224300 training loss 0.68599963 lr 0.00016\n",
      "iteration 224400 training loss 0.55212307 lr 0.00016\n",
      "iteration 224500 training loss 0.87049973 lr 0.00016\n",
      "iteration 224600 training loss 0.7617917 lr 0.00016\n",
      "iteration 224700 training loss 0.7552345 lr 0.00016\n",
      "iteration 224800 training loss 0.67096156 lr 0.00016\n",
      "iteration 224900 training loss 0.8440642 lr 0.00016\n",
      "iteration 225000 training loss 0.69551057 lr 0.00016\n",
      "iteration 225100 training loss 0.7306393 lr 0.00016\n",
      "iteration 225200 training loss 0.7470202 lr 0.00016\n",
      "iteration 225300 training loss 0.681648 lr 0.00016\n",
      "iteration 225400 training loss 0.9010113 lr 0.00016\n",
      "iteration 225500 training loss 0.9523105 lr 0.00016\n",
      "iteration 225600 training loss 0.8439907 lr 0.00016\n",
      "iteration 225700 training loss 0.6547537 lr 0.00016\n",
      "iteration 225800 training loss 0.9635784 lr 0.00016\n",
      "iteration 225900 training loss 0.6934494 lr 0.00016\n",
      "iteration 226000 training loss 0.78487283 lr 0.00016\n",
      "iteration 226100 training loss 0.5990806 lr 0.00016\n",
      "iteration 226200 training loss 0.79574895 lr 0.00016\n",
      "iteration 226300 training loss 0.7400354 lr 0.00016\n",
      "iteration 226400 training loss 0.6688627 lr 0.00016\n",
      "iteration 226500 training loss 0.6990273 lr 0.00016\n",
      "iteration 226600 training loss 0.7209692 lr 0.00016\n",
      "iteration 226700 training loss 0.75312585 lr 0.00016\n",
      "iteration 226800 training loss 0.8324293 lr 0.00016\n",
      "iteration 226900 training loss 0.64304006 lr 0.00016\n",
      "iteration 227000 training loss 0.63045466 lr 0.00016\n",
      "iteration 227100 training loss 0.7609673 lr 0.00016\n",
      "iteration 227200 training loss 0.58794796 lr 0.00016\n",
      "iteration 227300 training loss 0.66778785 lr 0.00016\n",
      "iteration 227400 training loss 0.70616996 lr 0.00016\n",
      "iteration 227500 training loss 0.76430434 lr 0.00016\n",
      "iteration 227600 training loss 0.9253241 lr 0.00016\n",
      "iteration 227700 training loss 0.6786702 lr 0.00016\n",
      "iteration 227800 training loss 0.734672 lr 0.00016\n",
      "iteration 227900 training loss 0.6599833 lr 0.00016\n",
      "iteration 228000 training loss 0.74982005 lr 0.00016\n",
      "iteration 228100 training loss 0.8899542 lr 0.00016\n",
      "iteration 228200 training loss 0.8380243 lr 0.00016\n",
      "iteration 228300 training loss 0.7312797 lr 0.00016\n",
      "iteration 228400 training loss 0.73881906 lr 0.00016\n",
      "iteration 228500 training loss 0.56353587 lr 0.00016\n",
      "iteration 228600 training loss 0.66689616 lr 0.00016\n",
      "iteration 228700 training loss 0.6582095 lr 0.00016\n",
      "iteration 228800 training loss 0.6054053 lr 0.00016\n",
      "iteration 228900 training loss 0.5706419 lr 0.00016\n",
      "iteration 229000 training loss 0.6685392 lr 0.00016\n",
      "iteration 229100 training loss 0.7568916 lr 0.00016\n",
      "iteration 229200 training loss 0.64711344 lr 0.00016\n",
      "iteration 229300 training loss 0.6826734 lr 0.00016\n",
      "iteration 229400 training loss 0.7160986 lr 0.00016\n",
      "iteration 229500 training loss 0.6280172 lr 0.00016\n",
      "iteration 229600 training loss 0.5375128 lr 0.00016\n",
      "iteration 229700 training loss 0.7621921 lr 0.00016\n",
      "iteration 229800 training loss 0.87309283 lr 0.00016\n",
      "iteration 229900 training loss 0.7741784 lr 0.00016\n",
      "iteration 230000 training loss 0.5836958 lr 0.00016\n",
      "iteration 230100 training loss 0.61148465 lr 0.00016\n",
      "iteration 230200 training loss 0.7967557 lr 0.00016\n",
      "iteration 230300 training loss 0.5205363 lr 0.00016\n",
      "iteration 230400 training loss 0.66120166 lr 0.00016\n",
      "iteration 230500 training loss 0.68503714 lr 0.00016\n",
      "iteration 230600 training loss 0.7340415 lr 0.00016\n",
      "iteration 230700 training loss 0.7164139 lr 0.00016\n",
      "iteration 230800 training loss 0.6470681 lr 0.00016\n",
      "iteration 230900 training loss 0.73479915 lr 0.00016\n",
      "iteration 231000 training loss 0.6293603 lr 0.00016\n",
      "iteration 231100 training loss 0.75967866 lr 0.00016\n",
      "iteration 231200 training loss 0.4071177 lr 0.00016\n",
      "iteration 231300 training loss 0.60469574 lr 0.00016\n",
      "iteration 231400 training loss 0.51978135 lr 0.00016\n",
      "iteration 231500 training loss 0.70054084 lr 0.00016\n",
      "iteration 231600 training loss 0.8215078 lr 0.00016\n",
      "iteration 231700 training loss 0.90255475 lr 0.00016\n",
      "iteration 231800 training loss 0.9962237 lr 0.00016\n",
      "iteration 231900 training loss 0.7147689 lr 0.00016\n",
      "iteration 232000 training loss 0.7223296 lr 0.00016\n",
      "iteration 232100 training loss 0.6540627 lr 0.00016\n",
      "iteration 232200 training loss 0.69120866 lr 0.00016\n",
      "iteration 232300 training loss 0.87214583 lr 0.00016\n",
      "iteration 232400 training loss 0.82740635 lr 0.00016\n",
      "iteration 232500 training loss 0.72768617 lr 0.00016\n",
      "iteration 232600 training loss 0.78669673 lr 0.00016\n",
      "iteration 232700 training loss 0.82896394 lr 0.00016\n",
      "iteration 232800 training loss 0.7864939 lr 0.00016\n",
      "iteration 232900 training loss 0.6716713 lr 0.00016\n",
      "iteration 233000 training loss 1.0379766 lr 0.00016\n",
      "iteration 233100 training loss 0.64931035 lr 0.00016\n",
      "iteration 233200 training loss 0.7895497 lr 0.00016\n",
      "iteration 233300 training loss 0.9339344 lr 0.00016\n",
      "iteration 233400 training loss 0.73336303 lr 0.00016\n",
      "iteration 233500 training loss 0.8321867 lr 0.00016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 233600 training loss 0.87733567 lr 0.00016\n",
      "iteration 233700 training loss 0.8411595 lr 0.00016\n",
      "iteration 233800 training loss 0.71897084 lr 0.00016\n",
      "iteration 233900 training loss 0.7604808 lr 0.00016\n",
      "iteration 234000 training loss 0.7871445 lr 0.00016\n",
      "iteration 234100 training loss 0.8073729 lr 0.00016\n",
      "iteration 234200 training loss 0.7779092 lr 0.00016\n",
      "iteration 234300 training loss 0.7475126 lr 0.00016\n",
      "iteration 234400 training loss 0.71258193 lr 0.00016\n",
      "iteration 234500 training loss 0.7334283 lr 0.00016\n",
      "iteration 234600 training loss 0.75366384 lr 0.00016\n",
      "iteration 234700 training loss 0.6109296 lr 0.00016\n",
      "iteration 234800 training loss 0.9141864 lr 0.00016\n",
      "iteration 234900 training loss 0.95340866 lr 0.00016\n",
      "iteration 235000 training loss 0.890307 lr 0.00016\n",
      "iteration 235100 training loss 0.9732686 lr 0.00016\n",
      "iteration 235200 training loss 0.6200659 lr 0.00016\n",
      "iteration 235300 training loss 0.9304271 lr 0.00016\n",
      "iteration 235400 training loss 0.6624161 lr 0.00016\n",
      "iteration 235500 training loss 0.64075136 lr 0.00016\n",
      "iteration 235600 training loss 0.77240103 lr 0.00016\n",
      "iteration 235700 training loss 0.7317308 lr 0.00016\n",
      "iteration 235800 training loss 0.76586944 lr 0.00016\n",
      "iteration 235900 training loss 0.6519182 lr 0.00016\n",
      "iteration 236000 training loss 0.6088367 lr 0.00016\n",
      "iteration 236100 training loss 0.72654676 lr 0.00016\n",
      "iteration 236200 training loss 0.79605424 lr 0.00016\n",
      "iteration 236300 training loss 0.552098 lr 0.00016\n",
      "iteration 236400 training loss 0.6877479 lr 0.00016\n",
      "iteration 236500 training loss 0.6109016 lr 0.00016\n",
      "iteration 236600 training loss 0.6839268 lr 0.00016\n",
      "iteration 236700 training loss 0.6882831 lr 0.00016\n",
      "iteration 236800 training loss 0.6572832 lr 0.00016\n",
      "iteration 236900 training loss 0.7508748 lr 0.00016\n",
      "iteration 237000 training loss 0.8445709 lr 0.00016\n",
      "iteration 237100 training loss 0.76555425 lr 0.00016\n",
      "iteration 237200 training loss 0.7150128 lr 0.00016\n",
      "iteration 237300 training loss 0.9259845 lr 0.00016\n",
      "iteration 237400 training loss 0.86091214 lr 0.00016\n",
      "iteration 237500 training loss 0.7269029 lr 0.00016\n",
      "iteration 237600 training loss 0.7737541 lr 0.00016\n",
      "iteration 237700 training loss 0.6079785 lr 0.00016\n",
      "iteration 237800 training loss 0.82573396 lr 0.00016\n",
      "iteration 237900 training loss 0.7175069 lr 0.00016\n",
      "iteration 238000 training loss 0.65096945 lr 0.00016\n",
      "iteration 238100 training loss 0.8145103 lr 0.00016\n",
      "iteration 238200 training loss 0.7101254 lr 0.00016\n",
      "iteration 238300 training loss 0.7189001 lr 0.00016\n",
      "iteration 238400 training loss 0.6885818 lr 0.00016\n",
      "iteration 238500 training loss 0.59638786 lr 0.00016\n",
      "iteration 238600 training loss 0.8433196 lr 0.00016\n",
      "iteration 238700 training loss 0.5696818 lr 0.00016\n",
      "iteration 238800 training loss 0.5405664 lr 0.00016\n",
      "iteration 238900 training loss 0.45440558 lr 0.00016\n",
      "iteration 239000 training loss 0.80423546 lr 0.00016\n",
      "iteration 239100 training loss 0.94430566 lr 0.00016\n",
      "iteration 239200 training loss 0.6882243 lr 0.00016\n",
      "iteration 239300 training loss 0.60054636 lr 0.00016\n",
      "iteration 239400 training loss 0.69790435 lr 0.00016\n",
      "iteration 239500 training loss 0.68179536 lr 0.00016\n",
      "iteration 239600 training loss 0.8065493 lr 0.00016\n",
      "iteration 239700 training loss 0.68483865 lr 0.00016\n",
      "iteration 239800 training loss 0.8109874 lr 0.00016\n",
      "iteration 239900 training loss 0.8713214 lr 0.00016\n",
      "iteration 240000 training loss 0.64240795 lr 0.00016\n",
      "layout:nlp:random 0.8003769730898675\n",
      "layout:nlp:default 0.41770384273444205\n",
      "layout:xla:random 0.5155566878186538\n",
      "layout:xla:default 0.17845418865817972\n",
      "epoch 0, it 240000 validation loss -0.478\n",
      "iteration 240100 training loss 0.6824546 lr 0.00014\n",
      "iteration 240200 training loss 0.546028 lr 0.00014\n",
      "iteration 240300 training loss 0.9002456 lr 0.00014\n",
      "iteration 240400 training loss 0.8419638 lr 0.00014\n",
      "iteration 240500 training loss 0.7217946 lr 0.00014\n",
      "iteration 240600 training loss 0.6383001 lr 0.00014\n",
      "iteration 240700 training loss 0.6504208 lr 0.00014\n",
      "iteration 240800 training loss 0.8154385 lr 0.00014\n",
      "iteration 240900 training loss 0.7094442 lr 0.00014\n",
      "iteration 241000 training loss 0.7591836 lr 0.00014\n",
      "iteration 241100 training loss 0.5701915 lr 0.00014\n",
      "iteration 241200 training loss 0.67938995 lr 0.00014\n",
      "iteration 241300 training loss 0.796205 lr 0.00014\n",
      "iteration 241400 training loss 0.6257832 lr 0.00014\n",
      "iteration 241500 training loss 0.68976414 lr 0.00014\n",
      "iteration 241600 training loss 0.8384257 lr 0.00014\n",
      "iteration 241700 training loss 0.7487352 lr 0.00014\n",
      "iteration 241800 training loss 0.7472351 lr 0.00014\n",
      "iteration 241900 training loss 0.8753074 lr 0.00014\n",
      "iteration 242000 training loss 1.0589161 lr 0.00014\n",
      "iteration 242100 training loss 1.0065825 lr 0.00014\n",
      "iteration 242200 training loss 0.93592805 lr 0.00014\n",
      "iteration 242300 training loss 0.83113027 lr 0.00014\n",
      "iteration 242400 training loss 0.8079397 lr 0.00014\n",
      "iteration 242500 training loss 0.6732063 lr 0.00014\n",
      "iteration 242600 training loss 0.68293774 lr 0.00014\n",
      "iteration 242700 training loss 0.76678073 lr 0.00014\n",
      "iteration 242800 training loss 0.93498725 lr 0.00014\n",
      "iteration 242900 training loss 0.6758719 lr 0.00014\n",
      "iteration 243000 training loss 0.7592136 lr 0.00014\n",
      "iteration 243100 training loss 0.7795261 lr 0.00014\n",
      "iteration 243200 training loss 0.69587433 lr 0.00014\n",
      "iteration 243300 training loss 0.7383216 lr 0.00014\n",
      "iteration 243400 training loss 0.7171041 lr 0.00014\n",
      "iteration 243500 training loss 0.69272375 lr 0.00014\n",
      "iteration 243600 training loss 0.694212 lr 0.00014\n",
      "iteration 243700 training loss 0.85041726 lr 0.00014\n",
      "iteration 243800 training loss 0.6553528 lr 0.00014\n",
      "iteration 243900 training loss 0.7515026 lr 0.00014\n",
      "iteration 244000 training loss 0.5372547 lr 0.00014\n",
      "iteration 244100 training loss 0.70891935 lr 0.00014\n",
      "iteration 244200 training loss 0.5766685 lr 0.00014\n",
      "iteration 244300 training loss 0.59091973 lr 0.00014\n",
      "iteration 244400 training loss 0.69435096 lr 0.00014\n",
      "iteration 244500 training loss 0.6941219 lr 0.00014\n",
      "iteration 244600 training loss 0.69390935 lr 0.00014\n",
      "iteration 244700 training loss 0.6150488 lr 0.00014\n",
      "iteration 244800 training loss 0.6389435 lr 0.00014\n",
      "iteration 244900 training loss 0.73273575 lr 0.00014\n",
      "iteration 245000 training loss 0.44020188 lr 0.00014\n",
      "iteration 245100 training loss 0.6981411 lr 0.00014\n",
      "iteration 245200 training loss 0.6087257 lr 0.00014\n",
      "iteration 245300 training loss 0.64760226 lr 0.00014\n",
      "iteration 245400 training loss 0.46582547 lr 0.00014\n",
      "iteration 245500 training loss 0.7064721 lr 0.00014\n",
      "iteration 245600 training loss 0.66748935 lr 0.00014\n",
      "iteration 245700 training loss 0.9470287 lr 0.00014\n",
      "iteration 245800 training loss 0.78287077 lr 0.00014\n",
      "iteration 245900 training loss 0.5117366 lr 0.00014\n",
      "iteration 246000 training loss 0.5771662 lr 0.00014\n",
      "iteration 246100 training loss 0.5526402 lr 0.00014\n",
      "iteration 246200 training loss 0.7472938 lr 0.00014\n",
      "iteration 246300 training loss 0.738351 lr 0.00014\n",
      "iteration 246400 training loss 0.6979967 lr 0.00014\n",
      "iteration 246500 training loss 0.64798063 lr 0.00014\n",
      "iteration 246600 training loss 0.5792096 lr 0.00014\n",
      "iteration 246700 training loss 0.83016413 lr 0.00014\n",
      "iteration 246800 training loss 0.84782803 lr 0.00014\n",
      "iteration 246900 training loss 0.5936064 lr 0.00014\n",
      "iteration 247000 training loss 0.80745655 lr 0.00014\n",
      "iteration 247100 training loss 0.68912166 lr 0.00014\n",
      "iteration 247200 training loss 0.79590243 lr 0.00014\n",
      "iteration 247300 training loss 0.7061697 lr 0.00014\n",
      "iteration 247400 training loss 0.74635136 lr 0.00014\n",
      "iteration 247500 training loss 0.76399744 lr 0.00014\n",
      "iteration 247600 training loss 0.91893065 lr 0.00014\n",
      "iteration 247700 training loss 0.6757705 lr 0.00014\n",
      "iteration 247800 training loss 0.6289761 lr 0.00014\n",
      "iteration 247900 training loss 0.54201186 lr 0.00014\n",
      "iteration 248000 training loss 0.6779514 lr 0.00014\n",
      "iteration 248100 training loss 0.8235638 lr 0.00014\n",
      "iteration 248200 training loss 0.6618185 lr 0.00014\n",
      "iteration 248300 training loss 0.6477973 lr 0.00014\n",
      "iteration 248400 training loss 0.64523584 lr 0.00014\n",
      "iteration 248500 training loss 0.6171707 lr 0.00014\n",
      "iteration 248600 training loss 0.8069821 lr 0.00014\n",
      "iteration 248700 training loss 0.72513086 lr 0.00014\n",
      "iteration 248800 training loss 0.8238993 lr 0.00014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 248900 training loss 0.8089574 lr 0.00014\n",
      "iteration 249000 training loss 0.92060924 lr 0.00014\n",
      "iteration 249100 training loss 0.81604666 lr 0.00014\n",
      "iteration 249200 training loss 0.62410855 lr 0.00014\n",
      "iteration 249300 training loss 0.8051528 lr 0.00014\n",
      "iteration 249400 training loss 0.69037384 lr 0.00014\n",
      "iteration 249500 training loss 0.59723467 lr 0.00014\n",
      "iteration 249600 training loss 0.83230644 lr 0.00014\n",
      "iteration 249700 training loss 0.83822906 lr 0.00014\n",
      "iteration 249800 training loss 0.84956884 lr 0.00014\n",
      "iteration 249900 training loss 0.7919425 lr 0.00014\n",
      "iteration 250000 training loss 0.84338915 lr 0.00014\n",
      "iteration 250100 training loss 0.8047674 lr 0.00014\n",
      "iteration 250200 training loss 0.95234364 lr 0.00014\n",
      "iteration 250300 training loss 0.7289329 lr 0.00014\n",
      "iteration 250400 training loss 0.7396436 lr 0.00014\n",
      "iteration 250500 training loss 0.6110736 lr 0.00014\n",
      "iteration 250600 training loss 0.9141866 lr 0.00014\n",
      "iteration 250700 training loss 0.97714716 lr 0.00014\n",
      "iteration 250800 training loss 0.7411919 lr 0.00014\n",
      "iteration 250900 training loss 0.8518703 lr 0.00014\n",
      "iteration 251000 training loss 0.795004 lr 0.00014\n",
      "iteration 251100 training loss 0.89045113 lr 0.00014\n",
      "iteration 251200 training loss 0.69656104 lr 0.00014\n",
      "iteration 251300 training loss 0.66360885 lr 0.00014\n",
      "iteration 251400 training loss 0.781777 lr 0.00014\n",
      "iteration 251500 training loss 0.6905289 lr 0.00014\n",
      "iteration 251600 training loss 0.6904701 lr 0.00014\n",
      "iteration 251700 training loss 0.79666984 lr 0.00014\n",
      "iteration 251800 training loss 0.65244013 lr 0.00014\n",
      "iteration 251900 training loss 0.6239059 lr 0.00014\n",
      "iteration 252000 training loss 0.7290069 lr 0.00014\n",
      "iteration 252100 training loss 0.55715984 lr 0.00014\n",
      "iteration 252200 training loss 0.69283956 lr 0.00014\n",
      "iteration 252300 training loss 0.8673873 lr 0.00014\n",
      "iteration 252400 training loss 0.74689 lr 0.00014\n",
      "iteration 252500 training loss 0.898943 lr 0.00014\n",
      "iteration 252600 training loss 0.88870335 lr 0.00014\n",
      "iteration 252700 training loss 0.8050482 lr 0.00014\n",
      "iteration 252800 training loss 0.9193604 lr 0.00014\n",
      "iteration 252900 training loss 0.83661026 lr 0.00014\n",
      "iteration 253000 training loss 0.7479333 lr 0.00014\n",
      "iteration 253100 training loss 0.5901801 lr 0.00014\n",
      "iteration 253200 training loss 0.6155849 lr 0.00014\n",
      "iteration 253300 training loss 0.6089475 lr 0.00014\n",
      "iteration 253400 training loss 0.7512504 lr 0.00014\n",
      "iteration 253500 training loss 0.59921885 lr 0.00014\n",
      "iteration 253600 training loss 0.5842601 lr 0.00014\n",
      "iteration 253700 training loss 0.90179837 lr 0.00014\n",
      "iteration 253800 training loss 0.82555693 lr 0.00014\n",
      "iteration 253900 training loss 0.6403787 lr 0.00014\n",
      "iteration 254000 training loss 0.67184937 lr 0.00014\n",
      "iteration 254100 training loss 0.75145864 lr 0.00014\n",
      "iteration 254200 training loss 0.6501178 lr 0.00014\n",
      "iteration 254300 training loss 0.6574358 lr 0.00014\n",
      "iteration 254400 training loss 0.67158616 lr 0.00014\n",
      "iteration 254500 training loss 0.78145003 lr 0.00014\n",
      "iteration 254600 training loss 0.7483396 lr 0.00014\n",
      "iteration 254700 training loss 0.8518552 lr 0.00014\n",
      "iteration 254800 training loss 0.7022026 lr 0.00014\n",
      "iteration 254900 training loss 0.69823223 lr 0.00014\n",
      "iteration 255000 training loss 0.67938197 lr 0.00014\n",
      "iteration 255100 training loss 0.73885685 lr 0.00014\n",
      "iteration 255200 training loss 0.6239416 lr 0.00014\n",
      "iteration 255300 training loss 0.71273065 lr 0.00014\n",
      "iteration 255400 training loss 0.69049156 lr 0.00014\n",
      "iteration 255500 training loss 0.74964345 lr 0.00014\n",
      "iteration 255600 training loss 0.868523 lr 0.00014\n",
      "iteration 255700 training loss 0.6126606 lr 0.00014\n",
      "iteration 255800 training loss 0.6858189 lr 0.00014\n",
      "iteration 255900 training loss 0.7123161 lr 0.00014\n",
      "iteration 256000 training loss 0.8317088 lr 0.00014\n",
      "iteration 256100 training loss 0.75801164 lr 0.00014\n",
      "iteration 256200 training loss 0.7245036 lr 0.00014\n",
      "iteration 256300 training loss 0.5949392 lr 0.00014\n",
      "iteration 256400 training loss 0.8015306 lr 0.00014\n",
      "iteration 256500 training loss 0.68044007 lr 0.00014\n",
      "iteration 256600 training loss 0.8510134 lr 0.00014\n",
      "iteration 256700 training loss 0.81460184 lr 0.00014\n",
      "iteration 256800 training loss 0.86501294 lr 0.00014\n",
      "iteration 256900 training loss 0.8005766 lr 0.00014\n",
      "iteration 257000 training loss 0.76127994 lr 0.00014\n",
      "iteration 257100 training loss 0.7843653 lr 0.00014\n",
      "iteration 257200 training loss 0.61419684 lr 0.00014\n",
      "iteration 257300 training loss 0.8088938 lr 0.00014\n",
      "iteration 257400 training loss 0.82156 lr 0.00014\n",
      "iteration 257500 training loss 0.65016437 lr 0.00014\n",
      "iteration 257600 training loss 0.6655501 lr 0.00014\n",
      "iteration 257700 training loss 0.83498174 lr 0.00014\n",
      "iteration 257800 training loss 0.73126125 lr 0.00014\n",
      "iteration 257900 training loss 0.8419999 lr 0.00014\n",
      "iteration 258000 training loss 0.6258374 lr 0.00014\n",
      "iteration 258100 training loss 0.79044837 lr 0.00014\n",
      "iteration 258200 training loss 0.6281767 lr 0.00014\n",
      "iteration 258300 training loss 0.570265 lr 0.00014\n",
      "iteration 258400 training loss 0.57427084 lr 0.00014\n",
      "iteration 258500 training loss 0.867632 lr 0.00014\n",
      "iteration 258600 training loss 0.74351865 lr 0.00014\n",
      "iteration 258700 training loss 0.78816056 lr 0.00014\n",
      "iteration 258800 training loss 0.6347322 lr 0.00014\n",
      "iteration 258900 training loss 0.56997114 lr 0.00014\n",
      "iteration 259000 training loss 0.7809352 lr 0.00014\n",
      "iteration 259100 training loss 0.69562846 lr 0.00014\n",
      "iteration 259200 training loss 0.6066701 lr 0.00014\n",
      "iteration 259300 training loss 0.7368707 lr 0.00014\n",
      "iteration 259400 training loss 0.52445817 lr 0.00014\n",
      "iteration 259500 training loss 0.6420922 lr 0.00014\n",
      "iteration 259600 training loss 0.6503368 lr 0.00014\n",
      "iteration 259700 training loss 0.43265322 lr 0.00014\n",
      "iteration 259800 training loss 0.58972377 lr 0.00014\n",
      "iteration 259900 training loss 0.6745673 lr 0.00014\n",
      "iteration 260000 training loss 0.6008147 lr 0.00014\n",
      "layout:nlp:random 0.7931307027658379\n",
      "layout:nlp:default 0.4195149957097747\n",
      "layout:xla:random 0.5022317098564125\n",
      "layout:xla:default 0.20284092824714278\n",
      "epoch 0, it 260000 validation loss -0.479\n",
      "iteration 260100 training loss 0.69870275 lr 0.00013\n",
      "iteration 260200 training loss 0.6654106 lr 0.00013\n",
      "iteration 260300 training loss 0.6135744 lr 0.00013\n",
      "iteration 260400 training loss 0.70362276 lr 0.00013\n",
      "iteration 260500 training loss 0.6399609 lr 0.00013\n",
      "iteration 260600 training loss 0.7516643 lr 0.00013\n",
      "iteration 260700 training loss 0.53200144 lr 0.00013\n",
      "iteration 260800 training loss 0.6774699 lr 0.00013\n",
      "iteration 260900 training loss 0.52803344 lr 0.00013\n",
      "iteration 261000 training loss 0.7977133 lr 0.00013\n",
      "iteration 261100 training loss 0.8392039 lr 0.00013\n",
      "iteration 261200 training loss 0.87377137 lr 0.00013\n",
      "iteration 261300 training loss 0.6271799 lr 0.00013\n",
      "iteration 261400 training loss 0.7230055 lr 0.00013\n",
      "iteration 261500 training loss 0.7734078 lr 0.00013\n",
      "iteration 261600 training loss 0.6209153 lr 0.00013\n",
      "iteration 261700 training loss 0.72277117 lr 0.00013\n",
      "iteration 261800 training loss 0.7039693 lr 0.00013\n",
      "iteration 261900 training loss 0.91826445 lr 0.00013\n",
      "iteration 262000 training loss 0.8450206 lr 0.00013\n",
      "iteration 262100 training loss 0.737288 lr 0.00013\n",
      "iteration 262200 training loss 0.63016075 lr 0.00013\n",
      "iteration 262300 training loss 0.65296936 lr 0.00013\n",
      "iteration 262400 training loss 0.7841488 lr 0.00013\n",
      "iteration 262500 training loss 0.7104809 lr 0.00013\n",
      "iteration 262600 training loss 0.7335109 lr 0.00013\n",
      "iteration 262700 training loss 0.74039495 lr 0.00013\n",
      "iteration 262800 training loss 0.73669034 lr 0.00013\n",
      "iteration 262900 training loss 0.76848197 lr 0.00013\n",
      "iteration 263000 training loss 0.858278 lr 0.00013\n",
      "iteration 263100 training loss 0.7805238 lr 0.00013\n",
      "iteration 263200 training loss 0.737831 lr 0.00013\n",
      "iteration 263300 training loss 0.75316185 lr 0.00013\n",
      "iteration 263400 training loss 0.65102524 lr 0.00013\n",
      "iteration 263500 training loss 0.814359 lr 0.00013\n",
      "iteration 263600 training loss 0.7409627 lr 0.00013\n",
      "iteration 263700 training loss 0.79807436 lr 0.00013\n",
      "iteration 263800 training loss 0.75850594 lr 0.00013\n",
      "iteration 263900 training loss 0.66334987 lr 0.00013\n",
      "iteration 264000 training loss 0.8750477 lr 0.00013\n",
      "iteration 264100 training loss 0.6978139 lr 0.00013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 264200 training loss 0.57917386 lr 0.00013\n",
      "iteration 264300 training loss 0.74230695 lr 0.00013\n",
      "iteration 264400 training loss 0.6348012 lr 0.00013\n",
      "iteration 264500 training loss 0.5810783 lr 0.00013\n",
      "iteration 264600 training loss 0.7010599 lr 0.00013\n",
      "iteration 264700 training loss 0.81075644 lr 0.00013\n",
      "iteration 264800 training loss 0.65579456 lr 0.00013\n",
      "iteration 264900 training loss 0.69204897 lr 0.00013\n",
      "iteration 265000 training loss 0.8046129 lr 0.00013\n",
      "iteration 265100 training loss 0.67653626 lr 0.00013\n",
      "iteration 265200 training loss 0.9429178 lr 0.00013\n",
      "iteration 265300 training loss 0.70135283 lr 0.00013\n",
      "iteration 265400 training loss 0.7563585 lr 0.00013\n",
      "iteration 265500 training loss 0.7350637 lr 0.00013\n",
      "iteration 265600 training loss 0.8977157 lr 0.00013\n",
      "iteration 265700 training loss 0.5680526 lr 0.00013\n",
      "iteration 265800 training loss 0.6035184 lr 0.00013\n",
      "iteration 265900 training loss 0.6541412 lr 0.00013\n",
      "iteration 266000 training loss 0.82511866 lr 0.00013\n",
      "iteration 266100 training loss 0.9891013 lr 0.00013\n",
      "iteration 266200 training loss 0.74826664 lr 0.00013\n",
      "iteration 266300 training loss 0.6663113 lr 0.00013\n",
      "iteration 266400 training loss 0.74684453 lr 0.00013\n",
      "iteration 266500 training loss 0.71909183 lr 0.00013\n",
      "iteration 266600 training loss 0.9608358 lr 0.00013\n",
      "iteration 266700 training loss 0.7168207 lr 0.00013\n",
      "iteration 266800 training loss 0.45167577 lr 0.00013\n",
      "iteration 266900 training loss 0.8579701 lr 0.00013\n",
      "iteration 267000 training loss 0.926407 lr 0.00013\n",
      "iteration 267100 training loss 0.7307664 lr 0.00013\n",
      "iteration 267200 training loss 0.857417 lr 0.00013\n",
      "iteration 267300 training loss 0.8243798 lr 0.00013\n",
      "iteration 267400 training loss 0.60067683 lr 0.00013\n",
      "iteration 267500 training loss 0.78126836 lr 0.00013\n",
      "iteration 267600 training loss 0.7404785 lr 0.00013\n",
      "iteration 267700 training loss 0.7319627 lr 0.00013\n",
      "iteration 267800 training loss 0.7656509 lr 0.00013\n",
      "iteration 267900 training loss 0.7129393 lr 0.00013\n",
      "iteration 268000 training loss 0.6252132 lr 0.00013\n",
      "iteration 268100 training loss 0.5667106 lr 0.00013\n",
      "iteration 268200 training loss 0.5394012 lr 0.00013\n",
      "iteration 268300 training loss 0.745283 lr 0.00013\n",
      "iteration 268400 training loss 0.52523696 lr 0.00013\n",
      "iteration 268500 training loss 0.664325 lr 0.00013\n",
      "iteration 268600 training loss 0.7316708 lr 0.00013\n",
      "iteration 268700 training loss 0.77211726 lr 0.00013\n",
      "iteration 268800 training loss 0.70609784 lr 0.00013\n",
      "iteration 268900 training loss 0.9121932 lr 0.00013\n",
      "iteration 269000 training loss 0.68664825 lr 0.00013\n",
      "iteration 269100 training loss 0.85563916 lr 0.00013\n",
      "iteration 269200 training loss 0.64473236 lr 0.00013\n",
      "iteration 269300 training loss 0.7157481 lr 0.00013\n",
      "iteration 269400 training loss 0.7078644 lr 0.00013\n",
      "iteration 269500 training loss 0.5740665 lr 0.00013\n",
      "iteration 269600 training loss 0.79494345 lr 0.00013\n",
      "iteration 269700 training loss 0.7690054 lr 0.00013\n",
      "iteration 269800 training loss 0.9061144 lr 0.00013\n",
      "iteration 269900 training loss 0.81501275 lr 0.00013\n",
      "iteration 270000 training loss 0.81927496 lr 0.00013\n",
      "iteration 270100 training loss 0.7737091 lr 0.00013\n",
      "iteration 270200 training loss 0.75270575 lr 0.00013\n",
      "iteration 270300 training loss 0.7553204 lr 0.00013\n",
      "iteration 270400 training loss 0.59476864 lr 0.00013\n",
      "iteration 270500 training loss 0.4843118 lr 0.00013\n",
      "iteration 270600 training loss 0.6868587 lr 0.00013\n",
      "iteration 270700 training loss 0.59818935 lr 0.00013\n",
      "iteration 270800 training loss 0.63227004 lr 0.00013\n",
      "iteration 270900 training loss 0.8855468 lr 0.00013\n",
      "iteration 271000 training loss 0.8646398 lr 0.00013\n",
      "iteration 271100 training loss 0.7885059 lr 0.00013\n",
      "iteration 271200 training loss 0.92696697 lr 0.00013\n",
      "iteration 271300 training loss 0.5710199 lr 0.00013\n",
      "iteration 271400 training loss 0.7514275 lr 0.00013\n",
      "iteration 271500 training loss 0.6030735 lr 0.00013\n",
      "iteration 271600 training loss 0.683931 lr 0.00013\n",
      "iteration 271700 training loss 0.71981823 lr 0.00013\n",
      "iteration 271800 training loss 0.53821474 lr 0.00013\n",
      "iteration 271900 training loss 0.73733616 lr 0.00013\n",
      "iteration 272000 training loss 0.6188585 lr 0.00013\n",
      "iteration 272100 training loss 0.5754554 lr 0.00013\n",
      "iteration 272200 training loss 0.67339194 lr 0.00013\n",
      "iteration 272300 training loss 0.5874331 lr 0.00013\n",
      "iteration 272400 training loss 0.6553858 lr 0.00013\n",
      "iteration 272500 training loss 0.6576946 lr 0.00013\n",
      "iteration 272600 training loss 0.6506895 lr 0.00013\n",
      "iteration 272700 training loss 0.7488769 lr 0.00013\n",
      "iteration 272800 training loss 0.776044 lr 0.00013\n",
      "iteration 272900 training loss 0.6579843 lr 0.00013\n",
      "iteration 273000 training loss 0.8020712 lr 0.00013\n",
      "iteration 273100 training loss 0.63998973 lr 0.00013\n",
      "iteration 273200 training loss 0.72153866 lr 0.00013\n",
      "iteration 273300 training loss 0.7923374 lr 0.00013\n",
      "iteration 273400 training loss 0.7039945 lr 0.00013\n",
      "iteration 273500 training loss 0.7253769 lr 0.00013\n",
      "iteration 273600 training loss 0.6875039 lr 0.00013\n",
      "iteration 273700 training loss 0.80806994 lr 0.00013\n",
      "iteration 273800 training loss 0.63440293 lr 0.00013\n",
      "iteration 273900 training loss 0.72463036 lr 0.00013\n",
      "iteration 274000 training loss 0.7451199 lr 0.00013\n",
      "iteration 274100 training loss 0.49980474 lr 0.00013\n",
      "iteration 274200 training loss 0.85990834 lr 0.00013\n",
      "iteration 274300 training loss 0.7148416 lr 0.00013\n",
      "iteration 274400 training loss 0.6152196 lr 0.00013\n",
      "iteration 274500 training loss 0.54193264 lr 0.00013\n",
      "iteration 274600 training loss 0.6936747 lr 0.00013\n",
      "iteration 274700 training loss 0.77412784 lr 0.00013\n",
      "iteration 274800 training loss 0.6345962 lr 0.00013\n",
      "iteration 274900 training loss 0.7373815 lr 0.00013\n",
      "iteration 275000 training loss 0.999271 lr 0.00013\n",
      "iteration 275100 training loss 0.43687662 lr 0.00013\n",
      "iteration 275200 training loss 0.78465277 lr 0.00013\n",
      "iteration 275300 training loss 0.6867782 lr 0.00013\n",
      "iteration 275400 training loss 0.7525826 lr 0.00013\n",
      "iteration 275500 training loss 0.6868013 lr 0.00013\n",
      "iteration 275600 training loss 0.91297454 lr 0.00013\n",
      "iteration 275700 training loss 0.67268753 lr 0.00013\n",
      "iteration 275800 training loss 0.7719429 lr 0.00013\n",
      "iteration 275900 training loss 0.86220896 lr 0.00013\n",
      "iteration 276000 training loss 0.8358958 lr 0.00013\n",
      "iteration 276100 training loss 0.7018691 lr 0.00013\n",
      "iteration 276200 training loss 0.6262516 lr 0.00013\n",
      "iteration 276300 training loss 0.8183698 lr 0.00013\n",
      "iteration 276400 training loss 0.88695836 lr 0.00013\n",
      "iteration 276500 training loss 0.83495176 lr 0.00013\n",
      "iteration 276600 training loss 0.83269936 lr 0.00013\n",
      "iteration 276700 training loss 0.84257203 lr 0.00013\n",
      "iteration 276800 training loss 0.7917201 lr 0.00013\n",
      "iteration 276900 training loss 0.6336547 lr 0.00013\n",
      "iteration 277000 training loss 0.7749221 lr 0.00013\n",
      "iteration 277100 training loss 0.8556193 lr 0.00013\n",
      "iteration 277200 training loss 0.60363716 lr 0.00013\n",
      "iteration 277300 training loss 0.70460415 lr 0.00013\n",
      "iteration 277400 training loss 0.7064262 lr 0.00013\n",
      "iteration 277500 training loss 0.82015556 lr 0.00013\n",
      "iteration 277600 training loss 0.743732 lr 0.00013\n",
      "iteration 277700 training loss 0.5923906 lr 0.00013\n",
      "iteration 277800 training loss 0.8152115 lr 0.00013\n",
      "iteration 277900 training loss 0.94724643 lr 0.00013\n",
      "iteration 278000 training loss 0.706389 lr 0.00013\n",
      "iteration 278100 training loss 0.7359457 lr 0.00013\n",
      "iteration 278200 training loss 0.78847235 lr 0.00013\n",
      "iteration 278300 training loss 0.67700994 lr 0.00013\n",
      "iteration 278400 training loss 0.6030191 lr 0.00013\n",
      "iteration 278500 training loss 0.7319901 lr 0.00013\n",
      "iteration 278600 training loss 0.76366943 lr 0.00013\n",
      "iteration 278700 training loss 0.6883401 lr 0.00013\n",
      "iteration 278800 training loss 0.6225562 lr 0.00013\n",
      "iteration 278900 training loss 0.67990077 lr 0.00013\n",
      "iteration 279000 training loss 0.94874877 lr 0.00013\n",
      "iteration 279100 training loss 0.7638956 lr 0.00013\n",
      "iteration 279200 training loss 0.6271347 lr 0.00013\n",
      "iteration 279300 training loss 0.7118199 lr 0.00013\n",
      "iteration 279400 training loss 0.60872155 lr 0.00013\n",
      "iteration 279500 training loss 0.70265704 lr 0.00013\n",
      "iteration 279600 training loss 0.4849401 lr 0.00013\n",
      "iteration 279700 training loss 0.79815096 lr 0.00013\n",
      "iteration 279800 training loss 0.7493158 lr 0.00013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 279900 training loss 0.64847845 lr 0.00013\n",
      "iteration 280000 training loss 0.77490443 lr 0.00013\n",
      "layout:nlp:random 0.8095316519096183\n",
      "layout:nlp:default 0.415086514681294\n",
      "layout:xla:random 0.4978366376847672\n",
      "layout:xla:default 0.1846724988505307\n",
      "epoch 0, it 280000 validation loss -0.477\n",
      "iteration 280100 training loss 0.78121775 lr 0.00011\n",
      "iteration 280200 training loss 0.74429315 lr 0.00011\n",
      "iteration 280300 training loss 0.6496787 lr 0.00011\n",
      "iteration 280400 training loss 0.64641994 lr 0.00011\n",
      "iteration 280500 training loss 0.68976647 lr 0.00011\n",
      "iteration 280600 training loss 0.83473754 lr 0.00011\n",
      "iteration 280700 training loss 0.81115645 lr 0.00011\n",
      "iteration 280800 training loss 0.7978167 lr 0.00011\n",
      "iteration 280900 training loss 0.7281265 lr 0.00011\n",
      "iteration 281000 training loss 0.75635296 lr 0.00011\n",
      "iteration 281100 training loss 0.70032305 lr 0.00011\n",
      "iteration 281200 training loss 0.75312346 lr 0.00011\n",
      "iteration 281300 training loss 0.76118153 lr 0.00011\n",
      "iteration 281400 training loss 0.5797811 lr 0.00011\n",
      "iteration 281500 training loss 0.84228563 lr 0.00011\n",
      "iteration 281600 training loss 0.65782875 lr 0.00011\n",
      "iteration 281700 training loss 0.6793938 lr 0.00011\n",
      "iteration 281800 training loss 0.6921003 lr 0.00011\n",
      "iteration 281900 training loss 0.71858865 lr 0.00011\n",
      "iteration 282000 training loss 0.7737385 lr 0.00011\n",
      "iteration 282100 training loss 0.7699359 lr 0.00011\n",
      "iteration 282200 training loss 0.8001836 lr 0.00011\n",
      "iteration 282300 training loss 0.8856262 lr 0.00011\n",
      "iteration 282400 training loss 0.6299309 lr 0.00011\n",
      "iteration 282500 training loss 0.6310481 lr 0.00011\n",
      "iteration 282600 training loss 0.91370296 lr 0.00011\n",
      "iteration 282700 training loss 0.8542817 lr 0.00011\n",
      "iteration 282800 training loss 0.7473032 lr 0.00011\n",
      "iteration 282900 training loss 0.82893413 lr 0.00011\n",
      "iteration 283000 training loss 0.6182035 lr 0.00011\n",
      "iteration 283100 training loss 0.6468487 lr 0.00011\n",
      "iteration 283200 training loss 0.70439655 lr 0.00011\n",
      "iteration 283300 training loss 0.7033698 lr 0.00011\n",
      "iteration 283400 training loss 0.8132603 lr 0.00011\n",
      "iteration 283500 training loss 0.7991773 lr 0.00011\n",
      "iteration 283600 training loss 0.84905076 lr 0.00011\n",
      "iteration 283700 training loss 0.66714007 lr 0.00011\n",
      "iteration 283800 training loss 0.6806329 lr 0.00011\n",
      "iteration 283900 training loss 0.8455069 lr 0.00011\n",
      "iteration 284000 training loss 0.7581444 lr 0.00011\n",
      "iteration 284100 training loss 0.658672 lr 0.00011\n",
      "iteration 284200 training loss 0.7300276 lr 0.00011\n",
      "iteration 284300 training loss 0.6547415 lr 0.00011\n",
      "iteration 284400 training loss 0.67530364 lr 0.00011\n",
      "iteration 284500 training loss 0.75510484 lr 0.00011\n",
      "iteration 284600 training loss 0.5211017 lr 0.00011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/kaggle_model_runtime/models.py:106\u001b[0m, in \u001b[0;36mMLP.train\u001b[0;34m(self, training_dataset, validation_dataset)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m training_dataset:\n\u001b[0;32m--> 106\u001b[0m         training_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m         iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;66;03m# TODO: use tensorboard -> tune learning rate\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.train(dataset.train_data, dataset.valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.normalization_layer_config_nodes.mean.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba29597",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mlp.dense_layer_3.kernel.numpy().flatten(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c005ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tile_ids, config_indexes, config_descriptors, valid_mask, graph_descriptor, normalized_runtimes in dataset.train_data:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tile_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ffd752",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_descriptors.numpy()[5, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mlp.normalization_layer_config_nodes(config_descriptors)\n",
    "x = mlp.dense_layer_1(x)\n",
    "x = mlp.relu_layer(x)  # (batch_size, n_config_nodes_upper_limit, n_units)\n",
    "\n",
    "float_mask = tf.cast(valid_mask, tf.float32)  # (batch_size, n_config_nodes_upper_limit)\n",
    "float_mask = tf.expand_dims(float_mask, axis=-1)\n",
    "x = x * float_mask\n",
    "x = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "normal_graph_descriptor = mlp.normalization_layer_graph_descriptor(graph_descriptor)\n",
    "x = tf.concat([x, normal_graph_descriptor], axis=-1)\n",
    "x = mlp.dense_layer_2(x)\n",
    "x = mlp.relu_layer(x)\n",
    "x = mlp.dense_layer_3(x)\n",
    "x = tf.reshape(x, (-1,))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ea294",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.random.permutation(np.arange(10))\n",
    "new_order = order.copy()\n",
    "new_order[0] = order[1]\n",
    "new_order[1] = order[0]\n",
    "kendalltau(order, new_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3418de",
   "metadata": {},
   "source": [
    "## Evaluate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac0e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = mlp.predict_over_dataset(dataset.valid_data, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043565f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>492038.000000</td>\n",
       "      <td>492038.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.118693</td>\n",
       "      <td>16.887459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.993702</td>\n",
       "      <td>2.029978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-167.809280</td>\n",
       "      <td>13.622271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-18.549092</td>\n",
       "      <td>15.501094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.116626</td>\n",
       "      <td>16.255013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.061701</td>\n",
       "      <td>17.911939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>101.065628</td>\n",
       "      <td>23.752274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          prediction         target\n",
       "count  492038.000000  492038.000000\n",
       "mean       -2.118693      16.887459\n",
       "std        54.993702       2.029978\n",
       "min      -167.809280      13.622271\n",
       "25%       -18.549092      15.501094\n",
       "50%         9.116626      16.255013\n",
       "75%        29.061701      17.911939\n",
       "max       101.065628      23.752274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[['prediction', 'target']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1d9033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(val_df['target'], bins=50)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(val_df['prediction'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11aff8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test',\n",
       "       b'layout:xla:default:unet_3d.4x4.bf16',\n",
       "       b'layout:nlp:random:albert_en_xlarge_batch_size_16_test',\n",
       "       b'layout:xla:random:resnet50.4x4.fp16',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test',\n",
       "       b'layout:xla:random:inception_v3_batch_128_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:xla:default:mlperf_bert_batch_24_2x2',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:nlp:default:albert_en_xlarge_batch_size_16_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train',\n",
       "       b'layout:xla:default:bert_pretraining.4x4.fp16',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train',\n",
       "       b'layout:nlp:default:talking-heads_large_batch_size_16_train',\n",
       "       b'layout:nlp:random:bert_en_cased_L-12_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train',\n",
       "       b'layout:xla:random:unet_3d.4x4.bf16',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test',\n",
       "       b'layout:xla:random:mlperf_bert_batch_24_2x2',\n",
       "       b'layout:xla:default:inception_v3_batch_128_train',\n",
       "       b'layout:xla:random:tf2_bert_pretrain_dynamic_batch_size',\n",
       "       b'layout:nlp:default:bert_en_cased_L-12_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:xla:random:resnet_v1_50_official_batch_128_bf16',\n",
       "       b'layout:nlp:random:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test',\n",
       "       b'layout:xla:default:resnet50.4x4.fp16',\n",
       "       b'layout:nlp:default:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train',\n",
       "       b'layout:xla:default:resnet_v1_50_official_batch_128_bf16',\n",
       "       b'layout:xla:random:bert_pretraining.4x4.fp16',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test',\n",
       "       b'layout:xla:default:tf2_bert_pretrain_dynamic_batch_size',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test',\n",
       "       b'layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train',\n",
       "       b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train',\n",
       "       b'layout:nlp:random:talking-heads_large_batch_size_16_train'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72249379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"b'layout:nlp:random:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test'\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_lin = np.linspace(15, 25, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "random_sample = val_df.sample(10_000)\n",
    "graph_id = np.random.choice(val_df['ID'].unique())\n",
    "random_sample = val_df[val_df['ID'] == graph_id]\n",
    "\n",
    "plt.scatter(\n",
    "    random_sample.target,\n",
    "    np.clip(random_sample.prediction, a_min=-10000.0, a_max=1000.0),\n",
    "    alpha=0.1,\n",
    "    #c=random_sample['ID'].apply(lambda x: x.decode('UTF-8').split(':')[1] == 'xla').values.astype(float)\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('prediction')\n",
    "plt.title(graph_id)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0639b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = val_df.sample(5_000)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.abs(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('abs error')\n",
    "x_lin = np.linspace(0, 0.7, 100)\n",
    "#plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.square(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('squared error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c359b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "layout:nlp:default:albert_en_xlarge_batch_size_16_test                               51243;38779;36787;48225;26190;4124;30988;6227;...\n",
       "layout:nlp:default:bert_en_cased_L-12_H-768_A-12_batch_size_16_test                  21836;952;74994;64836;18698;74993;64159;74987;...\n",
       "layout:nlp:default:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train              7170;7163;15580;7156;15831;15831;20081;20081;2...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test      14029;69717;69717;88324;88293;88034;58542;6216...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train     7777;44452;44452;30639;7735;41500;12989;50684;...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test      1538;1544;1544;1947;1563;54992;7664;1931;1900;...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train     25715;23244;26904;41464;41476;41455;41468;2561...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test      52830;35927;73088;44111;21869;50666;97223;5928...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train    23007;22990;23005;23023;597;601;23037;23042;23...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test     5726;59510;5945;5945;75013;83620;48917;84649;6...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train    20301;20301;20309;20348;20348;20352;20281;2033...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train      832;832;819;820;45476;45474;821;779;45439;5001...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train      10919;10919;60492;43500;84649;84680;47298;2397...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train      46586;10211;24560;15460;15460;46550;23663;4638...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train      35761;42948;14491;18456;14487;69605;14499;5211...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train      54521;54531;54546;54490;54610;54496;54475;3311...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test       26342;57529;28325;70939;39973;42483;16290;3819...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test      3689;39076;21098;42112;42135;84383;58939;77280...\n",
       "layout:nlp:default:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train     19005;35045;39821;30173;39808;39807;39807;3977...\n",
       "layout:nlp:default:talking-heads_large_batch_size_16_train                           5876;1529;1529;10066;10066;1169;1169;6616;1045...\n",
       "layout:nlp:random:albert_en_xlarge_batch_size_16_test                                51691;6548;30844;47184;33987;51459;46062;11812...\n",
       "layout:nlp:random:bert_en_cased_L-12_H-768_A-12_batch_size_16_test                   15170;15170;10900;30970;19653;39708;60782;6233...\n",
       "layout:nlp:random:bert_multi_cased_L-12_H-768_A-12_batch_size_16_train               1375;26124;5170;23022;10696;2354;2354;18351;18...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_32_test       30700;49309;40263;40466;54226;2344;45266;88262...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-128_A-2_batch_size_64_train      22113;9809;25743;42570;11709;37939;37939;33513...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_32_test       75171;87267;94824;54282;86918;13237;35748;6871...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-256_A-4_batch_size_64_train      8069;4810;23461;40813;12718;4120;32225;11934;1...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-512_A-8_batch_size_64_test       87954;83431;13310;97234;91890;69666;18679;8573...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_16_train     21913;548;19063;28839;27871;26600;18516;10613;...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-10_H-768_A-12_batch_size_32_test      57357;38181;38181;14166;35747;35747;1671;62904...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-12_H-768_A-12_batch_size_64_train     16003;3351;3351;2545;1179;6762;5334;22246;1348...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-2_H-256_A-4_batch_size_32_train       39169;71944;45729;74063;7870;39823;65819;57361...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-4_H-256_A-4_batch_size_32_train       59161;59161;75454;50289;72483;59262;78689;8533...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-4_H-512_A-8_batch_size_32_train       29847;34026;16682;2114;41490;15897;26802;50360...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train       34282;69519;23481;48507;67380;29032;13359;5143...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_64_train       53186;58984;42339;3798;33806;46996;15517;41359...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-512_A-8_batch_size_64_test        54164;34502;85339;78343;78343;27608;1301;53802...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_16_test       21411;61412;61412;41517;60080;82530;62945;4664...\n",
       "layout:nlp:random:small_bert_bert_en_uncased_L-6_H-768_A-12_batch_size_32_train      37991;37991;1247;20588;13324;33340;32870;16778...\n",
       "layout:nlp:random:talking-heads_large_batch_size_16_train                            4546;4546;10156;2511;4150;3057;6880;4695;4150;...\n",
       "layout:xla:default:bert_pretraining.4x4.fp16                                         6017;14748;14748;12930;12930;14744;14744;14729...\n",
       "layout:xla:default:inception_v3_batch_128_train                                      3082;5768;3355;4087;3043;1551;4092;4093;5763;5...\n",
       "layout:xla:default:mlperf_bert_batch_24_2x2                                          3228;3121;3143;3224;5691;3123;3114;575;3232;30...\n",
       "layout:xla:default:resnet50.4x4.fp16                                                 4942;4990;3453;5174;4002;4000;5075;3882;3842;1...\n",
       "layout:xla:default:resnet_v1_50_official_batch_128_bf16                              7221;6812;6814;193;1727;7648;43;7232;7217;4555...\n",
       "layout:xla:default:tf2_bert_pretrain_dynamic_batch_size                              13560;13560;10837;17459;3106;3115;3115;3116;31...\n",
       "layout:xla:default:unet_3d.4x4.bf16                                                  469;494;452;489;1617;1739;495;714;445;496;491;...\n",
       "layout:xla:random:bert_pretraining.4x4.fp16                                          11989;8070;18720;11903;6604;6604;6604;15642;86...\n",
       "layout:xla:random:inception_v3_batch_128_train                                       494;905;1989;4521;3737;2745;3335;1377;2082;248...\n",
       "layout:xla:random:mlperf_bert_batch_24_2x2                                           1033;2590;2023;5246;3686;3753;4908;6;136;1781;...\n",
       "layout:xla:random:resnet50.4x4.fp16                                                  3474;4129;1857;1769;397;668;5266;2588;112;657;...\n",
       "layout:xla:random:resnet_v1_50_official_batch_128_bf16                               1656;4381;5229;78;6526;4987;7061;7226;2007;419...\n",
       "layout:xla:random:tf2_bert_pretrain_dynamic_batch_size                               16118;13832;13832;16524;4156;13868;2968;2968;2...\n",
       "layout:xla:random:unet_3d.4x4.bf16                                                   400;478;397;162;417;183;160;460;345;277;290;11...\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_configs(df):\n",
    "    top = df.sort_values('prediction')\n",
    "    top = top['config_index'].values.tolist()\n",
    "    top = [str(i) for i in top]\n",
    "    return ';'.join(top)\n",
    "\n",
    "val_prediction = val_df.groupby('ID').apply(sort_configs)\n",
    "val_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb521f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3])).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_layout_score_group(df):\n",
    "    score, _ = kendalltau(df['prediction'], df['target'])\n",
    "    return score\n",
    "\n",
    "val_df['subset'] = val_df['ID'].map(lambda x: ':'.join(x.decode('UTF-8').split(':')[:3]))\n",
    "for subset in val_df['subset'].unique():\n",
    "    mean = np.mean(val_df[val_df['subset'] == subset].groupby('ID').apply(compute_layout_score_group))\n",
    "    print(subset, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16107638",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.368, 0.137, 0.738, 0.346, 0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_score(candidate_order, layout_dict):\n",
    "    runtimes = layout_dict['config_runtime']\n",
    "    best_ranking = np.argsort(runtimes)\n",
    "    assert len(candidate_order) == len(runtimes)\n",
    "    score, _ = kendalltau(candidate_order, best_ranking)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4289a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_order = np.argsort(layout_dict['config_runtime'])\n",
    "plt.scatter(true_order, candidate_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d439d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layout_set = 'valid'\n",
    "true_orders = []\n",
    "layout_ids = []\n",
    "for dirpath, dirnames, filenames in os.walk('predict-ai-model-runtime/npz_all/npz/layout'):\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "    \n",
    "    if dirpath.split('/')[-1] != layout_set:\n",
    "        continue\n",
    "        \n",
    "    layout_id_prefix = ':'.join(dirpath.split('/')[-4:-1])\n",
    "    for filename in os.listdir(dirpath):\n",
    "        print(filename)\n",
    "        layout_id = layout_id_prefix+':'+filename[:-4]\n",
    "        layout_dict = dict(np.load(os.path.join(dirpath, filename)))\n",
    "        runtimes = layout_dict['config_runtime']\n",
    "        best_ranking = np.argsort(runtimes)\n",
    "        best_ranking = ';'.join([str(i) for i in best_ranking])\n",
    "        true_orders.append(best_ranking)\n",
    "        layout_ids.append(layout_id)\n",
    "        \n",
    "true_order_df = pd.DataFrame(\n",
    "    data=np.stack([layout_ids, true_orders], axis=-1),\n",
    "    columns=['ID', 'true_order']\n",
    ")\n",
    "true_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6177a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layout_id = true_order_df.sample()['ID'].values[0]\n",
    "layout_id = 'layout:xla:default:resnet50.4x4.fp16'\n",
    "true_order = [int(i) for i in true_order_df[true_order_df['ID'] == layout_id]['true_order'].values[0].split(';')]\n",
    "candidate_order = [int(i) for i in val_prediction[layout_id].split(';')]\n",
    "\n",
    "plt.scatter(true_order, candidate_order)\n",
    "plt.xlabel('true order')\n",
    "plt.ylabel('candidate order')\n",
    "plt.title(f'{layout_id}, len {len(true_order)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d506a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_dict = dict(np.load('predict-ai-model-runtime/npz_all/npz/layout/nlp/default/valid/small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train.npz'))\n",
    "layout_dict['node_config_feat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[val_df['ID'] == b'layout:nlp:default:small_bert_bert_en_uncased_L-6_H-256_A-4_batch_size_16_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18339",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_layout['score'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434f868",
   "metadata": {},
   "source": [
    "## Inference over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac93738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>config_index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...</td>\n",
       "      <td>1</td>\n",
       "      <td>-109.468857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...</td>\n",
       "      <td>2</td>\n",
       "      <td>-101.939209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...</td>\n",
       "      <td>19</td>\n",
       "      <td>-109.803253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...</td>\n",
       "      <td>17</td>\n",
       "      <td>-117.019867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...</td>\n",
       "      <td>15</td>\n",
       "      <td>-110.746201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...</td>\n",
       "      <td>956</td>\n",
       "      <td>23.913906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...</td>\n",
       "      <td>936</td>\n",
       "      <td>25.535656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...</td>\n",
       "      <td>995</td>\n",
       "      <td>28.096973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...</td>\n",
       "      <td>971</td>\n",
       "      <td>20.557175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...</td>\n",
       "      <td>989</td>\n",
       "      <td>22.660225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50002 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ID  config_index  \\\n",
       "0      b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...             1   \n",
       "1      b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...             2   \n",
       "2      b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...            19   \n",
       "3      b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...            17   \n",
       "4      b'layout:xla:random:3e7156ac468dfb75cf5c9615e1...            15   \n",
       "...                                                  ...           ...   \n",
       "49997  b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...           956   \n",
       "49998  b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...           936   \n",
       "49999  b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...           995   \n",
       "50000  b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...           971   \n",
       "50001  b'layout:nlp:random:f6c146fc5cf10be4f3accbaca9...           989   \n",
       "\n",
       "       prediction  \n",
       "0     -109.468857  \n",
       "1     -101.939209  \n",
       "2     -109.803253  \n",
       "3     -117.019867  \n",
       "4     -110.746201  \n",
       "...           ...  \n",
       "49997   23.913906  \n",
       "49998   25.535656  \n",
       "49999   28.096973  \n",
       "50000   20.557175  \n",
       "50001   22.660225  \n",
       "\n",
       "[50002 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = mlp.predict_over_dataset(dataset.test_data, return_labels=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b21ba9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "layout:nlp:default:016ac66a44a906a695afd2228509046a    91;405;951;762;792;996;262;408;979;701;296;954...\n",
       "layout:nlp:default:171b0513d8874a427ccfa46d136fbadc    650;382;582;592;466;510;565;21;849;15;84;976;2...\n",
       "layout:nlp:default:23559853d9702baaaacbb0c83fd32266    214;142;48;350;664;825;637;830;204;930;539;447...\n",
       "layout:nlp:default:29886a50d55cfe77a9497bc906c76ce9    237;370;953;974;644;295;508;292;26;733;870;34;...\n",
       "layout:nlp:default:32531d07a084b319dce484f53a4cf3fc    772;692;935;483;543;161;313;327;226;133;790;63...\n",
       "layout:nlp:default:38524e2ff135ded55b5286407e7af6b7    136;997;524;383;736;917;35;406;114;129;378;589...\n",
       "layout:nlp:default:3a0c5517a87df8d82fd637b83298a3ba    518;202;222;492;466;308;293;947;774;585;506;53...\n",
       "layout:nlp:default:492c7a94d559aa4a88769142d2a68362    11;630;931;634;91;483;24;487;827;545;755;766;5...\n",
       "layout:nlp:default:58cc2e418c3a8a19b871e15964b534ad    499;903;300;844;751;429;444;61;642;328;583;780...\n",
       "layout:nlp:default:60880ed76de53f4d7a1b960b24f20f7d    602;349;140;825;619;341;34;236;798;756;325;146...\n",
       "layout:nlp:default:6c1101f6231f4d1722c3b9f6d1e25026    289;61;330;637;867;333;547;127;970;700;192;729...\n",
       "layout:nlp:default:7105451001e119f65b66570d170b94a8    923;3;17;517;390;530;720;488;834;258;961;962;7...\n",
       "layout:nlp:default:71b79ca6db513e7979c3702c595150c2    894;33;381;287;980;276;922;536;508;714;928;577...\n",
       "layout:nlp:default:7f6284ebe027b1e9a3850fc703858a59    766;354;659;601;466;371;168;999;328;69;895;596...\n",
       "layout:nlp:default:b2fdde3b72980907578648774101543e    316;86;685;265;332;896;283;102;225;697;861;777...\n",
       "layout:nlp:default:d15316c12eefdef1ba549eb433797f77    949;455;635;585;373;566;366;300;740;58;309;198...\n",
       "layout:nlp:default:f6c146fc5cf10be4f3accbaca9897311    47;808;697;842;884;621;109;147;709;509;972;611...\n",
       "layout:nlp:random:016ac66a44a906a695afd2228509046a     317;338;540;46;635;533;644;47;273;529;548;32;7...\n",
       "layout:nlp:random:171b0513d8874a427ccfa46d136fbadc     75;311;565;799;940;629;873;140;260;861;434;502...\n",
       "layout:nlp:random:23559853d9702baaaacbb0c83fd32266     214;204;924;703;350;377;215;51;297;624;126;619...\n",
       "layout:nlp:random:29886a50d55cfe77a9497bc906c76ce9     4;689;488;156;84;100;348;392;409;740;626;170;3...\n",
       "layout:nlp:random:32531d07a084b319dce484f53a4cf3fc     22;26;907;466;691;853;536;977;364;949;189;197;...\n",
       "layout:nlp:random:38524e2ff135ded55b5286407e7af6b7     482;874;604;162;693;934;268;669;922;809;926;41...\n",
       "layout:nlp:random:3a0c5517a87df8d82fd637b83298a3ba     242;615;654;405;859;897;783;765;19;49;729;314;...\n",
       "layout:nlp:random:492c7a94d559aa4a88769142d2a68362     720;543;812;656;802;126;331;798;111;941;219;71...\n",
       "layout:nlp:random:58cc2e418c3a8a19b871e15964b534ad     772;737;399;667;489;526;669;34;463;350;973;858...\n",
       "layout:nlp:random:60880ed76de53f4d7a1b960b24f20f7d     356;320;645;742;836;219;143;270;457;301;623;64...\n",
       "layout:nlp:random:6c1101f6231f4d1722c3b9f6d1e25026     138;444;142;649;519;589;166;362;20;6;222;24;26...\n",
       "layout:nlp:random:7105451001e119f65b66570d170b94a8     930;46;392;966;87;376;2;340;485;530;247;463;95...\n",
       "layout:nlp:random:71b79ca6db513e7979c3702c595150c2     104;887;418;741;655;107;213;425;408;60;994;781...\n",
       "layout:nlp:random:7f6284ebe027b1e9a3850fc703858a59     12;901;499;422;179;102;988;359;747;880;952;811...\n",
       "layout:nlp:random:b2fdde3b72980907578648774101543e     746;888;413;103;350;225;192;856;482;832;957;28...\n",
       "layout:nlp:random:d15316c12eefdef1ba549eb433797f77     140;825;912;901;425;717;511;58;131;979;925;663...\n",
       "layout:nlp:random:f6c146fc5cf10be4f3accbaca9897311     749;352;158;846;911;334;634;726;487;667;926;61...\n",
       "layout:xla:default:05ae41e26dd3c4c06390371a0423233c    807;730;874;96;498;925;323;345;212;28;298;358;...\n",
       "layout:xla:default:3e7156ac468dfb75cf5c9615e1e5887d    258;186;961;732;504;165;548;955;997;117;654;11...\n",
       "layout:xla:default:5335ed13823b0a518ee3c79ba4425f34    735;94;285;925;277;693;109;338;630;30;801;165;...\n",
       "layout:xla:default:937ee0eb0d5d6151b7b8252933b5c1c9    344;293;364;500;578;466;355;932;949;636;489;24...\n",
       "layout:xla:default:cd708819d3f5103afd6460b15e74eaf3    12;276;847;40;264;539;805;335;725;465;215;694;...\n",
       "layout:xla:default:db59a991b7c607634f13570d52ce885f    85;165;833;236;730;327;956;811;403;806;89;750;...\n",
       "layout:xla:default:e8a3a1401b5e79f66d7037e424f3b6df    579;711;79;926;974;110;908;976;441;245;252;730...\n",
       "layout:xla:default:fbaa8bb6a1aed9988281085c91065c05    10;529;526;330;564;299;322;482;386;754;428;746...\n",
       "layout:xla:random:05ae41e26dd3c4c06390371a0423233c     98;466;983;17;597;365;956;512;715;617;298;407;...\n",
       "layout:xla:random:3e7156ac468dfb75cf5c9615e1e5887d     29;385;690;336;65;317;838;981;387;542;904;512;...\n",
       "layout:xla:random:5335ed13823b0a518ee3c79ba4425f34     916;214;302;604;486;81;463;730;337;45;716;227;...\n",
       "layout:xla:random:937ee0eb0d5d6151b7b8252933b5c1c9     536;391;803;59;16;604;901;935;641;132;417;18;8...\n",
       "layout:xla:random:cd708819d3f5103afd6460b15e74eaf3     84;145;489;672;551;209;840;816;693;460;927;521...\n",
       "layout:xla:random:db59a991b7c607634f13570d52ce885f     98;124;920;903;164;663;548;224;927;104;669;543...\n",
       "layout:xla:random:e8a3a1401b5e79f66d7037e424f3b6df     635;508;690;423;568;696;946;513;669;59;136;920...\n",
       "layout:xla:random:fbaa8bb6a1aed9988281085c91065c05     214;738;67;53;611;190;176;875;246;377;341;556;...\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = test_df.groupby('ID').apply(sort_configs)\n",
    "test_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d09fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_prediction, columns=['TopConfigs']).to_csv('layout_none_test_prediction_10_08_01_31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(mlp.dense_layer_1.kernel.numpy().flatten()), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f73f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
