{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db851207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 01:15:06.055095: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 01:15:06.056974: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-11 01:15:06.083743: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-11 01:15:06.083768: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-11 01:15:06.083784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-11 01:15:06.088578: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-11 01:15:06.089290: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 01:15:06.702745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from dataset import TileDataset\n",
    "from models import TileMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dda8a3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "batch_per_file_size = 16\n",
    "dataset = TileDataset(\n",
    "    batch_size=batch_size,\n",
    "    batch_per_file_size=batch_per_file_size,\n",
    "    build_tfrecords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d32ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = TileMLP(\n",
    "    batch_size, \n",
    "    learning_rate=1e-3, \n",
    "    batch_per_file_size=batch_per_file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a121fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 training loss 2.850186 lr 0.00100\n",
      "iteration 1000 training loss 2.3755388 lr 0.00100\n",
      "iteration 1500 training loss 2.2705274 lr 0.00100\n",
      "iteration 2000 training loss 2.5989246 lr 0.00100\n",
      "iteration 2500 training loss 1.7902882 lr 0.00100\n",
      "iteration 3000 training loss 1.8702297 lr 0.00100\n",
      "iteration 3500 training loss 1.8695171 lr 0.00100\n",
      "iteration 4000 training loss 2.2868195 lr 0.00100\n",
      "iteration 4500 training loss 1.7866676 lr 0.00100\n",
      "iteration 5000 training loss 1.8613592 lr 0.00100\n",
      "iteration 5500 training loss 1.8637788 lr 0.00090\n",
      "iteration 6000 training loss 2.502059 lr 0.00090\n",
      "iteration 6500 training loss 2.045004 lr 0.00090\n",
      "iteration 7000 training loss 1.8974097 lr 0.00090\n",
      "iteration 7500 training loss 2.4369378 lr 0.00090\n",
      "iteration 8000 training loss 1.969621 lr 0.00090\n",
      "iteration 8500 training loss 2.1023715 lr 0.00090\n",
      "iteration 9000 training loss 2.0017438 lr 0.00090\n",
      "iteration 9500 training loss 1.7806907 lr 0.00090\n",
      "iteration 10000 training loss 2.3087435 lr 0.00090\n",
      "epoch 0, it 10000 validation loss -0.930\n",
      "iteration 10500 training loss 2.0374022 lr 0.00081\n",
      "iteration 11000 training loss 1.9327266 lr 0.00081\n",
      "iteration 11500 training loss 1.7255907 lr 0.00081\n",
      "iteration 12000 training loss 1.8960332 lr 0.00081\n",
      "iteration 12500 training loss 1.9966176 lr 0.00081\n",
      "iteration 13000 training loss 2.0337212 lr 0.00081\n",
      "iteration 13500 training loss 2.1468058 lr 0.00081\n",
      "iteration 14000 training loss 2.0352015 lr 0.00081\n",
      "iteration 14500 training loss 1.9961369 lr 0.00081\n",
      "iteration 15000 training loss 2.45164 lr 0.00081\n",
      "iteration 15500 training loss 1.9247015 lr 0.00073\n",
      "iteration 16000 training loss 2.0723224 lr 0.00073\n",
      "iteration 16500 training loss 2.2108915 lr 0.00073\n",
      "iteration 17000 training loss 1.8493899 lr 0.00073\n",
      "iteration 17500 training loss 1.9830567 lr 0.00073\n",
      "iteration 18000 training loss 2.4082193 lr 0.00073\n",
      "iteration 18500 training loss 1.8276981 lr 0.00073\n",
      "iteration 19000 training loss 1.9453213 lr 0.00073\n",
      "iteration 19500 training loss 2.2202234 lr 0.00073\n",
      "iteration 20000 training loss 1.8869143 lr 0.00073\n",
      "epoch 0, it 20000 validation loss -0.942\n",
      "iteration 20500 training loss 1.7573287 lr 0.00066\n",
      "iteration 21000 training loss 1.5705223 lr 0.00066\n",
      "iteration 21500 training loss 1.9253048 lr 0.00066\n",
      "iteration 22000 training loss 2.3570201 lr 0.00066\n",
      "iteration 22500 training loss 1.9758248 lr 0.00066\n",
      "iteration 23000 training loss 1.4017382 lr 0.00066\n",
      "iteration 23500 training loss 1.7928855 lr 0.00066\n",
      "iteration 24000 training loss 1.8962064 lr 0.00066\n",
      "iteration 24500 training loss 1.5988038 lr 0.00066\n",
      "iteration 25000 training loss 2.0586665 lr 0.00066\n",
      "iteration 25500 training loss 2.0412936 lr 0.00059\n",
      "iteration 26000 training loss 1.7172508 lr 0.00059\n",
      "iteration 26500 training loss 1.9994355 lr 0.00059\n",
      "iteration 27000 training loss 1.7357109 lr 0.00059\n",
      "iteration 27500 training loss 2.1899605 lr 0.00059\n",
      "iteration 28000 training loss 2.0224915 lr 0.00059\n",
      "iteration 28500 training loss 1.8042359 lr 0.00059\n",
      "iteration 29000 training loss 1.969626 lr 0.00059\n",
      "iteration 29500 training loss 1.6798177 lr 0.00059\n",
      "iteration 30000 training loss 1.8324308 lr 0.00059\n",
      "epoch 1, it 30000 validation loss -0.945\n",
      "iteration 30500 training loss 2.004229 lr 0.00053\n",
      "iteration 31000 training loss 1.4849865 lr 0.00053\n",
      "iteration 31500 training loss 1.7509773 lr 0.00053\n",
      "iteration 32000 training loss 1.8179789 lr 0.00053\n",
      "iteration 32500 training loss 1.5689975 lr 0.00053\n",
      "iteration 33000 training loss 1.8456109 lr 0.00053\n",
      "iteration 33500 training loss 1.8309832 lr 0.00053\n",
      "iteration 34000 training loss 1.9750792 lr 0.00053\n",
      "iteration 34500 training loss 1.6408085 lr 0.00053\n",
      "iteration 35000 training loss 1.7579138 lr 0.00053\n",
      "iteration 35500 training loss 1.5920913 lr 0.00048\n",
      "iteration 36000 training loss 1.9772359 lr 0.00048\n",
      "iteration 36500 training loss 2.009963 lr 0.00048\n",
      "iteration 37000 training loss 2.1980214 lr 0.00048\n",
      "iteration 37500 training loss 1.8919612 lr 0.00048\n",
      "iteration 38000 training loss 4.762428 lr 0.00048\n",
      "iteration 38500 training loss 1.8835468 lr 0.00048\n",
      "iteration 39000 training loss 1.9151645 lr 0.00048\n",
      "iteration 39500 training loss 2.2558875 lr 0.00048\n",
      "iteration 40000 training loss 1.9805609 lr 0.00048\n",
      "epoch 1, it 40000 validation loss -0.943\n",
      "iteration 40500 training loss 1.7854648 lr 0.00043\n",
      "iteration 41000 training loss 1.9438722 lr 0.00043\n",
      "iteration 41500 training loss 1.7953494 lr 0.00043\n",
      "iteration 42000 training loss 1.6864398 lr 0.00043\n",
      "iteration 42500 training loss 1.6773163 lr 0.00043\n",
      "iteration 43000 training loss 1.8103201 lr 0.00043\n",
      "iteration 43500 training loss 1.8975654 lr 0.00043\n",
      "iteration 44000 training loss 1.9821959 lr 0.00043\n",
      "iteration 44500 training loss 1.65289 lr 0.00043\n",
      "iteration 45000 training loss 1.5132324 lr 0.00043\n",
      "iteration 45500 training loss 1.9309211 lr 0.00039\n",
      "iteration 46000 training loss 1.8546674 lr 0.00039\n",
      "iteration 46500 training loss 2.181292 lr 0.00039\n",
      "iteration 47000 training loss 2.290155 lr 0.00039\n",
      "iteration 47500 training loss 1.9079286 lr 0.00039\n",
      "iteration 48000 training loss 1.9796191 lr 0.00039\n",
      "iteration 48500 training loss 1.6353387 lr 0.00039\n",
      "iteration 49000 training loss 2.092764 lr 0.00039\n",
      "iteration 49500 training loss 2.4864795 lr 0.00039\n",
      "iteration 50000 training loss 2.0015793 lr 0.00039\n",
      "epoch 2, it 50000 validation loss -0.953\n",
      "iteration 50500 training loss 1.8516077 lr 0.00035\n",
      "iteration 51000 training loss 1.714037 lr 0.00035\n",
      "iteration 51500 training loss 1.7883668 lr 0.00035\n",
      "iteration 52000 training loss 1.9064288 lr 0.00035\n",
      "iteration 52500 training loss 1.882246 lr 0.00035\n",
      "iteration 53000 training loss 1.8698967 lr 0.00035\n",
      "iteration 53500 training loss 1.8396667 lr 0.00035\n",
      "iteration 54000 training loss 2.0353022 lr 0.00035\n",
      "iteration 54500 training loss 1.90382 lr 0.00035\n",
      "iteration 55000 training loss 1.9094713 lr 0.00035\n",
      "iteration 55500 training loss 1.9889657 lr 0.00031\n",
      "iteration 56000 training loss 1.6973135 lr 0.00031\n",
      "iteration 56500 training loss 1.7573957 lr 0.00031\n",
      "iteration 57000 training loss 1.8916578 lr 0.00031\n",
      "iteration 57500 training loss 1.6558119 lr 0.00031\n",
      "iteration 58000 training loss 1.9669915 lr 0.00031\n",
      "iteration 58500 training loss 1.584003 lr 0.00031\n",
      "iteration 59000 training loss 1.5428323 lr 0.00031\n",
      "iteration 59500 training loss 1.8857317 lr 0.00031\n",
      "iteration 60000 training loss 2.0171494 lr 0.00031\n",
      "epoch 2, it 60000 validation loss -0.951\n",
      "iteration 60500 training loss 1.9305565 lr 0.00028\n",
      "iteration 61000 training loss 2.3014128 lr 0.00028\n",
      "iteration 61500 training loss 1.8306249 lr 0.00028\n",
      "iteration 62000 training loss 2.0568192 lr 0.00028\n",
      "iteration 62500 training loss 1.804951 lr 0.00028\n",
      "iteration 63000 training loss 2.143188 lr 0.00028\n",
      "iteration 63500 training loss 1.7899412 lr 0.00028\n",
      "iteration 64000 training loss 1.9284563 lr 0.00028\n",
      "iteration 64500 training loss 1.9264588 lr 0.00028\n",
      "iteration 65000 training loss 1.8810097 lr 0.00028\n",
      "iteration 65500 training loss 1.8091574 lr 0.00025\n",
      "iteration 66000 training loss 1.8652596 lr 0.00025\n",
      "iteration 66500 training loss 1.8880424 lr 0.00025\n",
      "iteration 67000 training loss 1.5303876 lr 0.00025\n",
      "iteration 67500 training loss 1.4288825 lr 0.00025\n",
      "iteration 68000 training loss 1.8123224 lr 0.00025\n",
      "iteration 68500 training loss 1.7415627 lr 0.00025\n",
      "iteration 69000 training loss 1.86409 lr 0.00025\n",
      "iteration 69500 training loss 1.4938174 lr 0.00025\n",
      "iteration 70000 training loss 1.7540927 lr 0.00025\n",
      "epoch 3, it 70000 validation loss -0.957\n",
      "iteration 70500 training loss 1.9477775 lr 0.00023\n",
      "iteration 71000 training loss 2.0105233 lr 0.00023\n",
      "iteration 71500 training loss 1.6157656 lr 0.00023\n",
      "iteration 72000 training loss 1.6692978 lr 0.00023\n",
      "iteration 72500 training loss 1.6701813 lr 0.00023\n",
      "iteration 73000 training loss 2.1879215 lr 0.00023\n",
      "iteration 73500 training loss 3.136208 lr 0.00023\n",
      "iteration 74000 training loss 1.6199279 lr 0.00023\n",
      "iteration 74500 training loss 1.7709346 lr 0.00023\n",
      "iteration 75000 training loss 1.7372802 lr 0.00023\n",
      "iteration 75500 training loss 1.7402792 lr 0.00021\n",
      "iteration 76000 training loss 2.2843032 lr 0.00021\n",
      "iteration 76500 training loss 1.8455565 lr 0.00021\n",
      "iteration 77000 training loss 1.5811455 lr 0.00021\n",
      "iteration 77500 training loss 1.7179329 lr 0.00021\n",
      "iteration 78000 training loss 1.698885 lr 0.00021\n",
      "iteration 78500 training loss 1.7686355 lr 0.00021\n",
      "iteration 79000 training loss 1.9280541 lr 0.00021\n",
      "iteration 79500 training loss 1.3871104 lr 0.00021\n",
      "iteration 80000 training loss 1.9119126 lr 0.00021\n",
      "epoch 3, it 80000 validation loss -0.954\n",
      "iteration 80500 training loss 1.4637765 lr 0.00019\n",
      "iteration 81000 training loss 1.763341 lr 0.00019\n",
      "iteration 81500 training loss 2.1387148 lr 0.00019\n",
      "iteration 82000 training loss 1.796541 lr 0.00019\n",
      "iteration 82500 training loss 1.683631 lr 0.00019\n",
      "iteration 83000 training loss 1.9481221 lr 0.00019\n",
      "iteration 83500 training loss 1.711122 lr 0.00019\n",
      "iteration 84000 training loss 1.89124 lr 0.00019\n",
      "iteration 84500 training loss 2.0138927 lr 0.00019\n",
      "iteration 85000 training loss 2.1108975 lr 0.00019\n",
      "iteration 85500 training loss 1.5993872 lr 0.00017\n",
      "iteration 86000 training loss 1.7658325 lr 0.00017\n",
      "iteration 86500 training loss 1.7938194 lr 0.00017\n",
      "iteration 87000 training loss 2.1470346 lr 0.00017\n",
      "iteration 87500 training loss 1.706045 lr 0.00017\n",
      "iteration 88000 training loss 2.0487401 lr 0.00017\n",
      "iteration 88500 training loss 1.6093683 lr 0.00017\n",
      "iteration 89000 training loss 2.0840585 lr 0.00017\n",
      "iteration 89500 training loss 1.7071621 lr 0.00017\n",
      "iteration 90000 training loss 1.5764494 lr 0.00017\n",
      "epoch 4, it 90000 validation loss -0.959\n",
      "iteration 90500 training loss 1.6957211 lr 0.00015\n",
      "iteration 91000 training loss 1.544836 lr 0.00015\n",
      "iteration 91500 training loss 1.5988457 lr 0.00015\n",
      "iteration 92000 training loss 2.0161166 lr 0.00015\n",
      "iteration 92500 training loss 1.9217108 lr 0.00015\n",
      "iteration 93000 training loss 1.5843055 lr 0.00015\n",
      "iteration 93500 training loss 1.6712728 lr 0.00015\n",
      "iteration 94000 training loss 1.5039766 lr 0.00015\n",
      "iteration 94500 training loss 1.8460103 lr 0.00015\n",
      "iteration 95000 training loss 1.6340078 lr 0.00015\n",
      "iteration 95500 training loss 1.7067267 lr 0.00014\n",
      "iteration 96000 training loss 1.6946521 lr 0.00014\n",
      "iteration 96500 training loss 1.5366952 lr 0.00014\n",
      "iteration 97000 training loss 1.6161633 lr 0.00014\n",
      "iteration 97500 training loss 2.2100992 lr 0.00014\n",
      "iteration 98000 training loss 2.0308838 lr 0.00014\n",
      "iteration 98500 training loss 1.5922966 lr 0.00014\n",
      "iteration 99000 training loss 1.8445034 lr 0.00014\n",
      "iteration 99500 training loss 1.8941753 lr 0.00014\n",
      "iteration 100000 training loss 2.0249953 lr 0.00014\n",
      "epoch 4, it 100000 validation loss -0.955\n",
      "iteration 100500 training loss 1.9206389 lr 0.00012\n",
      "iteration 101000 training loss 1.8956628 lr 0.00012\n",
      "iteration 101500 training loss 1.733762 lr 0.00012\n",
      "iteration 102000 training loss 1.885616 lr 0.00012\n",
      "iteration 102500 training loss 1.643168 lr 0.00012\n",
      "iteration 103000 training loss 2.131026 lr 0.00012\n",
      "iteration 103500 training loss 1.6479487 lr 0.00012\n",
      "iteration 104000 training loss 1.5206392 lr 0.00012\n",
      "iteration 104500 training loss 1.8125352 lr 0.00012\n",
      "iteration 105000 training loss 1.678686 lr 0.00012\n",
      "iteration 105500 training loss 1.806975 lr 0.00011\n",
      "iteration 106000 training loss 1.7587967 lr 0.00011\n",
      "iteration 106500 training loss 1.3042485 lr 0.00011\n",
      "iteration 107000 training loss 1.8571528 lr 0.00011\n",
      "iteration 107500 training loss 1.9640107 lr 0.00011\n",
      "iteration 108000 training loss 1.6037459 lr 0.00011\n",
      "iteration 108500 training loss 1.9498764 lr 0.00011\n",
      "iteration 109000 training loss 1.4992034 lr 0.00011\n",
      "iteration 109500 training loss 1.7081056 lr 0.00011\n",
      "iteration 110000 training loss 1.5915265 lr 0.00011\n",
      "epoch 5, it 110000 validation loss -0.958\n",
      "iteration 110500 training loss 2.2074194 lr 0.00010\n",
      "iteration 111000 training loss 1.527641 lr 0.00010\n",
      "iteration 111500 training loss 1.6587856 lr 0.00010\n",
      "iteration 112000 training loss 1.6723008 lr 0.00010\n",
      "iteration 112500 training loss 2.012899 lr 0.00010\n",
      "iteration 113000 training loss 1.6712949 lr 0.00010\n",
      "iteration 113500 training loss 1.9855986 lr 0.00010\n",
      "iteration 114000 training loss 1.8018402 lr 0.00010\n",
      "iteration 114500 training loss 1.6474125 lr 0.00010\n",
      "iteration 115000 training loss 1.6114848 lr 0.00010\n",
      "iteration 115500 training loss 1.6823376 lr 0.00009\n",
      "iteration 116000 training loss 1.721961 lr 0.00009\n",
      "iteration 116500 training loss 1.9879225 lr 0.00009\n",
      "iteration 117000 training loss 1.6449337 lr 0.00009\n",
      "iteration 117500 training loss 2.0305443 lr 0.00009\n",
      "iteration 118000 training loss 1.9583242 lr 0.00009\n",
      "iteration 118500 training loss 1.6884733 lr 0.00009\n",
      "iteration 119000 training loss 1.5390984 lr 0.00009\n",
      "iteration 119500 training loss 1.8240691 lr 0.00009\n",
      "iteration 120000 training loss 1.7746611 lr 0.00009\n",
      "epoch 5, it 120000 validation loss -0.951\n",
      "stopping training\n"
     ]
    }
   ],
   "source": [
    "mlp.train(dataset, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3418de",
   "metadata": {},
   "source": [
    "## Evaluate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = mlp.predict_over_dataset(dataset.valid_data, return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043565f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[['prediction', 'target']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72249379",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin = np.linspace(0, 3, 100)\n",
    "plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "random_sample = val_df.sample(1000)\n",
    "\n",
    "plt.scatter(\n",
    "    random_sample.target,\n",
    "    random_sample.prediction,\n",
    "    alpha=0.1\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0639b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = val_df.sample(5_000)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.abs(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('abs error')\n",
    "x_lin = np.linspace(0, 3, 100)\n",
    "plt.plot(x_lin, x_lin, color='orange')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(\n",
    "    random_sample['target'],\n",
    "    np.square(random_sample['target'] - random_sample['prediction']),\n",
    "    alpha=0.07\n",
    ")\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('squared error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88c5f32-def0-4ee6-bc8d-970098aea415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_5(df):\n",
    "    top = df.sort_values('prediction').iloc[:5]\n",
    "    top = top['config_index'].values.tolist()\n",
    "    top = [str(i) for i in top]\n",
    "    return ';'.join(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b272b05-81de-4159-980c-8d2743099c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prediction = val_df.groupby('ID').apply(compute_best_5)\n",
    "val_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_score(candidate_configs, tile_dict):\n",
    "    config_scores = tile_dict['config_runtime'] / tile_dict['config_runtime_normalizers']\n",
    "    best_runtime = np.min(config_scores)\n",
    "    best_candidate_runtime = np.min(config_scores[candidate_configs])\n",
    "    score = 2 - best_candidate_runtime / best_runtime\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_valid_dir = 'predict-ai-model-runtime/npz_all/npz/tile/xla/valid'\n",
    "\n",
    "scores = []\n",
    "tile_ids = []\n",
    "for filename in os.listdir(tile_valid_dir):\n",
    "    tile_id = 'tile:xla:' + filename.rstrip('.npz')\n",
    "    tile_dict = dict(np.load(os.path.join(tile_valid_dir, filename)))\n",
    "    n_configurations = len(tile_dict['config_runtime'])\n",
    "    candidate_configs = val_prediction[tile_id]\n",
    "    candidate_configs = [int(i) for i in candidate_configs.split(';')]\n",
    "    score = tile_score(candidate_configs, tile_dict)\n",
    "    scores.append(score)\n",
    "    tile_ids.append(tile_id)\n",
    "\n",
    "evaluation_result = pd.DataFrame(\n",
    "    data=np.stack([tile_ids, scores], axis=-1),\n",
    "    columns=['ID', 'score']\n",
    ")\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18339",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result['score'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434f868",
   "metadata": {},
   "source": [
    "## Inference over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac93738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>config_index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'tile:xla:6ec96ca52640832842b7bf32dfb43b5d'</td>\n",
       "      <td>2</td>\n",
       "      <td>8.226151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'tile:xla:72e9fc567e22796ad3c058d93c056dbc'</td>\n",
       "      <td>326</td>\n",
       "      <td>3.133325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'tile:xla:fd3fa3d0a39c05343eeb8403562f20cc'</td>\n",
       "      <td>3551</td>\n",
       "      <td>3.306273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'tile:xla:75121ad778d92722504f620c4c108f45'</td>\n",
       "      <td>1023</td>\n",
       "      <td>9.414258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'tile:xla:403b920fef3cc6021cffe9960fdafce1'</td>\n",
       "      <td>279</td>\n",
       "      <td>10.113779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420531</th>\n",
       "      <td>b'tile:xla:47037df67e24913fc54f25f01a8e2df6'</td>\n",
       "      <td>237</td>\n",
       "      <td>2.976924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420532</th>\n",
       "      <td>b'tile:xla:47037df67e24913fc54f25f01a8e2df6'</td>\n",
       "      <td>5310</td>\n",
       "      <td>1.528257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420533</th>\n",
       "      <td>b'tile:xla:47037df67e24913fc54f25f01a8e2df6'</td>\n",
       "      <td>2796</td>\n",
       "      <td>3.012055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420534</th>\n",
       "      <td>b'tile:xla:47037df67e24913fc54f25f01a8e2df6'</td>\n",
       "      <td>1546</td>\n",
       "      <td>-0.609680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420535</th>\n",
       "      <td>b'tile:xla:47037df67e24913fc54f25f01a8e2df6'</td>\n",
       "      <td>3264</td>\n",
       "      <td>8.312181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1420536 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ID  config_index  \\\n",
       "0        b'tile:xla:6ec96ca52640832842b7bf32dfb43b5d'             2   \n",
       "1        b'tile:xla:72e9fc567e22796ad3c058d93c056dbc'           326   \n",
       "2        b'tile:xla:fd3fa3d0a39c05343eeb8403562f20cc'          3551   \n",
       "3        b'tile:xla:75121ad778d92722504f620c4c108f45'          1023   \n",
       "4        b'tile:xla:403b920fef3cc6021cffe9960fdafce1'           279   \n",
       "...                                               ...           ...   \n",
       "1420531  b'tile:xla:47037df67e24913fc54f25f01a8e2df6'           237   \n",
       "1420532  b'tile:xla:47037df67e24913fc54f25f01a8e2df6'          5310   \n",
       "1420533  b'tile:xla:47037df67e24913fc54f25f01a8e2df6'          2796   \n",
       "1420534  b'tile:xla:47037df67e24913fc54f25f01a8e2df6'          1546   \n",
       "1420535  b'tile:xla:47037df67e24913fc54f25f01a8e2df6'          3264   \n",
       "\n",
       "         prediction  \n",
       "0          8.226151  \n",
       "1          3.133325  \n",
       "2          3.306273  \n",
       "3          9.414258  \n",
       "4         10.113779  \n",
       "...             ...  \n",
       "1420531    2.976924  \n",
       "1420532    1.528257  \n",
       "1420533    3.012055  \n",
       "1420534   -0.609680  \n",
       "1420535    8.312181  \n",
       "\n",
       "[1420536 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = mlp.predict_over_dataset(dataset.test_data, return_labels=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21ba9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "tile:xla:0023795810403f8b0b244d88c901322f     4135;772;3193;2683;2133\n",
       "tile:xla:005c91ca7a50fffc663678fd44316f04         644;588;502;288;623\n",
       "tile:xla:0070642211d5a98a16b94f4d7df229fe         1030;61;848;447;958\n",
       "tile:xla:008191e0c67a6e7a62cde1a3e1d66795        701;610;1077;195;978\n",
       "tile:xla:008730b43f100be7c2800d7cb89578a4         606;595;757;642;395\n",
       "                                                       ...           \n",
       "tile:xla:fe52756188d770ee661f69dcd2688142    2780;8089;6761;7912;7418\n",
       "tile:xla:fe91ecfc5176e4dea4cec619beb19aeb           168;163;101;15;67\n",
       "tile:xla:ff9a30ba54b97a48b9f5370a9b1e8cb6             106;33;36;15;21\n",
       "tile:xla:ffa452493cbec7e4d2fee040879f98ae        778;700;426;815;1616\n",
       "tile:xla:ffbfd532de0a28c79f4740753fd78006           71;141;118;41;140\n",
       "Length: 844, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = test_df.groupby('ID').apply(compute_best_5)\n",
    "test_prediction.rename(index=lambda x: x.decode('UTF-8'), inplace=True)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d09fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_prediction, columns=['TopConfigs']).to_csv('tile_test_prediction_10_11_01_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4511f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(mlp.dense_layer_1.kernel.numpy().flatten()), bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f73f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
